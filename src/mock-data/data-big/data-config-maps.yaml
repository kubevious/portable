apiVersion: v1
items:
- apiVersion: v1
  data:
    foo1: bar1
    foo2: bar2
  kind: ConfigMap
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","data":{"foo1":"bar1","foo2":"bar2"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"my-config-1","namespace":"book"}}
    creationTimestamp: "2020-01-11T21:42:29Z"
    name: my-config-1
    namespace: book
    resourceVersion: "196818"
    selfLink: /api/v1/namespaces/book/configmaps/my-config-1
    uid: 445bbe59-34bb-11ea-9cdc-42010a8001cf
- apiVersion: v1
  kind: ConfigMap
  metadata:
    annotations:
      control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"gitlab-cainjector-5d757b9fdd-zd7sf_eb3797fe-9a0c-46ee-ac94-5d6e27193ca9","leaseDurationSeconds":15,"acquireTime":"2020-04-10T23:48:22Z","renewTime":"2020-04-10T23:49:42Z","leaderTransitions":1592}'
    creationTimestamp: "2020-01-18T00:44:03Z"
    name: cert-manager-cainjector-leader-election
    namespace: gitlab
    resourceVersion: "37201451"
    selfLink: /api/v1/namespaces/gitlab/configmaps/cert-manager-cainjector-leader-election
    uid: a00dce84-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  kind: ConfigMap
  metadata:
    annotations:
      control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"gitlab-cainjector-5d757b9fdd-zd7sf_20cc0cb9-c886-4ad7-8fc8-d90a7e116853","leaseDurationSeconds":15,"acquireTime":"2020-04-10T23:48:22Z","renewTime":"2020-04-10T23:49:42Z","leaderTransitions":1600}'
    creationTimestamp: "2020-01-18T00:44:03Z"
    name: cert-manager-cainjector-leader-election-core
    namespace: gitlab
    resourceVersion: "37201604"
    selfLink: /api/v1/namespaces/gitlab/configmaps/cert-manager-cainjector-leader-election-core
    uid: a01aa0f3-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  kind: ConfigMap
  metadata:
    annotations:
      control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"gitlab-cert-manager-5ffcc7f99f-b5h99-external-cert-manager-controller","leaseDurationSeconds":60,"acquireTime":"2020-03-31T18:42:30Z","renewTime":"2020-04-10T23:54:03Z","leaderTransitions":11}'
    creationTimestamp: "2020-01-18T00:44:07Z"
    name: cert-manager-controller
    namespace: gitlab
    resourceVersion: "37202413"
    selfLink: /api/v1/namespaces/gitlab/configmaps/cert-manager-controller
    uid: a2549524-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    create-issuer: |
      #!/bin/bash
      set -e ;

      issuer_file=$1
      namespace=gitlab

      echo "Creating the certmanager issuer..."
      set +e ; # The CRD may not exist yet. We need to retry until this passes
      while ! kubectl --namespace=$namespace apply -f ${issuer_file:=issuer.yml}; do
        sleep 1;
      done ;
      set -e ; # reset `e` as active
    issuer.yml: "\napiVersion: certmanager.k8s.io/v1alpha1\nkind: Issuer\nmetadata:\n
      \ name: gitlab-issuer\n  namespace: gitlab\n  labels:\n    app: certmanager-issuer\n
      \   chart: certmanager-issuer-0.1.0\n    release: gitlab\n    heritage: Helm\n
      \   \nspec:\n  acme:\n    # The ACME server URL\n    server: \"https://acme-v02.api.letsencrypt.org/directory\"\n
      \   # Email address used for ACME registration\n    email: \"me@example.com\"\n
      \   # Name of a secret used to store the ACME account private key\n    privateKeySecretRef:\n
      \     name: gitlab-acme-key\n    # Enable the HTTP-01 challenge provider\n    solvers:\n
      \   - selector: {}\n      http01:\n        ingress:\n          class: gitlab-nginx\n"
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: certmanager-issuer
      chart: certmanager-issuer-0.1.0
      heritage: Helm
      release: gitlab
    name: gitlab-certmanager-issuer-certmanager
    namespace: gitlab
    resourceVersion: "2016125"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-certmanager-issuer-certmanager
    uid: 980af3c3-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    config.toml.erb: |
      # The directory where Gitaly's executables are stored
      bin_dir = "/usr/local/bin"

      # listen on a TCP socket. This is insecure (no authentication)
      listen_addr = "0.0.0.0:8075"

      # Directory where internal sockets reside
      internal_socket_dir = "/home/git"

      # If metrics collection is enabled, inform gitaly about that
      prometheus_listen_addr = "localhost:9236"

      <% @storages = [ "default", ] %>
      <% @index=`echo ${HOSTNAME##*-}`.to_i %>
      <% if @storages.length > @index %>
      [[storage]]
      name = "<%= @storages[@index] %>"
      path = "/home/git/repositories"
      <% else %>
      <% raise Exception, "Storage for node #{@index} is not present in the storageNames array. Did you use kubectl to scale up ? You need to solely use helm for this purpose" %>
      <% end %>

      [logging]
      format = "json"
      dir = "/var/log/gitaly"

      [auth]
      token = "<%= File.read('/etc/gitlab-secrets/gitaly/gitaly_token').strip.dump[1..-2] %>"

      [git]

      [gitaly-ruby]
      # The directory where gitaly-ruby is installed
      dir = "/srv/gitaly-ruby"
      rugged_git_config_search_path = "/usr/local/etc"

      [gitlab-shell]
      # The directory where gitlab-shell is installed
      dir = "/srv/gitlab-shell"
    configure: |
      set -e
      mkdir -p /init-secrets/gitaly /init-secrets/shell
      cp -v -r -L /init-config/.gitlab_shell_secret  /init-secrets/shell/.gitlab_shell_secret
      cp -v -r -L /init-config/gitaly_token  /init-secrets/gitaly/gitaly_token
      mkdir -p /init-secrets/redis
      cp -v -r -L /init-config/redis_password  /init-secrets/redis/redis_password
    shell-config.yml.erb: |
      # GitLab user. git by default
      user: git

      # Url to gitlab instance. Used for api calls. Should end with a slash.
      gitlab_url: "http://gitlab-unicorn:8080/"

      secret_file: /etc/gitlab-secrets/shell/.gitlab_shell_secret

      http_settings:
        self_signed_cert: false

      # File used as authorized_keys for gitlab user
      auth_file: "/home/git/.ssh/authorized_keys"

      # Redis settings used for pushing commit notices to gitlab
      redis:
        host: gitlab-redis
        port: 6379
        pass: "<%= File.read("/etc/gitlab-secrets/redis/redis_password").strip.dump[1..-2] %>"
        database: nil
        namespace: resque:gitlab

      # Audit usernames.
      # Set to true to see real usernames in the logs instead of key ids, which is easier to follow, but
      # incurs an extra API call on every gitlab-shell command.
      audit_usernames: false
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: gitaly
      chart: gitaly-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-gitaly
    namespace: gitlab
    resourceVersion: "2016127"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-gitaly
    uid: 980c4aaf-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    gitlabChartVersion: 2.6.5
    gitlabVersion: 12.6.4
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: gitlab
      chart: gitlab-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-gitlab-chart-info
    namespace: gitlab
    resourceVersion: "2016119"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-gitlab-chart-info
    uid: 980562f2-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    configure: |
      set -e
      config_dir="/init-config"
      secret_dir="/init-secrets"

      for secret in postgres ; do
        mkdir -p "${secret_dir}/${secret}"
        cp -v -r -L "${config_dir}/${secret}/." "${secret_dir}/${secret}/"
      done
      for secret in redis ; do
        if [ -e "${config_dir}/${secret}" ]; then
          mkdir -p "${secret_dir}/${secret}"
          cp -v -r -L "${config_dir}/${secret}/." "${secret_dir}/${secret}/"
        fi
      done
    gitlab-exporter.yml.erb: |
      server:
        listen_address: 0.0.0.0
        listen_port: 9168

      probes:
        db_common: &db_common
          methods:
            - probe_db
          opts:
            connection_string: dbname=gitlabhq_production user=gitlab host=gitlab-postgresql port=5432 password='<%= File.read("/etc/gitlab/postgres/psql-password").strip.gsub(/[\'\\]/) { |esc| '\\' + esc } %>'
        database:
          multiple: true
          ci_builds:
            class_name: Database::CiBuildsProber
            <<: *db_common
          tuple_stats:
            class_name: Database::TuplesProber
            <<: *db_common
          rows_count:
            class_name: Database::RowCountProber
            <<: *db_common

        sidekiq: &sidekiq
          methods:
            - probe_queues
            - probe_workers
            - probe_retries
            - probe_stats
          opts:
            redis_url: redis://:<%= URI.escape(File.read("/etc/gitlab/redis/password").strip) %>@gitlab-redis:6379
            redis_enable_client: false

        metrics:
          multiple: true
          sidekiq:
            <<: *sidekiq
          ci_builds:
            class_name: Database::CiBuildsProber
            <<: *db_common
          tuple_stats:
            class_name: Database::TuplesProber
            <<: *db_common
          rows_count:
            class_name: Database::RowCountProber
            <<: *db_common
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: gitlab-exporter
      chart: gitlab-exporter-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-gitlab-exporter
    namespace: gitlab
    resourceVersion: "2016129"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-gitlab-exporter
    uid: 980d4d67-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    check-live: |
      #!/bin/bash
      if /usr/bin/pgrep -f .*register-the-runner; then
        exit 0
      elif /usr/bin/pgrep gitlab.*runner; then
        exit 0
      else
        exit 1
      fi
    config.toml: |
      concurrent = 10
      check_interval = 30
      log_level = "info"
      listen_address = '[::]:9252'
    configure: |
      set -e
      cp /init-secrets/* /secrets
    entrypoint: |
      #!/bin/bash
      set -e
      mkdir -p /home/gitlab-runner/.gitlab-runner/
      cp /scripts/config.toml /home/gitlab-runner/.gitlab-runner/

      # Register the runner
      if [[ -f /secrets/accesskey && -f /secrets/secretkey ]]; then
        export CACHE_S3_ACCESS_KEY=$(cat /secrets/accesskey)
        export CACHE_S3_SECRET_KEY=$(cat /secrets/secretkey)
      fi

      if [[ -f /secrets/gcs-applicaton-credentials-file ]]; then
        export GOOGLE_APPLICATION_CREDENTIALS="/secrets/gcs-applicaton-credentials-file"
      else
        if [[ -f /secrets/gcs-access-id && -f /secrets/gcs-private-key ]]; then
          export CACHE_GCS_ACCESS_ID=$(cat /secrets/gcs-access-id)
          # echo -e used to make private key multiline (in google json auth key private key is oneline with \n)
          export CACHE_GCS_PRIVATE_KEY=$(echo -e $(cat /secrets/gcs-private-key))
        fi
      fi

      if [[ -f /secrets/runner-registration-token ]]; then
        export REGISTRATION_TOKEN=$(cat /secrets/runner-registration-token)
      fi

      if [[ -f /secrets/runner-token ]]; then
        export CI_SERVER_TOKEN=$(cat /secrets/runner-token)
      fi

      if ! sh /scripts/register-the-runner; then
        exit 1
      fi

      # Start the runner
      exec /entrypoint run --user=gitlab-runner \
        --working-directory=/home/gitlab-runner
    register-the-runner: "#!/bin/bash\nMAX_REGISTER_ATTEMPTS=30\n\nfor i in $(seq
      1 \"${MAX_REGISTER_ATTEMPTS}\"); do\n  echo \"Registration attempt ${i} of ${MAX_REGISTER_ATTEMPTS}\"\n
      \ /entrypoint register \\\n    --non-interactive\n\n  retval=$?\n\n  if [ ${retval}
      = 0 ]; then\n    break\n  elif [ ${i} = ${MAX_REGISTER_ATTEMPTS} ]; then\n    exit
      1\n  fi\n\n  sleep 5 \ndone\n\nexit 0\n"
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: gitlab-gitlab-runner
      chart: gitlab-runner-0.12.0
      heritage: Helm
      release: gitlab
    name: gitlab-gitlab-runner
    namespace: gitlab
    resourceVersion: "2016121"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-gitlab-runner
    uid: 98060eda-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    config.yml.erb: |
      # GitLab user. git by default
      user: git

      # Url to gitlab instance. Used for api calls. Should end with a slash.
      gitlab_url: "http://gitlab-unicorn:8080/"

      secret_file: /etc/gitlab-secrets/shell/.gitlab_shell_secret

      http_settings:
        self_signed_cert: false

      # File used as authorized_keys for gitlab user
      auth_file: "/home/git/.ssh/authorized_keys"

      # Redis settings used for pushing commit notices to gitlab
      redis:
        host: gitlab-redis
        port: 6379
        pass: "<%= File.read("/etc/gitlab-secrets/redis/password").strip.dump[1..-2] %>"
        database: nil
        namespace: resque:gitlab

      # Log file.
      # Default is gitlab-shell.log in the root directory.
      log_file: "/var/log/gitlab-shell/gitlab-shell.log"

      # Log level. INFO by default
      log_level: INFO

      # Audit usernames.
      # Set to true to see real usernames in the logs instead of key ids, which is easier to follow, but
      # incurs an extra API call on every gitlab-shell command.
      audit_usernames: false
    configure: |
      set -e
      config_dir="/init-config"
      secret_dir="/init-secrets"

      for secret in shell ; do
        mkdir -p "${secret_dir}/${secret}"
        cp -v -r -L "${config_dir}/${secret}/." "${secret_dir}/${secret}/"
      done
      for secret in redis minio objectstorage ldap omniauth smtp ; do
        if [ -e "${config_dir}/${secret}" ]; then
          mkdir -p "${secret_dir}/${secret}"
          cp -v -r -L "${config_dir}/${secret}/." "${secret_dir}/${secret}/"
        fi
      done
      mkdir -p /${secret_dir}/ssh
      cp -v -r -L /${config_dir}/ssh_host_* /${secret_dir}/ssh/
      chmod 0400 /${secret_dir}/ssh/ssh_host_*
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: gitlab-shell
      chart: gitlab-shell-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-gitlab-shell
    namespace: gitlab
    resourceVersion: "2016130"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-gitlab-shell
    uid: 980db610-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    configure: |
      set -e
      config_dir="/init-config"
      secret_dir="/init-secrets"

      for secret in postgres rails-secrets migrations gitaly ; do
        mkdir -p "${secret_dir}/${secret}"
        cp -v -r -L "${config_dir}/${secret}/." "${secret_dir}/${secret}/"
      done
      for secret in redis minio objectstorage ldap omniauth smtp ; do
        if [ -e "${config_dir}/${secret}" ]; then
          mkdir -p "${secret_dir}/${secret}"
          cp -v -r -L "${config_dir}/${secret}/." "${secret_dir}/${secret}/"
        fi
      done
    database.yml.erb: "production:\n  adapter: postgresql\n  encoding: unicode\n  database:
      gitlabhq_production\n  pool: 10\n  username: gitlab\n  password: \"<%= File.read(\"/etc/gitlab/postgres/psql-password\").strip.dump[1..-2]
      %>\"\n  host: \"gitlab-postgresql\"\n  port: 5432\n  prepared_statements: false\n
      \ # load_balancing:\n  #   hosts:\n  #     - host1.example.com\n  #     - host2.example.com
      \     \n"
    gitlab.yml.erb: "production: &base\n  gitlab:\n  gitaly:\n    client_path: /home/git/gitaly/bin\n
      \   token: \"<%= File.read('/etc/gitlab/gitaly/gitaly_token').strip.dump[1..-2]
      %>\"\n  repositories:\n    storages: # You must have at least a `default` storage
      path.\n      default:\n        path: /var/opt/gitlab/repo\n        gitaly_address:
      tcp://gitlab-gitaly-0.gitlab-gitaly.gitlab:8075\n      \n  \n"
    resque.yml.erb: |
      production:
        # Redis (single instance)
        url: redis://:<%= URI.escape(File.read("/etc/gitlab/redis/password").strip) %>@gitlab-redis:6379
        id:
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: migrations
      chart: migrations-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-migrations
    namespace: gitlab
    resourceVersion: "2016118"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-migrations
    uid: 98061885-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    config.json: |-
      {
        "version": "20",
        "credential": {
          "accessKey": "ACCESS_KEY",
          "secretKey": "SECRET_KEY"
        },
        "region": "us-east-1",
        "browser": "on",
        "domain": "",
        "logger": {
          "console": {
            "enable": true
          },
          "file": {
            "enable": false,
            "fileName": ""
          }
        },
        "notify": {
          "amqp": {
            "1": {
              "enable": false,
              "url": "",
              "exchange": "",
              "routingKey": "",
              "exchangeType": "",
              "deliveryMode": 0,
              "mandatory": false,
              "immediate": false,
              "durable": false,
              "internal": false,
              "noWait": false,
              "autoDeleted": false
            }
          },
          "nats": {
            "1": {
              "enable": false,
              "address": "",
              "subject": "",
              "username": "",
              "password": "",
              "token": "",
              "secure": false,
              "pingInterval": 0,
              "streaming": {
                "enable": false,
                "clusterID": "",
                "clientID": "",
                "async": false,
                "maxPubAcksInflight": 0
              }
            }
          },
          "elasticsearch": {
            "1": {
              "enable": false,
              "format": "namespace",
              "url": "",
              "index": ""
            }
          },
          "redis": {
            "1": {
              "enable": false,
              "format": "namespace",
              "address": "",
              "password": "",
              "key": ""
            }
          },
          "postgresql": {
            "1": {
              "enable": false,
              "format": "namespace",
              "connectionString": "",
              "table": "",
              "host": "",
              "port": "",
              "user": "",
              "password": "",
              "database": ""
            }
          },
          "kafka": {
            "1": {
              "enable": false,
              "brokers": null,
              "topic": ""
            }
          },
          "webhook": {
            "1": {
              "enable": false,
              "endpoint": ""
            }
          },
          "mysql": {
            "1": {
              "enable": false,
              "format": "namespace",
              "dsnString": "",
              "table": "",
              "host": "",
              "port": "",
              "user": "",
              "password": "",
              "database": ""
            }
          },
          "mqtt": {
            "1": {
              "enable": false,
              "broker": "",
              "topic": "",
              "qos": 0,
              "clientId": "",
              "username": "",
              "password": ""
            }
          }
        }
      }
    configure: sed -e 's@ACCESS_KEY@'"$(cat /config/accesskey)"'@' -e 's@SECRET_KEY@'"$(cat
      /config/secretkey)"'@' /config/config.json > /minio/config.json
    initialize: |-
      #!/bin/sh
      # minio/mc container has Busybox Ash, be sure to be POSIX compliant and avoid Bash-isms
      set -e ; # Have script exit in the event of a failed command.

      # connectToMinio
      # Use a check-sleep-check loop to wait for Minio service to be available
      connectToMinio() {
        set -e ; # fail if we can't read the keys.
        ACCESS=$(cat /config/accesskey) ; SECRET=$(cat /config/secretkey) ;
        set +e ; # The connections to minio are allowed to fail.
        echo "Connecting to Minio server: http://$MINIO_ENDPOINT:$MINIO_PORT" ;
        MC_COMMAND="mc config host add myminio http://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
        $MC_COMMAND ;
        STATUS=$? ;
        until [ $STATUS -eq 0 ] ;
        do
          sleep 1 ; # 1 second intervals between attempts
          $MC_COMMAND ;
          STATUS=$? ;
        done ;
        set -e ; # reset `e` as active
        return 0
      }

      # checkBucketExists ($bucket)
      # Check if the bucket exists, by using the exit code of `mc ls`
      checkBucketExists() {
        BUCKET=$1
        CMD=$(/usr/bin/mc ls myminio/$BUCKET > /dev/null 2>&1)
        return $?
      }

      # createBucket ($bucket, $policy, $purge)
      # Ensure bucket exists, purging if asked to
      createBucket() {
        BUCKET=$1
        POLICY=$2
        PURGE=$3


        # Purge the bucket, if set & exists
        # Since PURGE is user input, check explicitly for `true`
        if [ $PURGE = true ]; then
          if checkBucketExists $BUCKET ; then
            echo "Purging bucket '$BUCKET'."
            set +e ; # don't exit if this fails
            /usr/bin/mc rm -r --force myminio/$BUCKET
            set -e ; # reset `e` as active
          else
            echo "Bucket '$BUCKET' does not exist, skipping purge."
          fi
        fi

        # Create the bucket if it does not exist
        if ! checkBucketExists $BUCKET ; then
          echo "Creating bucket '$BUCKET'"
          /usr/bin/mc mb myminio/$BUCKET
        else
          echo "Bucket '$BUCKET' already exists."
        fi

        # At this point, the bucket should exist, skip checking for existance
        # Set policy on the bucket
        echo "Setting policy of bucket '$BUCKET' to '$POLICY'."
        /usr/bin/mc policy $POLICY myminio/$BUCKET
      }

      connectToMinio
      createBucket registry none false
      createBucket git-lfs none false
      createBucket runner-cache none false
      createBucket gitlab-uploads none false
      createBucket gitlab-artifacts none false
      createBucket gitlab-backups none false
      createBucket gitlab-packages none false
      createBucket tmp none false
      createBucket gitlab-pseudo none false
      createBucket gitlab-mr-diffs none false
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: minio
      chart: minio-0.4.3
      heritage: Helm
      release: gitlab
    name: gitlab-minio-config-cm
    namespace: gitlab
    resourceVersion: "2016126"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-minio-config-cm
    uid: 980ad998-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    enable-vts-status: "true"
    hsts-include-subdomains: "false"
    proxy-set-headers: gitlab/gitlab-nginx-ingress-custom-headers
    server-name-hash-bucket-size: "256"
    server-tokens: "false"
    ssl-ciphers: ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:AES256-GCM-SHA384:AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:AES256-SHA:AES128-SHA:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4
    ssl-protocols: TLSv1.3 TLSv1.2
    use-http2: "false"
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.30.0-1
      component: controller
      heritage: Helm
      release: gitlab
    name: gitlab-nginx-ingress-controller
    namespace: gitlab
    resourceVersion: "2016138"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-nginx-ingress-controller
    uid: 981325d4-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    Referrer-Policy: strict-origin-when-cross-origin
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.30.0-1
      component: controller
      heritage: Helm
      release: gitlab
    name: gitlab-nginx-ingress-custom-headers
    namespace: gitlab
    resourceVersion: "2016124"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-nginx-ingress-custom-headers
    uid: 980a55c4-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    "22": gitlab/gitlab-gitlab-shell:22
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: gitlab-shell
      chart: gitlab-shell-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-nginx-ingress-tcp
    namespace: gitlab
    resourceVersion: "2016128"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-nginx-ingress-tcp
    uid: 980ca1e0-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: postgresql
      chart: postgresql-0.12.0
      heritage: Helm
      release: gitlab
    name: gitlab-postgresql
    namespace: gitlab
    resourceVersion: "2016131"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-postgresql
    uid: 980e0681-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    alerts: |
      {}
    prometheus.yml: |
      global:
        evaluation_interval: 1m
        scrape_interval: 1m
        scrape_timeout: 10s
      rule_files:
      - /etc/config/rules
      - /etc/config/alerts
      scrape_configs:
      - job_name: prometheus
        static_configs:
        - targets:
          - localhost:9090
      - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        job_name: kubernetes-apiservers
        kubernetes_sd_configs:
        - role: endpoints
        relabel_configs:
        - action: keep
          regex: default;kubernetes;https
          source_labels:
          - __meta_kubernetes_namespace
          - __meta_kubernetes_service_name
          - __meta_kubernetes_endpoint_port_name
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
      - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        job_name: kubernetes-nodes
        kubernetes_sd_configs:
        - role: node
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - replacement: kubernetes.default.svc:443
          target_label: __address__
        - regex: (.+)
          replacement: /api/v1/nodes/$1/proxy/metrics
          source_labels:
          - __meta_kubernetes_node_name
          target_label: __metrics_path__
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
      - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        job_name: kubernetes-nodes-cadvisor
        kubernetes_sd_configs:
        - role: node
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - replacement: kubernetes.default.svc:443
          target_label: __address__
        - regex: (.+)
          replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
          source_labels:
          - __meta_kubernetes_node_name
          target_label: __metrics_path__
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
      - job_name: kubernetes-service-endpoints
        kubernetes_sd_configs:
        - role: endpoints
        relabel_configs:
        - action: keep
          regex: true
          source_labels:
          - __meta_kubernetes_service_annotation_prometheus_io_scrape
        - action: replace
          regex: (https?)
          source_labels:
          - __meta_kubernetes_service_annotation_prometheus_io_scheme
          target_label: __scheme__
        - action: replace
          regex: (.+)
          source_labels:
          - __meta_kubernetes_service_annotation_prometheus_io_path
          target_label: __metrics_path__
        - action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          source_labels:
          - __address__
          - __meta_kubernetes_service_annotation_prometheus_io_port
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - action: replace
          source_labels:
          - __meta_kubernetes_namespace
          target_label: kubernetes_namespace
        - action: replace
          source_labels:
          - __meta_kubernetes_service_name
          target_label: kubernetes_name
        - action: replace
          source_labels:
          - __meta_kubernetes_pod_node_name
          target_label: kubernetes_node
      - honor_labels: true
        job_name: prometheus-pushgateway
        kubernetes_sd_configs:
        - role: service
        relabel_configs:
        - action: keep
          regex: pushgateway
          source_labels:
          - __meta_kubernetes_service_annotation_prometheus_io_probe
      - job_name: kubernetes-services
        kubernetes_sd_configs:
        - role: service
        metrics_path: /probe
        params:
          module:
          - http_2xx
        relabel_configs:
        - action: keep
          regex: true
          source_labels:
          - __meta_kubernetes_service_annotation_prometheus_io_probe
        - source_labels:
          - __address__
          target_label: __param_target
        - replacement: blackbox
          target_label: __address__
        - source_labels:
          - __param_target
          target_label: instance
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels:
          - __meta_kubernetes_namespace
          target_label: kubernetes_namespace
        - source_labels:
          - __meta_kubernetes_service_name
          target_label: kubernetes_name
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - action: keep
          regex: true
          source_labels:
          - __meta_kubernetes_pod_annotation_prometheus_io_scrape
        - action: replace
          regex: (.+)
          source_labels:
          - __meta_kubernetes_pod_annotation_prometheus_io_path
          target_label: __metrics_path__
        - action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          source_labels:
          - __address__
          - __meta_kubernetes_pod_annotation_prometheus_io_port
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - action: replace
          source_labels:
          - __meta_kubernetes_namespace
          target_label: kubernetes_namespace
        - action: replace
          source_labels:
          - __meta_kubernetes_pod_name
          target_label: kubernetes_pod_name
    rules: |
      {}
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: prometheus
      chart: prometheus-9.0.0
      component: server
      heritage: Helm
      release: gitlab
    name: gitlab-prometheus-server
    namespace: gitlab
    resourceVersion: "2016122"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-prometheus-server
    uid: 9806b3cd-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    configure: |
      set -e
      cat /config/redis-master.conf > /redis/master.conf
      cat /config/redis-slave.conf > /redis/slave.conf
      echo "$(cat /config/password)" >> /redis/pass
      echo "127.0.0.1:6379,$(cat /config/password),gitlab-redis" > /metrics/redis
    redis-master.conf: |
      # requirepass
      # stay in foreground
      daemonize no
      # listen on all interfaces
      bind 0.0.0.0
      port 6379
      timeout 60
      tcp-keepalive 300
      # Log level
      loglevel notice
      # Log to stdout
      logfile ""
      # database count (picked from Omnibus' redis.conf)
      databases 16
      # Database filename
      dbfilename gitlab-redis.rdb
      # Working Directory (where DB is written)
      dir /redis-master-data
      # Configure persistence snapshotting
      save 60 1000
      save 300 10
      save 900 1
    redis-slave.conf: |
      slaveof %master-ip% %master-port%
      slave-priority 100
      slave-read-only yes
      slave-serve-stale-data yes
      # requirepass
      # masterauth
      # stay in foreground
      daemonize no
      # listen on all interfaces
      bind 0.0.0.0
      port 6379
      timeout 60
      tcp-keepalive 300
      # Log level
      loglevel notice
      # Log to stdout
      logfile ""
      # database count (picked from Omnibus' redis.conf)
      databases 16
      # Database filename
      dbfilename gitlab-redis.rdb
      # Working Directory (where DB is written)
      dir /redis-master-data
      # Configure persistence snapshotting
      save 60 1000
      save 300 10
      save 900 1
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: redis
      chart: redis-ha-0.1.0
      heritage: Helm
      release: gitlab
    name: gitlab-redis
    namespace: gitlab
    resourceVersion: "2016136"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-redis
    uid: 98104e21-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    config.yml: |
      version: 0.1
      log:
        fields:
          service: registry
        level: warn
      http:
        debug:
          addr: ':5001'
          prometheus:
            enabled: false
            path: /metrics
        draintimeout: 0
        headers:
          X-Content-Type-Options: [nosniff]
        addr: :5000
        secret: "HTTP_SECRET"
        relativeurls: false
      health:
        storagedriver:
          enabled: false
          interval: 10s
          threshold: 3
      auth:
        token:
          realm: https://gitlab.example.com/jwt/auth
          service: container_registry
          issuer: "gitlab-issuer"
          # This is provided from the initContainer execution, at a known path.
          rootcertbundle: /etc/docker/registry/certificate.crt
          autoredirect: false
      compatibility:
        schema1:
          enabled: false
      validation:
        disabled: true
      storage:
        maintenance:
          readonly:
            enabled: false
        s3:
          accesskey: "ACCESS_KEY"
          secretkey: "SECRET_KEY"
          region: us-east-1
          regionendpoint: http://gitlab-minio-svc:9000
          bucket: registry
          secure: true
          v4auth: true
          rootdirectory: /
        cache:
          blobdescriptor: 'inmemory'
        delete:
          enabled: true
        redirect:
          disable: true
    configure: |-
      if [ -e /config/accesskey ] ; then
        sed -e 's@ACCESS_KEY@'"$(cat /config/accesskey)"'@' -e 's@SECRET_KEY@'"$(cat /config/secretkey)"'@' /config/config.yml > /registry/config.yml
      else
        cp -v -r -L /config/config.yml  /registry/config.yml
      fi
      # Place the `http.secret` value from the kubernetes secret
      sed -i -e 's@HTTP_SECRET@'"$(cat /config/httpSecret)"'@' /registry/config.yml
      # Insert any provided `storage` block from kubernetes secret
      if [ -d /config/storage ]; then
        # Copy contents of storage secret(s)
        mkdir -p /registry/storage
        cp -v -r -L /config/storage/* /registry/storage/
        # Ensure there is a new line in the end
        echo '' >> /registry/storage/config
        # Default `delete.enabled: true` if not present.
        ## Note: busybox grep doesn't support multiline, so we chain `egrep`.
        if ! $(egrep -A1 '^delete:\s*$' /registry/storage/config | egrep -q '\s{2,4}enabled:') ; then
          echo 'delete:' >> /registry/storage/config
          echo '  enabled: true' >> /registry/storage/config
        fi
        # Indent /registry/storage/config 2 spaces before inserting into config.yml
        sed -i 's/^/  /' /registry/storage/config
        # Insert into /registry/config.yml after `storage:`
        sed -i '/storage:/ r /registry/storage/config' /registry/config.yml
        # Remove the now extraneous `config` file
        rm /registry/storage/config
      fi
      # Set to known path, to used ConfigMap
      cat /config/certificate.crt > /registry/certificate.crt
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: registry
      chart: registry-0.3.0
      heritage: Helm
      release: gitlab
    name: gitlab-registry
    namespace: gitlab
    resourceVersion: "2016120"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-registry
    uid: 9805c5ad-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    configure: |
      set -e
      config_dir="/init-config"
      secret_dir="/init-secrets"

      for secret in gitaly registry postgres rails-secrets ; do
        mkdir -p "${secret_dir}/${secret}"
        cp -v -r -L "${config_dir}/${secret}/." "${secret_dir}/${secret}/"
      done
      for secret in redis minio objectstorage ldap omniauth smtp ; do
        if [ -e "${config_dir}/${secret}" ]; then
          mkdir -p "${secret_dir}/${secret}"
          cp -v -r -L "${config_dir}/${secret}/." "${secret_dir}/${secret}/"
        fi
      done
    database.yml.erb: "production:\n  adapter: postgresql\n  encoding: unicode\n  database:
      gitlabhq_production\n  pool: 10\n  username: gitlab\n  password: \"<%= File.read(\"/etc/gitlab/postgres/psql-password\").strip.dump[1..-2]
      %>\"\n  host: \"gitlab-postgresql\"\n  port: 5432\n  prepared_statements: false\n
      \ # load_balancing:\n  #   hosts:\n  #     - host1.example.com\n  #     - host2.example.com\n
      \ \n"
    gitlab.yml.erb: "production: &base\n  gitlab:\n    host: gitlab.example.com\n
      \   https: true\n    impersonation_enabled: \n    usage_ping_enabled: true\n
      \   default_can_create_group: true\n    username_changing_enabled: true\n    issue_closing_pattern:
      \n    default_theme: \n    default_projects_features:\n      issues: true\n
      \     merge_requests: true\n      wiki: true\n      snippets: true\n      builds:
      true\n      container_registry: true\n    webhook_timeout: \n    trusted_proxies:\n
      \   time_zone: \"UTC\"        \n    email_from: \"gitlab@example.com\"\n    email_display_name:
      \"GitLab\"\n    email_reply_to: \"noreply@example.com\"\n    email_subject_suffix:
      \"\"\n  gravatar:\n    plain_url: \n    ssl_url: \n  \n  extra:\n    \n    \n
      \   \n  artifacts:\n    enabled: true\n    object_store:\n      enabled: true\n
      \     remote_directory: gitlab-artifacts\n      direct_upload: true\n      background_upload:
      false\n      proxy_download: true\n      connection:\n        provider: AWS\n
      \       region: us-east-1\n        aws_access_key_id: \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2]
      %>\"\n        aws_secret_access_key: \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2]
      %>\"\n        host: minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n
      \       path_style: true\n  lfs:\n    enabled: true\n    object_store:\n      enabled:
      true\n      remote_directory: git-lfs\n      direct_upload: true\n      background_upload:
      false\n      proxy_download: true\n      connection:\n        provider: AWS\n
      \       region: us-east-1\n        aws_access_key_id: \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2]
      %>\"\n        aws_secret_access_key: \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2]
      %>\"\n        host: minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n
      \       path_style: true\n  uploads:\n    enabled: true\n    object_store:\n
      \     enabled: true\n      remote_directory: gitlab-uploads\n      direct_upload:
      true\n      background_upload: false\n      proxy_download: true\n      connection:\n
      \       provider: AWS\n        region: us-east-1\n        aws_access_key_id:
      \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2] %>\"\n        aws_secret_access_key:
      \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2] %>\"\n        host:
      minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n        path_style:
      true\n  packages:\n    enabled: true\n    object_store:\n      enabled: true\n
      \     remote_directory: gitlab-packages\n      direct_upload: true\n      background_upload:
      false\n      proxy_download: true\n      connection:\n        provider: AWS\n
      \       region: us-east-1\n        aws_access_key_id: \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2]
      %>\"\n        aws_secret_access_key: \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2]
      %>\"\n        host: minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n
      \       path_style: true\n  external_diffs:\n    enabled: \n    when: \n    object_store:\n
      \     enabled: false\n      remote_directory: gitlab-mr-diffs\n      direct_upload:
      true\n      background_upload: false\n      proxy_download: true\n      connection:\n
      \       provider: AWS\n        region: us-east-1\n        aws_access_key_id:
      \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2] %>\"\n        aws_secret_access_key:
      \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2] %>\"\n        host:
      minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n        path_style:
      true\n  pseudonymizer:\n    manifest: config/pseudonymizer.yml\n    upload:\n
      \     remote_directory: gitlab-pseudo\n      connection:\n        provider:
      AWS\n        region: us-east-1\n        aws_access_key_id: \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2]
      %>\"\n        aws_secret_access_key: \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2]
      %>\"\n        host: minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n
      \       path_style: true\n  \n  pages:\n    enabled: false\n  mattermost:\n
      \   enabled: false\n  ## Registry Integration\n  registry:\n    enabled: true\n
      \   host: registry.example.com\n    api_url: http://gitlab-registry:5000\n    key:
      /etc/gitlab/registry/gitlab-registry.key\n    issuer: gitlab-issuer\n  gitlab_ci:\n
      \ ldap:\n    enabled: false\n  \n  omniauth:\n    enabled: false\n    sync_profile_from_provider:
      []\n    sync_profile_attributes: [\"email\"]\n    allow_single_sign_on: [\"saml\"]\n
      \   block_auto_created_users: true\n    auto_link_ldap_user: false\n    auto_link_saml_user:
      false\n    external_providers: []\n  kerberos:\n    enabled: false\n  shared:\n
      \ gitaly:\n    client_path: /home/git/gitaly/bin\n    token: \"<%= File.read('/etc/gitlab/gitaly/gitaly_token').strip.dump[1..-2]
      %>\"\n  repositories:\n    storages: # You must have at least a `default` storage
      path.\n      default:\n        path: /var/opt/gitlab/repo\n        gitaly_address:
      tcp://gitlab-gitaly-0.gitlab-gitaly.gitlab:8075\n      \n  backup:\n    path:
      \"tmp/backups\"   # Relative paths are relative to Rails.root (default: tmp/backups/)\n
      \ gitlab_shell:\n    path: /home/git/gitlab-shell/\n    hooks_path: /home/git/gitlab-shell/hooks/\n
      \   upload_pack: true\n    receive_pack: true\n  workhorse:\n  git:\n    bin_path:
      /usr/bin/git\n  webpack:\n  monitoring:\n    ip_whitelist:\n      - 127.0.0.0/8\n
      \   sidekiq_exporter:\n      enabled: true\n      address: 0.0.0.0\n      port:
      3807\n"
    installation_type: |
      gitlab-helm-chart
    resque.yml.erb: |
      production:
        # Redis (single instance)
        url: redis://:<%= URI.escape(File.read("/etc/gitlab/redis/password").strip) %>@gitlab-redis:6379
        id:
    smtp_settings.rb: ""
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: sidekiq
      chart: sidekiq-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-sidekiq
    namespace: gitlab
    resourceVersion: "2016133"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-sidekiq
    uid: 9810a610-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    sidekiq_queues.yml.erb: "<%=\n   # this works because codebase default content
      has only `:\n   sq = YAML.load_file('/srv/gitlab/config/sidekiq_queues.yml')\n
      \  \n   sq.to_yaml\n%>\n:concurrency: 25\n"
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: sidekiq
      chart: sidekiq-2.6.5
      heritage: Helm
      queue_pod_name: all-in-1
      release: gitlab
    name: gitlab-sidekiq-all-in-1
    namespace: gitlab
    resourceVersion: "2016134"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-sidekiq-all-in-1
    uid: 98100362-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    configure: |
      set -e
      config_dir="/init-config"
      secret_dir="/init-secrets"

      for secret in shell gitaly registry postgres rails-secrets ; do
        mkdir -p "${secret_dir}/${secret}"
        cp -v -r -L "${config_dir}/${secret}/." "${secret_dir}/${secret}/"
      done
      for secret in redis minio objectstorage ldap omniauth smtp ; do
        if [ -e "${config_dir}/${secret}" ]; then
          mkdir -p "${secret_dir}/${secret}"
          cp -v -r -L "${config_dir}/${secret}/." "${secret_dir}/${secret}/"
        fi
      done



      if [ ! -f "/${secret_dir}/objectstorage/.s3cfg" ]; then
      cat <<EOF > "/${secret_dir}/.s3cfg"
      [default]
      access_key = $(cat /init-secrets/minio/accesskey)
      secret_key = $(cat /init-secrets/minio/secretkey)
      bucket_location = us-east-1
      host_base = minio.example.com
      host_bucket = minio.example.com/%(bucket)
      default_mime_type = binary/octet-stream
      enable_multipart = True
      multipart_max_chunks = 10000
      multipart_chunk_size_mb = 128
      recursive = True
      recv_chunk = 65536
      send_chunk = 65536
      server_side_encryption = False
      signature_v2 = True
      socket_timeout = 300
      use_mime_magic = True
      verbosity = WARNING
      website_endpoint = https://minio.example.com
      EOF
      else
        mv "/${secret_dir}/objectstorage/.s3cfg" "/${secret_dir}/.s3cfg"
      fi
    configure-gsutil: |
      # The following script is used to configure gsutil when creating backups
      # It provides inputs to the `gsutil config -e` prompt as follows:
      # 1) Path to service account JSON key file
      # 2) Do not set permissions for key file
      # 3) GCP Project ID
      # 4) Decline anonymous usage statistics
      printf "$GOOGLE_APPLICATION_CREDENTIALS\nN\n\nN\n" | gsutil config -e
    database.yml.erb: "production:\n  adapter: postgresql\n  encoding: unicode\n  database:
      gitlabhq_production\n  pool: 10\n  username: gitlab\n  password: \"<%= File.read(\"/etc/gitlab/postgres/psql-password\").strip.dump[1..-2]
      %>\"\n  host: \"gitlab-postgresql\"\n  port: 5432\n  prepared_statements: false\n
      \ \n"
    gitlab.yml.erb: "production: &base\n  gitlab:\n    host: gitlab.example.com\n
      \   https: true\n    impersonation_enabled: \n    usage_ping_enabled: true\n
      \   default_can_create_group: true\n    username_changing_enabled: true\n    issue_closing_pattern:
      \n    default_theme: \n    default_projects_features:\n      issues: true\n
      \     merge_requests: true\n      wiki: true\n      snippets: true\n      builds:
      true\n      container_registry: true\n    webhook_timeout: \n    trusted_proxies:\n
      \   time_zone: \"UTC\"        \n    email_from: \"gitlab@example.com\"\n    email_display_name:
      \"GitLab\"\n    email_reply_to: \"noreply@example.com\"\n    email_subject_suffix:
      \"\"\n  \n  artifacts:\n    enabled: true\n    object_store:\n      enabled:
      true\n      remote_directory: gitlab-artifacts\n      direct_upload: true\n
      \     background_upload: false\n      proxy_download: true\n      connection:\n
      \       provider: AWS\n        region: us-east-1\n        aws_access_key_id:
      \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2] %>\"\n        aws_secret_access_key:
      \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2] %>\"\n        host:
      minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n        path_style:
      true\n  lfs:\n    enabled: true\n    object_store:\n      enabled: true\n      remote_directory:
      git-lfs\n      direct_upload: true\n      background_upload: false\n      proxy_download:
      true\n      connection:\n        provider: AWS\n        region: us-east-1\n
      \       aws_access_key_id: \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2]
      %>\"\n        aws_secret_access_key: \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2]
      %>\"\n        host: minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n
      \       path_style: true\n  uploads:\n    enabled: true\n    object_store:\n
      \     enabled: true\n      remote_directory: gitlab-uploads\n      direct_upload:
      true\n      background_upload: false\n      proxy_download: true\n      connection:\n
      \       provider: AWS\n        region: us-east-1\n        aws_access_key_id:
      \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2] %>\"\n        aws_secret_access_key:
      \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2] %>\"\n        host:
      minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n        path_style:
      true\n  packages:\n    enabled: true\n    object_store:\n      enabled: true\n
      \     remote_directory: gitlab-packages\n      direct_upload: true\n      background_upload:
      false\n      proxy_download: true\n      connection:\n        provider: AWS\n
      \       region: us-east-1\n        aws_access_key_id: \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2]
      %>\"\n        aws_secret_access_key: \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2]
      %>\"\n        host: minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n
      \       path_style: true\n  external_diffs:\n    enabled: \n    when: \n    object_store:\n
      \     enabled: false\n      remote_directory: gitlab-mr-diffs\n      direct_upload:
      true\n      background_upload: false\n      proxy_download: true\n      connection:\n
      \       provider: AWS\n        region: us-east-1\n        aws_access_key_id:
      \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2] %>\"\n        aws_secret_access_key:
      \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2] %>\"\n        host:
      minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n        path_style:
      true\n  pseudonymizer:\n    manifest: config/pseudonymizer.yml\n    upload:\n
      \     remote_directory: gitlab-pseudo\n      connection:\n        provider:
      AWS\n        region: us-east-1\n        aws_access_key_id: \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2]
      %>\"\n        aws_secret_access_key: \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2]
      %>\"\n        host: minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n
      \       path_style: true\n  \n  pages:\n    enabled: false\n  mattermost:\n
      \   enabled: false\n  ## Registry Integration\n  registry:\n    enabled: true\n
      \   host: registry.example.com\n    api_url: http://gitlab-registry:5000\n    key:
      /etc/gitlab/registry/gitlab-registry.key\n    issuer: gitlab-issuer\n  gitlab_ci:\n
      \ ldap:\n    enabled: false\n  \n  omniauth:\n    enabled: false\n    sync_profile_from_provider:
      []\n    sync_profile_attributes: [\"email\"]\n    allow_single_sign_on: [\"saml\"]\n
      \   block_auto_created_users: true\n    auto_link_ldap_user: false\n    auto_link_saml_user:
      false\n    external_providers: []\n  kerberos:\n    enabled: false\n  shared:\n
      \ gitaly:\n    client_path: /home/git/gitaly/bin\n    token: \"<%= File.read('/etc/gitlab/gitaly/gitaly_token').strip.dump[1..-2]
      %>\"\n  repositories:\n    storages: # You must have at least a `default` storage
      path.\n      default:\n        path: /var/opt/gitlab/repo\n        gitaly_address:
      tcp://gitlab-gitaly-0.gitlab-gitaly.gitlab:8075\n      \n  backup:\n    path:
      \"tmp/backups\"   # Relative paths are relative to Rails.root (default: tmp/backups/)\n
      \ gitlab_shell:\n    path: /home/git/gitlab-shell/\n    hooks_path: /home/git/gitlab-shell/hooks/\n
      \   upload_pack: true\n    receive_pack: true\n    secret_file: /etc/gitlab/shell/.gitlab_shell_secret\n
      \ workhorse:\n  git:\n    bin_path: /usr/bin/git\n  webpack:\n  monitoring:\n
      \   ip_whitelist:\n      - 127.0.0.0/8\n    sidekiq_exporter:\n  extra:\n"
    resque.yml.erb: |
      production:
        # Redis (single instance)
        url: redis://:<%= URI.escape(File.read("/etc/gitlab/redis/password").strip) %>@gitlab-redis:6379
        id:
    smtp_settings.rb: ""
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: task-runner
      chart: task-runner-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-task-runner
    namespace: gitlab
    resourceVersion: "2016137"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-task-runner
    uid: 9812ba53-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    configure: |
      set -e
      config_dir="/init-config"
      secret_dir="/init-secrets"

      for secret in shell gitaly registry postgres rails-secrets gitlab-workhorse ; do
        mkdir -p "${secret_dir}/${secret}"
        cp -v -r -L "${config_dir}/${secret}/." "${secret_dir}/${secret}/"
      done
      for secret in redis minio objectstorage ldap omniauth smtp ; do
        if [ -e "${config_dir}/${secret}" ]; then
          mkdir -p "${secret_dir}/${secret}"
          cp -v -r -L "${config_dir}/${secret}/." "${secret_dir}/${secret}/"
        fi
      done
    database.yml.erb: "production:\n  adapter: postgresql\n  encoding: unicode\n  database:
      gitlabhq_production\n  pool: 10\n  username: gitlab\n  password: \"<%= File.read(\"/etc/gitlab/postgres/psql-password\").strip.dump[1..-2]
      %>\"\n  host: \"gitlab-postgresql\"\n  port: 5432\n  prepared_statements: false\n
      \ # load_balancing:\n  #   hosts:\n  #     - host1.example.com\n  #     - host2.example.com\n
      \ \n"
    gitlab.yml.erb: "production: &base\n  gitlab:\n    host: gitlab.example.com\n
      \   https: true\n    impersonation_enabled: \n    usage_ping_enabled: true\n
      \   default_can_create_group: true\n    username_changing_enabled: true\n    issue_closing_pattern:
      \n    default_theme: \n    default_projects_features:\n      issues: true\n
      \     merge_requests: true\n      wiki: true\n      snippets: true\n      builds:
      true\n      container_registry: true\n    webhook_timeout: \n    trusted_proxies:\n
      \   time_zone: \"UTC\"        \n    email_from: \"gitlab@example.com\"\n    email_display_name:
      \"GitLab\"\n    email_reply_to: \"noreply@example.com\"\n    email_subject_suffix:
      \"\"\n  \n  gravatar:\n    plain_url: \n    ssl_url: \n  extra:\n    \n    \n
      \   \n  artifacts:\n    enabled: true\n    object_store:\n      enabled: true\n
      \     remote_directory: gitlab-artifacts\n      direct_upload: true\n      background_upload:
      false\n      proxy_download: true\n      connection:\n        provider: AWS\n
      \       region: us-east-1\n        aws_access_key_id: \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2]
      %>\"\n        aws_secret_access_key: \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2]
      %>\"\n        host: minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n
      \       path_style: true\n  lfs:\n    enabled: true\n    object_store:\n      enabled:
      true\n      remote_directory: git-lfs\n      direct_upload: true\n      background_upload:
      false\n      proxy_download: true\n      connection:\n        provider: AWS\n
      \       region: us-east-1\n        aws_access_key_id: \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2]
      %>\"\n        aws_secret_access_key: \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2]
      %>\"\n        host: minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n
      \       path_style: true\n  uploads:\n    enabled: true\n    object_store:\n
      \     enabled: true\n      remote_directory: gitlab-uploads\n      direct_upload:
      true\n      background_upload: false\n      proxy_download: true\n      connection:\n
      \       provider: AWS\n        region: us-east-1\n        aws_access_key_id:
      \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2] %>\"\n        aws_secret_access_key:
      \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2] %>\"\n        host:
      minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n        path_style:
      true\n  packages:\n    enabled: true\n    object_store:\n      enabled: true\n
      \     remote_directory: gitlab-packages\n      direct_upload: true\n      background_upload:
      false\n      proxy_download: true\n      connection:\n        provider: AWS\n
      \       region: us-east-1\n        aws_access_key_id: \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2]
      %>\"\n        aws_secret_access_key: \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2]
      %>\"\n        host: minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n
      \       path_style: true\n  external_diffs:\n    enabled: \n    when: \n    object_store:\n
      \     enabled: false\n      remote_directory: gitlab-mr-diffs\n      direct_upload:
      true\n      background_upload: false\n      proxy_download: true\n      connection:\n
      \       provider: AWS\n        region: us-east-1\n        aws_access_key_id:
      \"<%= File.read('/etc/gitlab/minio/accesskey').strip.dump[1..-2] %>\"\n        aws_secret_access_key:
      \"<%= File.read('/etc/gitlab/minio/secretkey').strip.dump[1..-2] %>\"\n        host:
      minio.example.com\n        endpoint: http://gitlab-minio-svc:9000\n        path_style:
      true\n  \n  pages:\n    enabled: false\n  mattermost:\n    enabled: false\n
      \ gitlab_ci:\n  ldap:\n    enabled: false\n  \n  omniauth:\n    enabled: false\n
      \   sync_profile_from_provider: []\n    sync_profile_attributes: [\"email\"]\n
      \   allow_single_sign_on: [\"saml\"]\n    block_auto_created_users: true\n    auto_link_ldap_user:
      false\n    auto_link_saml_user: false\n    external_providers: []\n  kerberos:\n
      \   enabled: false\n  shared:\n  gitaly:\n    client_path: /home/git/gitaly/bin\n
      \   token: \"<%= File.read('/etc/gitlab/gitaly/gitaly_token').strip.dump[1..-2]
      %>\"\n  repositories:\n    storages: # You must have at least a `default` storage
      path.\n      default:\n        path: /var/opt/gitlab/repo\n        gitaly_address:
      tcp://gitlab-gitaly-0.gitlab-gitaly.gitlab:8075\n      \n  backup:\n    path:
      \"tmp/backups\"   # Relative paths are relative to Rails.root (default: tmp/backups/)\n
      \ gitlab_shell:\n    path: /home/git/gitlab-shell/\n    hooks_path: /home/git/gitlab-shell/hooks/\n
      \   upload_pack: true\n    receive_pack: true\n    ssh_port: 22\n    secret_file:
      /etc/gitlab/shell/.gitlab_shell_secret\n  workhorse:\n    secret_file: /etc/gitlab/gitlab-workhorse/secret\n
      \ git:\n    bin_path: /usr/bin/git\n  webpack:\n  monitoring:\n    ip_whitelist:\n
      \     - 0.0.0.0/0\n    web_exporter:\n      enabled: false\n      address: 0.0.0.0\n
      \     port: 8083\n    sidekiq_exporter:\n  shutdown:\n    blackout_seconds:
      10\n  rack_attack:\n    git_basic_auth:\n  ## Registry Integration\n  registry:\n
      \   enabled: true\n    host: registry.example.com\n    api_url: http://gitlab-registry:5000\n
      \   key: /etc/gitlab/registry/gitlab-registry.key\n    issuer: gitlab-issuer\n"
    installation_type: |
      gitlab-helm-chart
    resque.yml.erb: |
      production:
        # Redis (single instance)
        url: redis://:<%= URI.escape(File.read("/etc/gitlab/redis/password").strip) %>@gitlab-redis:6379
        id:
    smtp_settings.rb: ""
    unicorn.rb: |
      # This file should be equivalent to `unicorn.rb` from:
      # * gitlab-foss: https://gitlab.com/gitlab-org/gitlab-foss/blob/master/config/unicorn.rb.example
      # * omnibus: https://gitlab.com/gitlab-org/omnibus-gitlab/blob/master/files/gitlab-cookbooks/gitlab/templates/default/unicorn.rb.erb
      worker_processes 2
      working_directory "/srv/gitlab"
      listen "0.0.0.0:8080", :tcp_nopush => true
      timeout 60
      pid "/home/git/unicorn.pid"
      preload_app true

      require_relative "/srv/gitlab/lib/gitlab/cluster/lifecycle_events"

      before_exec do |server|
        # Signal application hooks that we're about to restart
        Gitlab::Cluster::LifecycleEvents.do_before_master_restart
      end

      run_once = true
      before_fork do |server, worker|
        if run_once
          # There is a difference between Puma and Unicorn:
          # - Puma calls before_fork once when booting up master process
          # - Unicorn runs before_fork whenever new work is spawned
          # To unify this behavior we call before_fork only once (we use
          # this callback for deleting Prometheus files so for our purposes
          # it makes sense to align behavior with Puma)
          run_once = false

          # Signal application hooks that we're about to fork
          Gitlab::Cluster::LifecycleEvents.do_before_fork
        end

        # The following is only recommended for memory/DB-constrained
        # installations.  It is not needed if your system can house
        # twice as many worker_processes as you have configured.
        #
        # This allows a new master process to incrementally
        # phase out the old master process with SIGTTOU to avoid a
        # thundering herd (especially in the "preload_app false" case)
        # when doing a transparent upgrade.  The last worker spawned
        # will then kill off the old master process with a SIGQUIT.
        old_pid = "#{server.config[:pid]}.oldbin"
        if old_pid != server.pid
          begin
            sig = (worker.nr + 1) >= server.worker_processes ? :QUIT : :TTOU
            Process.kill(sig, File.read(old_pid).to_i)
          rescue Errno::ENOENT, Errno::ESRCH
          end
        end
        #
        # Throttle the master from forking too quickly by sleeping.  Due
        # to the implementation of standard Unix signal handlers, this
        # helps (but does not completely) prevent identical, repeated signals
        # from being lost when the receiving process is busy.
        # sleep 1
      end

      after_fork do |server, worker|
        # Signal application hooks of worker start
        Gitlab::Cluster::LifecycleEvents.do_worker_start

        # per-process listener ports for debugging/admin/migrations
        # addr = "127.0.0.1:#{9293 + worker.nr}"
        # server.listen(addr, :tries => -1, :delay => 5, :tcp_nopush => true)
      end

      ENV['GITLAB_UNICORN_MEMORY_MIN'] = (700 * 1 << 20).to_s
      ENV['GITLAB_UNICORN_MEMORY_MAX'] = (1024 * 1 << 20).to_s
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: unicorn
      chart: unicorn-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-unicorn
    namespace: gitlab
    resourceVersion: "2016123"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-unicorn
    uid: 98096da1-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    test_login: |
      set -e
      echo 'Start Test'
      endpoint="https://gitlab.example.com"
      cookie_read="-c /tmp/test_login.cookie"
      cookie_readwrite="$cookie_read -b /tmp/test_login.cookie"

      signin_url="$endpoint/users/sign_in"
      echo "Login to create a session: $signin_url"
      csrf=$(curl $signin_url --fail -s $cookie_read | grep -Po '<meta.*name="csrf-token".*content="\K[a-zA-Z0-9\+=\-\/]*')
      curl --fail -X POST $signin_url -s $cookie_readwrite -F "authenticity_token=$csrf" -F 'user[login]=root' -F "user[password]=$(cat /initial_root_password)"

      profile_url="$endpoint/profile"
      echo "Confirm session valid: $profile_url"
      profile_status=$(curl -s -o /tmp/profile_output -w "%{http_code}" $cookie_readwrite $profile_url)

      if [ "$profile_status" != "200" ]; then
        echo "Error: Session Invalid"
        cat /tmp/profile_output
        exit 1
      fi

      echo 'Test Passed'
      exit 0
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    name: gitlab-unicorn-tests
    namespace: gitlab
    resourceVersion: "2016135"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-unicorn-tests
    uid: 98103e64-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    configure: |
      set -e
      mkdir -p /init-secrets-workhorse/gitlab-workhorse
      cp -v -r -L /init-config/gitlab-workhorse/secret /init-secrets-workhorse/gitlab-workhorse/secret
      mkdir -p /init-secrets-workhorse/redis
      cp -v -r -L /init-config/redis/password /init-secrets-workhorse/redis/
    installation_type: |
      gitlab-helm-chart
    workhorse-config.toml.erb: |
      [redis]
      URL = "redis://gitlab-redis:6379"
      Password = "<%= File.read("/etc/gitlab/redis/password").strip.dump[1..-2] %>"
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:43:50Z"
    labels:
      app: unicorn
      chart: unicorn-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-workhorse-config
    namespace: gitlab
    resourceVersion: "2016132"
    selfLink: /api/v1/namespaces/gitlab/configmaps/gitlab-workhorse-config
    uid: 980e67c1-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  kind: ConfigMap
  metadata:
    annotations:
      control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"gitlab-nginx-ingress-controller-7db754f856-c68sq","leaseDurationSeconds":30,"acquireTime":"2020-04-10T14:20:41Z","renewTime":"2020-04-10T23:54:07Z","leaderTransitions":43}'
    creationTimestamp: "2020-01-18T00:44:03Z"
    name: ingress-controller-leader-gitlab-nginx
    namespace: gitlab
    resourceVersion: "37202437"
    selfLink: /api/v1/namespaces/gitlab/configmaps/ingress-controller-leader-gitlab-nginx
    uid: a029393d-398b-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    node-autoscaler: |-
      {
        "calico-node": {
          "requests": {
            "cpu": {
              "base": "80m",
              "step": "20m",
              "nodesPerStep": 10,
              "max": "500m"
            }
          }
        }
      }
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-22T17:44:02Z"
    labels:
      addonmanager.kubernetes.io/mode: EnsureExists
      kubernetes.io/cluster-service: "true"
    name: calico-node-vertical-autoscaler
    namespace: kube-system
    resourceVersion: "4122738"
    selfLink: /api/v1/namespaces/kube-system/configmaps/calico-node-vertical-autoscaler
    uid: c6cd4b12-3d3e-11ea-96d3-42010a80017a
- apiVersion: v1
  data:
    ladder: |-
      {
        "coresToReplicas": [],
        "nodesToReplicas":
        [
          [1, 1],
          [10, 2],
          [100, 3],
          [250, 4],
          [500, 5],
          [1000, 6],
          [1500, 7],
          [2000, 8]
        ]
      }
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-22T17:44:02Z"
    labels:
      addonmanager.kubernetes.io/mode: EnsureExists
      kubernetes.io/cluster-service: "true"
    name: calico-typha-horizontal-autoscaler
    namespace: kube-system
    resourceVersion: "4122739"
    selfLink: /api/v1/namespaces/kube-system/configmaps/calico-typha-horizontal-autoscaler
    uid: c6d00f95-3d3e-11ea-96d3-42010a80017a
- apiVersion: v1
  data:
    typha-autoscaler: |-
      {
        "calico-typha": {
          "requests": {
            "cpu": {
              "base": "120m",
              "step": "80m",
              "nodesPerStep": 10,
              "max": "1000m"
            }
          }
        }
      }
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-22T17:44:02Z"
    labels:
      addonmanager.kubernetes.io/mode: EnsureExists
      kubernetes.io/cluster-service: "true"
    name: calico-typha-vertical-autoscaler
    namespace: kube-system
    resourceVersion: "4122740"
    selfLink: /api/v1/namespaces/kube-system/configmaps/calico-typha-vertical-autoscaler
    uid: c6d1ed97-3d3e-11ea-96d3-42010a80017a
- apiVersion: v1
  data:
    client-ca-file: |
      -----BEGIN CERTIFICATE-----
      MIIDDDCCAfSgAwIBAgIRAOfetxS79UigRCpuTvFn1/cwDQYJKoZIhvcNAQELBQAw
      LzEtMCsGA1UEAxMkZmY5YmY2MDUtMDlkNi00ZjBmLWEyZmYtZDgzMWQ1MDYwYWEz
      MB4XDTIwMDExMTA0NTgxOVoXDTI1MDEwOTA1NTgxOVowLzEtMCsGA1UEAxMkZmY5
      YmY2MDUtMDlkNi00ZjBmLWEyZmYtZDgzMWQ1MDYwYWEzMIIBIjANBgkqhkiG9w0B
      AQEFAAOCAQ8AMIIBCgKCAQEA3dlgasv+QXw3ElSeQFkzaJQtG8btdRW4gObyEOxq
      j8k/VZuROS6E69ODqfPq8WHoJmRpqj9ZUaRSCcUR21fLxlTs8rgRm5sLv/Gi1i+E
      3IVYJsVo7FZbmwixNRxWbEGfVJOKk8EW+URkdjX54T0Uwpo0gofB5/KekZgb4JDc
      Zi7wKsSGeZHBHRmUDkOvX38lpOre+EtQCXCjz2bv385Zf37FfGZaM0NcTDKLnt64
      7eW7UWJrxzi93D3AbBdr/oxg7E2XTaAd441av70zS4GwgwvYD+XiHq2jkTAJJtz+
      xF4r8wE9M8lptEesatdoznNhV0iXCDYsp2l5nbSW9QlU6wIDAQABoyMwITAOBgNV
      HQ8BAf8EBAMCAgQwDwYDVR0TAQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEA
      qqyCUA9qd5G37hYJf6+M1ZYCkOazJloMXECruO4cnHP96aVbab9AV2wbK2T6/Zss
      L1TVifPk3y9Eh5rDzPseckllZxNAoGHnUUOeuzUWe8GSSIt+MjscjN81FJqUbfNl
      ekJtabBt6LwgKRxBvnQRb3mxJs8hlD/OSqQD2NqqkY7Am/fr8UmXsmKV/vjjmAMF
      mx5VaS8gm86s2phGFpM0AlRDimI6kHFAGBvmSbqPJy5hpQO2TJYSnK3y+inZZYWm
      k20lViDNrXpk9MU463LrjhAquNsXveZwAkU1nK0ltpIsk5d4T8Yq9xSB0VVQ3IoU
      9DxWtmf3UCCGRzQ1yksMEA==
      -----END CERTIFICATE-----
    requestheader-allowed-names: '["aggregator"]'
    requestheader-client-ca-file: |
      -----BEGIN CERTIFICATE-----
      MIIDCzCCAfOgAwIBAgIQLoJ+DGjnKaFE6HSEc83XzzANBgkqhkiG9w0BAQsFADAv
      MS0wKwYDVQQDEyQxZTcwYWM4Ni05MWQwLTRhNGUtYmQyOC1kNTkzMTJlOWI4MTcw
      HhcNMjAwMTExMDQ1ODQyWhcNMjUwMTA5MDU1ODQyWjAvMS0wKwYDVQQDEyQxZTcw
      YWM4Ni05MWQwLTRhNGUtYmQyOC1kNTkzMTJlOWI4MTcwggEiMA0GCSqGSIb3DQEB
      AQUAA4IBDwAwggEKAoIBAQDwvBMk/7oUVWfw4svzNwwbY81q0TSXFaUU6cCIjbTu
      7SOF4e3K6HtBA9I+FM2vG5atmusVWIu/69opwc56mN5uwrbyumglXh67PJI9G1xC
      rVqI/nLdAmRickKw1lJhfkClq+J7pO3M1j6iSqAWs9Wh1Hlff+YTGWCEeHe0eG/r
      rR4U3yvIL/gmRWT+CxnACQjB2JFpQTCQfTdy+c/JT/j2D6cyb+QnhJkhfJ4R/vvK
      ufx5PFBQKNkWgIU7G/QcjSAjtCpdyH4ddkJ93Q7Bf4TW12UHXm8hN9tFFG6qTp31
      s3mJ1F6FKPSR7TNeDwmX3Xufpa6aOeI4yrf7GJVV39XLAgMBAAGjIzAhMA4GA1Ud
      DwEB/wQEAwICBDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAR
      5c86ugB77pyYMZc5Cz9/WeVLYgEGx9utU75AAbfAXHdWmnXifTtY7urHtd+kS0LN
      ECBDYLb+QVuQJjYvEkBMe0SfGqHifM6bcF7mZ28/bXj/buxcXFcirBc9aaFYVyLL
      ahpfdjpzr797KWnbQM60Xh2Qbj/MC0eAAe1ByBVY17ExQQdMyAbW1DuW6h+JTTXe
      CqZ1yhwP01rCrnjCvZXVvodNqQz1/elpL6pWITSo+o9YC0eQvcnB5l0hjP3Wg39H
      hNpysGIkqXvs9oSUTyvzuyNajJyZ9Fg0v8GxnQMhhTL+eVEGOxSL02iiNBgisnlC
      d1Q4MPAY9UZSuRx1Hshu
      -----END CERTIFICATE-----
    requestheader-extra-headers-prefix: '["X-Remote-Extra-"]'
    requestheader-group-headers: '["X-Remote-Group"]'
    requestheader-username-headers: '["X-Remote-User"]'
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-11T06:00:34Z"
    name: extension-apiserver-authentication
    namespace: kube-system
    resourceVersion: "35"
    selfLink: /api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication
    uid: aec951da-3437-11ea-9cdc-42010a8001cf
- apiVersion: v1
  data:
    containers.input.conf: |-
      # This configuration file for Fluentd is used
      # to watch changes to Docker log files that live in the
      # directory /var/lib/docker/containers/ and are symbolically
      # linked to from the /var/log/containers directory using names that capture the
      # pod name and container name. These logs are then submitted to
      # Google Cloud Logging which assumes the installation of the cloud-logging plug-in.
      #
      # Example
      # =======
      # A line in the Docker log file might look like this JSON:
      #
      # {"log":"2014/09/25 21:15:03 Got request with path wombat\\n",
      #  "stream":"stderr",
      #   "time":"2014-09-25T21:15:03.499185026Z"}
      #
      # The original tag is derived from the log file's location.
      # For example a Docker container's logs might be in the directory:
      #  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b
      # and in the file:
      #  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
      # where 997599971ee6... is the Docker ID of the running container.
      # The Kubernetes kubelet makes a symbolic link to this file on the host
      # machine in the /var/log/containers directory which includes the pod name,
      # the namespace name and the Kubernetes container name:
      #    synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
      #    ->
      #    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
      # The /var/log directory on the host is mapped to the /var/log directory in the container
      # running this instance of Fluentd and we end up collecting the file:
      #   /var/log/containers/synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
      # This results in the tag:
      #  var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
      # where 'synthetic-logger-0.25lps-pod' is the pod name, 'default' is the
      # namespace name, 'synth-lgr' is the container name and '997599971ee6..' is
      # the container ID.
      # The record reformer is used is discard the var.log.containers prefix and
      # the Docker container ID suffix and "kubernetes." is pre-pended giving the tag:
      #   kubernetes.synthetic-logger-0.25lps-pod_default_synth-lgr
      # Tag is then parsed by google_cloud plugin and translated to the metadata,
      # visible in the log viewer

      # Json Log Example:
      # {"log":"[info:2016-02-16T16:04:05.930-08:00] Some log text here\n","stream":"stdout","time":"2016-02-17T00:04:05.931087621Z"}
      # CRI Log Example:
      # 2016-02-17T00:04:05.931087621Z stdout F [info:2016-02-16T16:04:05.930-08:00] Some log text here
      <source>
        @type tail
        path /var/log/containers/*.log
        pos_file /var/log/gcp-containers.log.pos
        # Tags at this point are in the format of:
        # reform.var.log.containers.<POD_NAME>_<NAMESPACE_NAME>_<CONTAINER_NAME>-<CONTAINER_ID>.log
        tag reform.*
        read_from_head true
        <parse>
          @type multi_format
          <pattern>
            format json
            time_key time
            time_format %Y-%m-%dT%H:%M:%S.%NZ
          </pattern>
          <pattern>
            format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
            time_format %Y-%m-%dT%H:%M:%S.%N%:z
          </pattern>
        </parse>
      </source>

      <filter reform.**>
        @type parser
        format /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<log>.*)/
        reserve_data true
        suppress_parse_error_log true
        emit_invalid_record_to_error false
        key_name log
      </filter>

      <match reform.**>
        @type record_reformer
        enable_ruby true
        # Tags at this point are in the format of:
        # 'raw.kubernetes.<POD_NAME>_<NAMESPACE_NAME>_<CONTAINER_NAME>'.
        tag raw.kubernetes.${tag_suffix[4].split('-')[0..-2].join('-')}
      </match>

      # Detect exceptions in the log output and forward them as one log entry.
      <match raw.kubernetes.**>
        @type detect_exceptions

        remove_tag_prefix raw
        message log
        stream stream
        multiline_flush_interval 5
        max_bytes 500000
        max_lines 1000
      </match>
    monitoring.conf: |-
      # This source is used to acquire approximate process start timestamp,
      # which purpose is explained before the corresponding output plugin.
      <source>
        @type exec
        command /bin/sh -c 'date +%s'
        tag process_start
        time_format %Y-%m-%d %H:%M:%S
        keys process_start_timestamp
      </source>

      # This filter is used to convert process start timestamp to integer
      # value for correct ingestion in the prometheus output plugin.
      <filter process_start>
        @type record_transformer
        enable_ruby true
        auto_typecast true
        <record>
          process_start_timestamp ${record["process_start_timestamp"].to_i}
        </record>
      </filter>
    output.conf: |-
      # This match is placed before the all-matching output to provide metric
      # exporter with a process start timestamp for correct exporting of
      # cumulative metrics to Stackdriver.
      <match process_start>
        @type prometheus

        <metric>
          type gauge
          name process_start_time_seconds
          desc Timestamp of the process start in seconds
          key process_start_timestamp
        </metric>
      </match>

      # This filter allows to count the number of log entries read by fluentd
      # before they are processed by the output plugin. This in turn allows to
      # monitor the number of log entries that were read but never sent, e.g.
      # because of liveness probe removing buffer.
      <filter **>
        @type prometheus
        <metric>
          type counter
          name logging_entry_count
          desc Total number of log entries generated by either application containers or system components
        </metric>
      </filter>

      # TODO(instrumentation): Reconsider this workaround later.
      # Trim the entries which exceed slightly less than 100KB, to avoid
      # dropping them. It is a necessity, because Stackdriver only supports
      # entries that are up to 100KB in size.
      <filter kubernetes.**>
        @type record_transformer
        enable_ruby true
        <record>
          log ${record['log'].length > 100000 ? "[Trimmed]#{record['log'][0..100000]}..." : record['log']}
        </record>
      </filter>

      # Do not collect fluentd's own logs to avoid infinite loops.
      <match fluent.**>
        @type null
      </match>

      # We use 2 output stanzas - one to handle the container logs and one to handle
      # the node daemon logs, the latter of which explicitly sends its logs to the
      # compute.googleapis.com service rather than container.googleapis.com to keep
      # them separate since most users don't care about the node logs.
      <match kubernetes.**>
        @type google_cloud

        # Try to detect JSON formatted log entries.
        detect_json true
        # Collect metrics in Prometheus registry about plugin activity.
        enable_monitoring true
        monitoring_type prometheus
        # Allow log entries from multiple containers to be sent in the same request.
        split_logs_by_tag false
        # Set the buffer type to file to improve the reliability and reduce the memory consumption
        buffer_type file
        buffer_path /var/log/fluentd-buffers/kubernetes.containers.buffer
        # Set queue_full action to block because we want to pause gracefully
        # in case of the off-the-limits load instead of throwing an exception
        buffer_queue_full_action block
        # Set the chunk limit conservatively to avoid exceeding the recommended
        # chunk size of 5MB per write request.
        buffer_chunk_limit 1M
        # Cap the combined memory usage of this buffer and the one below to
        # 1MiB/chunk * (6 + 2) chunks = 8 MiB
        buffer_queue_limit 6
        # Never wait more than 5 seconds before flushing logs in the non-error case.
        flush_interval 5s
        # Never wait longer than 30 seconds between retries.
        max_retry_wait 30
        # Disable the limit on the number of retries (retry forever).
        disable_retry_limit
        # Use multiple threads for processing.
        num_threads 2
        use_grpc true
      </match>

      # Keep a smaller buffer here since these logs are less important than the user's
      # container logs.
      <match **>
        @type google_cloud

        detect_json true
        enable_monitoring true
        monitoring_type prometheus
        # Allow entries from multiple system logs to be sent in the same request.
        split_logs_by_tag false
        detect_subservice false
        buffer_type file
        buffer_path /var/log/fluentd-buffers/kubernetes.system.buffer
        buffer_queue_full_action block
        buffer_chunk_limit 1M
        buffer_queue_limit 2
        flush_interval 5s
        max_retry_wait 30
        disable_retry_limit
        num_threads 2
        use_grpc true
      </match>
    system.input.conf: |-
      # Example:
      # Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script /var/run/google.startup.script
      <source>
        @type tail
        format syslog
        path /var/log/startupscript.log
        pos_file /var/log/gcp-startupscript.log.pos
        tag startupscript
      </source>

      # Examples:
      # time="2016-02-04T06:51:03.053580605Z" level=info msg="GET /containers/json"
      # time="2016-02-04T07:53:57.505612354Z" level=error msg="HTTP Error" err="No such image: -f" statusCode=404
      # TODO(random-liu): Remove this after cri container runtime rolls out.
      <source>
        @type tail
        format /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
        path /var/log/docker.log
        pos_file /var/log/gcp-docker.log.pos
        tag docker
      </source>

      # Example:
      # 2016/02/04 06:52:38 filePurge: successfully removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal
      <source>
        @type tail
        # Not parsing this, because it doesn't have anything particularly useful to
        # parse out of it (like severities).
        format none
        path /var/log/etcd.log
        pos_file /var/log/gcp-etcd.log.pos
        tag etcd
      </source>

      # Multi-line parsing is required for all the kube logs because very large log
      # statements, such as those that include entire object bodies, get split into
      # multiple lines by glog.

      # Example:
      # I0204 07:32:30.020537    3368 server.go:1048] POST /stats/container/: (13.972191ms) 200 [[Go-http-client/1.1] 10.244.1.3:40537]
      <source>
        @type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/kubelet.log
        pos_file /var/log/gcp-kubelet.log.pos
        tag kubelet
      </source>

      # Example:
      # I1118 21:26:53.975789       6 proxier.go:1096] Port "nodePort for kube-system/default-http-backend:http" (:31429/tcp) was open before and is still needed
      <source>
        @type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/kube-proxy.log
        pos_file /var/log/gcp-kube-proxy.log.pos
        tag kube-proxy
      </source>

      # Example:
      # I0204 07:00:19.604280       5 handlers.go:131] GET /api/v1/nodes: (1.624207ms) 200 [[kube-controller-manager/v1.1.3 (linux/amd64) kubernetes/6a81b50] 127.0.0.1:38266]
      <source>
        @type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/kube-apiserver.log
        pos_file /var/log/gcp-kube-apiserver.log.pos
        tag kube-apiserver
      </source>

      # Example:
      # I0204 06:55:31.872680       5 servicecontroller.go:277] LB already exists and doesn't need update for service kube-system/kube-ui
      <source>
        @type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/kube-controller-manager.log
        pos_file /var/log/gcp-kube-controller-manager.log.pos
        tag kube-controller-manager
      </source>

      # Example:
      # W0204 06:49:18.239674       7 reflector.go:245] pkg/scheduler/factory/factory.go:193: watch of *api.Service ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [2578313/2577886]) [2579312]
      <source>
        @type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/kube-scheduler.log
        pos_file /var/log/gcp-kube-scheduler.log.pos
        tag kube-scheduler
      </source>

      # Example:
      # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
      <source>
        @type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/glbc.log
        pos_file /var/log/gcp-glbc.log.pos
        tag glbc
      </source>

      # Example:
      # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
      <source>
        @type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/cluster-autoscaler.log
        pos_file /var/log/gcp-cluster-autoscaler.log.pos
        tag cluster-autoscaler
      </source>

      # Logs from systemd-journal for interesting services.
      # TODO(random-liu): Keep this for compatibility, remove this after
      # cri container runtime rolls out.
      <source>
        @type systemd
        filters [{ "_SYSTEMD_UNIT": "docker.service" }]
        pos_file /var/log/gcp-journald-docker.pos
        read_from_head true
        tag docker
      </source>

      <source>
        @type systemd
        filters [{ "_SYSTEMD_UNIT": "containerd.service" }]
        pos_file /var/log/gcp-journald-container-runtime.pos
        read_from_head true
        tag container-runtime
      </source>

      <source>
        @type systemd
        filters [{ "_SYSTEMD_UNIT": "kubelet.service" }]
        pos_file /var/log/gcp-journald-kubelet.pos
        read_from_head true
        tag kubelet
      </source>

      <source>
        @type systemd
        filters [{ "_SYSTEMD_UNIT": "node-problem-detector.service" }]
        pos_file /var/log/gcp-journald-node-problem-detector.pos
        read_from_head true
        tag node-problem-detector
      </source>
  kind: ConfigMap
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","data":{"containers.input.conf":"# This configuration file for Fluentd is used\n# to watch changes to Docker log files that live in the\n# directory /var/lib/docker/containers/ and are symbolically\n# linked to from the /var/log/containers directory using names that capture the\n# pod name and container name. These logs are then submitted to\n# Google Cloud Logging which assumes the installation of the cloud-logging plug-in.\n#\n# Example\n# =======\n# A line in the Docker log file might look like this JSON:\n#\n# {\"log\":\"2014/09/25 21:15:03 Got request with path wombat\\\\n\",\n#  \"stream\":\"stderr\",\n#   \"time\":\"2014-09-25T21:15:03.499185026Z\"}\n#\n# The original tag is derived from the log file's location.\n# For example a Docker container's logs might be in the directory:\n#  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b\n# and in the file:\n#  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log\n# where 997599971ee6... is the Docker ID of the running container.\n# The Kubernetes kubelet makes a symbolic link to this file on the host\n# machine in the /var/log/containers directory which includes the pod name,\n# the namespace name and the Kubernetes container name:\n#    synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n#    -\u003e\n#    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log\n# The /var/log directory on the host is mapped to the /var/log directory in the container\n# running this instance of Fluentd and we end up collecting the file:\n#   /var/log/containers/synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n# This results in the tag:\n#  var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n# where 'synthetic-logger-0.25lps-pod' is the pod name, 'default' is the\n# namespace name, 'synth-lgr' is the container name and '997599971ee6..' is\n# the container ID.\n# The record reformer is used is discard the var.log.containers prefix and\n# the Docker container ID suffix and \"kubernetes.\" is pre-pended giving the tag:\n#   kubernetes.synthetic-logger-0.25lps-pod_default_synth-lgr\n# Tag is then parsed by google_cloud plugin and translated to the metadata,\n# visible in the log viewer\n\n# Json Log Example:\n# {\"log\":\"[info:2016-02-16T16:04:05.930-08:00] Some log text here\\n\",\"stream\":\"stdout\",\"time\":\"2016-02-17T00:04:05.931087621Z\"}\n# CRI Log Example:\n# 2016-02-17T00:04:05.931087621Z stdout F [info:2016-02-16T16:04:05.930-08:00] Some log text here\n\u003csource\u003e\n  @type tail\n  path /var/log/containers/*.log\n  pos_file /var/log/gcp-containers.log.pos\n  # Tags at this point are in the format of:\n  # reform.var.log.containers.\u003cPOD_NAME\u003e_\u003cNAMESPACE_NAME\u003e_\u003cCONTAINER_NAME\u003e-\u003cCONTAINER_ID\u003e.log\n  tag reform.*\n  read_from_head true\n  \u003cparse\u003e\n    @type multi_format\n    \u003cpattern\u003e\n      format json\n      time_key time\n      time_format %Y-%m-%dT%H:%M:%S.%NZ\n    \u003c/pattern\u003e\n    \u003cpattern\u003e\n      format /^(?\u003ctime\u003e.+) (?\u003cstream\u003estdout|stderr) [^ ]* (?\u003clog\u003e.*)$/\n      time_format %Y-%m-%dT%H:%M:%S.%N%:z\n    \u003c/pattern\u003e\n  \u003c/parse\u003e\n\u003c/source\u003e\n\n\u003cfilter reform.**\u003e\n  @type parser\n  format /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003clog\u003e.*)/\n  reserve_data true\n  suppress_parse_error_log true\n  emit_invalid_record_to_error false\n  key_name log\n\u003c/filter\u003e\n\n\u003cmatch reform.**\u003e\n  @type record_reformer\n  enable_ruby true\n  # Tags at this point are in the format of:\n  # 'raw.kubernetes.\u003cPOD_NAME\u003e_\u003cNAMESPACE_NAME\u003e_\u003cCONTAINER_NAME\u003e'.\n  tag raw.kubernetes.${tag_suffix[4].split('-')[0..-2].join('-')}\n\u003c/match\u003e\n\n# Detect exceptions in the log output and forward them as one log entry.\n\u003cmatch raw.kubernetes.**\u003e\n  @type detect_exceptions\n\n  remove_tag_prefix raw\n  message log\n  stream stream\n  multiline_flush_interval 5\n  max_bytes 500000\n  max_lines 1000\n\u003c/match\u003e","monitoring.conf":"# This source is used to acquire approximate process start timestamp,\n# which purpose is explained before the corresponding output plugin.\n\u003csource\u003e\n  @type exec\n  command /bin/sh -c 'date +%s'\n  tag process_start\n  time_format %Y-%m-%d %H:%M:%S\n  keys process_start_timestamp\n\u003c/source\u003e\n\n# This filter is used to convert process start timestamp to integer\n# value for correct ingestion in the prometheus output plugin.\n\u003cfilter process_start\u003e\n  @type record_transformer\n  enable_ruby true\n  auto_typecast true\n  \u003crecord\u003e\n    process_start_timestamp ${record[\"process_start_timestamp\"].to_i}\n  \u003c/record\u003e\n\u003c/filter\u003e","output.conf":"# This match is placed before the all-matching output to provide metric\n# exporter with a process start timestamp for correct exporting of\n# cumulative metrics to Stackdriver.\n\u003cmatch process_start\u003e\n  @type prometheus\n\n  \u003cmetric\u003e\n    type gauge\n    name process_start_time_seconds\n    desc Timestamp of the process start in seconds\n    key process_start_timestamp\n  \u003c/metric\u003e\n\u003c/match\u003e\n\n# This filter allows to count the number of log entries read by fluentd\n# before they are processed by the output plugin. This in turn allows to\n# monitor the number of log entries that were read but never sent, e.g.\n# because of liveness probe removing buffer.\n\u003cfilter **\u003e\n  @type prometheus\n  \u003cmetric\u003e\n    type counter\n    name logging_entry_count\n    desc Total number of log entries generated by either application containers or system components\n  \u003c/metric\u003e\n\u003c/filter\u003e\n\n# TODO(instrumentation): Reconsider this workaround later.\n# Trim the entries which exceed slightly less than 100KB, to avoid\n# dropping them. It is a necessity, because Stackdriver only supports\n# entries that are up to 100KB in size.\n\u003cfilter kubernetes.**\u003e\n  @type record_transformer\n  enable_ruby true\n  \u003crecord\u003e\n    log ${record['log'].length \u003e 100000 ? \"[Trimmed]#{record['log'][0..100000]}...\" : record['log']}\n  \u003c/record\u003e\n\u003c/filter\u003e\n\n# Do not collect fluentd's own logs to avoid infinite loops.\n\u003cmatch fluent.**\u003e\n  @type null\n\u003c/match\u003e\n\n# We use 2 output stanzas - one to handle the container logs and one to handle\n# the node daemon logs, the latter of which explicitly sends its logs to the\n# compute.googleapis.com service rather than container.googleapis.com to keep\n# them separate since most users don't care about the node logs.\n\u003cmatch kubernetes.**\u003e\n  @type google_cloud\n\n  # Try to detect JSON formatted log entries.\n  detect_json true\n  # Collect metrics in Prometheus registry about plugin activity.\n  enable_monitoring true\n  monitoring_type prometheus\n  # Allow log entries from multiple containers to be sent in the same request.\n  split_logs_by_tag false\n  # Set the buffer type to file to improve the reliability and reduce the memory consumption\n  buffer_type file\n  buffer_path /var/log/fluentd-buffers/kubernetes.containers.buffer\n  # Set queue_full action to block because we want to pause gracefully\n  # in case of the off-the-limits load instead of throwing an exception\n  buffer_queue_full_action block\n  # Set the chunk limit conservatively to avoid exceeding the recommended\n  # chunk size of 5MB per write request.\n  buffer_chunk_limit 1M\n  # Cap the combined memory usage of this buffer and the one below to\n  # 1MiB/chunk * (6 + 2) chunks = 8 MiB\n  buffer_queue_limit 6\n  # Never wait more than 5 seconds before flushing logs in the non-error case.\n  flush_interval 5s\n  # Never wait longer than 30 seconds between retries.\n  max_retry_wait 30\n  # Disable the limit on the number of retries (retry forever).\n  disable_retry_limit\n  # Use multiple threads for processing.\n  num_threads 2\n  use_grpc true\n\u003c/match\u003e\n\n# Keep a smaller buffer here since these logs are less important than the user's\n# container logs.\n\u003cmatch **\u003e\n  @type google_cloud\n\n  detect_json true\n  enable_monitoring true\n  monitoring_type prometheus\n  # Allow entries from multiple system logs to be sent in the same request.\n  split_logs_by_tag false\n  detect_subservice false\n  buffer_type file\n  buffer_path /var/log/fluentd-buffers/kubernetes.system.buffer\n  buffer_queue_full_action block\n  buffer_chunk_limit 1M\n  buffer_queue_limit 2\n  flush_interval 5s\n  max_retry_wait 30\n  disable_retry_limit\n  num_threads 2\n  use_grpc true\n\u003c/match\u003e","system.input.conf":"# Example:\n# Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script /var/run/google.startup.script\n\u003csource\u003e\n  @type tail\n  format syslog\n  path /var/log/startupscript.log\n  pos_file /var/log/gcp-startupscript.log.pos\n  tag startupscript\n\u003c/source\u003e\n\n# Examples:\n# time=\"2016-02-04T06:51:03.053580605Z\" level=info msg=\"GET /containers/json\"\n# time=\"2016-02-04T07:53:57.505612354Z\" level=error msg=\"HTTP Error\" err=\"No such image: -f\" statusCode=404\n# TODO(random-liu): Remove this after cri container runtime rolls out.\n\u003csource\u003e\n  @type tail\n  format /^time=\"(?\u003ctime\u003e[^)]*)\" level=(?\u003cseverity\u003e[^ ]*) msg=\"(?\u003cmessage\u003e[^\"]*)\"( err=\"(?\u003cerror\u003e[^\"]*)\")?( statusCode=($\u003cstatus_code\u003e\\d+))?/\n  path /var/log/docker.log\n  pos_file /var/log/gcp-docker.log.pos\n  tag docker\n\u003c/source\u003e\n\n# Example:\n# 2016/02/04 06:52:38 filePurge: successfully removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal\n\u003csource\u003e\n  @type tail\n  # Not parsing this, because it doesn't have anything particularly useful to\n  # parse out of it (like severities).\n  format none\n  path /var/log/etcd.log\n  pos_file /var/log/gcp-etcd.log.pos\n  tag etcd\n\u003c/source\u003e\n\n# Multi-line parsing is required for all the kube logs because very large log\n# statements, such as those that include entire object bodies, get split into\n# multiple lines by glog.\n\n# Example:\n# I0204 07:32:30.020537    3368 server.go:1048] POST /stats/container/: (13.972191ms) 200 [[Go-http-client/1.1] 10.244.1.3:40537]\n\u003csource\u003e\n  @type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/kubelet.log\n  pos_file /var/log/gcp-kubelet.log.pos\n  tag kubelet\n\u003c/source\u003e\n\n# Example:\n# I1118 21:26:53.975789       6 proxier.go:1096] Port \"nodePort for kube-system/default-http-backend:http\" (:31429/tcp) was open before and is still needed\n\u003csource\u003e\n  @type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/kube-proxy.log\n  pos_file /var/log/gcp-kube-proxy.log.pos\n  tag kube-proxy\n\u003c/source\u003e\n\n# Example:\n# I0204 07:00:19.604280       5 handlers.go:131] GET /api/v1/nodes: (1.624207ms) 200 [[kube-controller-manager/v1.1.3 (linux/amd64) kubernetes/6a81b50] 127.0.0.1:38266]\n\u003csource\u003e\n  @type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/kube-apiserver.log\n  pos_file /var/log/gcp-kube-apiserver.log.pos\n  tag kube-apiserver\n\u003c/source\u003e\n\n# Example:\n# I0204 06:55:31.872680       5 servicecontroller.go:277] LB already exists and doesn't need update for service kube-system/kube-ui\n\u003csource\u003e\n  @type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/kube-controller-manager.log\n  pos_file /var/log/gcp-kube-controller-manager.log.pos\n  tag kube-controller-manager\n\u003c/source\u003e\n\n# Example:\n# W0204 06:49:18.239674       7 reflector.go:245] pkg/scheduler/factory/factory.go:193: watch of *api.Service ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [2578313/2577886]) [2579312]\n\u003csource\u003e\n  @type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/kube-scheduler.log\n  pos_file /var/log/gcp-kube-scheduler.log.pos\n  tag kube-scheduler\n\u003c/source\u003e\n\n# Example:\n# I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf\n\u003csource\u003e\n  @type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/glbc.log\n  pos_file /var/log/gcp-glbc.log.pos\n  tag glbc\n\u003c/source\u003e\n\n# Example:\n# I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf\n\u003csource\u003e\n  @type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/cluster-autoscaler.log\n  pos_file /var/log/gcp-cluster-autoscaler.log.pos\n  tag cluster-autoscaler\n\u003c/source\u003e\n\n# Logs from systemd-journal for interesting services.\n# TODO(random-liu): Keep this for compatibility, remove this after\n# cri container runtime rolls out.\n\u003csource\u003e\n  @type systemd\n  filters [{ \"_SYSTEMD_UNIT\": \"docker.service\" }]\n  pos_file /var/log/gcp-journald-docker.pos\n  read_from_head true\n  tag docker\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type systemd\n  filters [{ \"_SYSTEMD_UNIT\": \"containerd.service\" }]\n  pos_file /var/log/gcp-journald-container-runtime.pos\n  read_from_head true\n  tag container-runtime\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type systemd\n  filters [{ \"_SYSTEMD_UNIT\": \"kubelet.service\" }]\n  pos_file /var/log/gcp-journald-kubelet.pos\n  read_from_head true\n  tag kubelet\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type systemd\n  filters [{ \"_SYSTEMD_UNIT\": \"node-problem-detector.service\" }]\n  pos_file /var/log/gcp-journald-node-problem-detector.pos\n  read_from_head true\n  tag node-problem-detector\n\u003c/source\u003e"},"kind":"ConfigMap","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile"},"name":"fluentd-gcp-config-old-v1.2.5","namespace":"kube-system"}}
    creationTimestamp: "2020-01-11T06:01:13Z"
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
    name: fluentd-gcp-config-old-v1.2.5
    namespace: kube-system
    resourceVersion: "473"
    selfLink: /api/v1/namespaces/kube-system/configmaps/fluentd-gcp-config-old-v1.2.5
    uid: c60b78c8-3437-11ea-9cdc-42010a8001cf
- apiVersion: v1
  data:
    containers.input.conf: |-
      # This configuration file for Fluentd is used
      # to watch changes to Docker log files that live in the
      # directory /var/lib/docker/containers/ and are symbolically
      # linked to from the /var/log/containers directory using names that capture the
      # pod name and container name. These logs are then submitted to
      # Google Cloud Logging which assumes the installation of the cloud-logging plug-in.
      #
      # Example
      # =======
      # A line in the Docker log file might look like this JSON:
      #
      # {"log":"2014/09/25 21:15:03 Got request with path wombat\\n",
      #  "stream":"stderr",
      #   "time":"2014-09-25T21:15:03.499185026Z"}
      #
      # The original tag is derived from the log file's location.
      # For example a Docker container's logs might be in the directory:
      #  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b
      # and in the file:
      #  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
      # where 997599971ee6... is the Docker ID of the running container.
      # The Kubernetes kubelet makes a symbolic link to this file on the host
      # machine in the /var/log/containers directory which includes the pod name,
      # the namespace name and the Kubernetes container name:
      #    synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
      #    ->
      #    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
      # The /var/log directory on the host is mapped to the /var/log directory in the container
      # running this instance of Fluentd and we end up collecting the file:
      #   /var/log/containers/synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
      # This results in the tag:
      #  var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
      # where 'synthetic-logger-0.25lps-pod' is the pod name, 'default' is the
      # namespace name, 'synth-lgr' is the container name and '997599971ee6..' is
      # the container ID.
      # The record reformer is used to extract pod_name, namespace_name and
      # container_name from the tag and set them in a local_resource_id in the
      # format of:
      # 'k8s_container.<NAMESPACE_NAME>.<POD_NAME>.<CONTAINER_NAME>'.
      # The reformer also changes the tags to 'stderr' or 'stdout' based on the
      # value of 'stream'.
      # local_resource_id is later used by google_cloud plugin to determine the
      # monitored resource to ingest logs against.

      # Json Log Example:
      # {"log":"[info:2016-02-16T16:04:05.930-08:00] Some log text here\n","stream":"stdout","time":"2016-02-17T00:04:05.931087621Z"}
      # CRI Log Example:
      # 2016-02-17T00:04:05.931087621Z stdout F [info:2016-02-16T16:04:05.930-08:00] Some log text here
      <source>
        @type tail
        path /var/log/containers/*.log
        pos_file /var/log/gcp-containers.log.pos
        # Tags at this point are in the format of:
        # reform.var.log.containers.<POD_NAME>_<NAMESPACE_NAME>_<CONTAINER_NAME>-<CONTAINER_ID>.log
        tag reform.*
        read_from_head true
        <parse>
          @type multi_format
          <pattern>
            format json
            time_key time
            time_format %Y-%m-%dT%H:%M:%S.%NZ
          </pattern>
          <pattern>
            format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
            time_format %Y-%m-%dT%H:%M:%S.%N%:z
          </pattern>
        </parse>
      </source>

      <filter reform.**>
        @type parser
        format /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<log>.*)/
        reserve_data true
        suppress_parse_error_log true
        emit_invalid_record_to_error false
        key_name log
      </filter>

      <match reform.**>
        @type record_reformer
        enable_ruby true
        <record>
          # Extract local_resource_id from tag for 'k8s_container' monitored
          # resource. The format is:
          # 'k8s_container.<namespace_name>.<pod_name>.<container_name>'.
          "logging.googleapis.com/local_resource_id" ${"k8s_container.#{tag_suffix[4].rpartition('.')[0].split('_')[1]}.#{tag_suffix[4].rpartition('.')[0].split('_')[0]}.#{tag_suffix[4].rpartition('.')[0].split('_')[2].rpartition('-')[0]}"}
          # Rename the field 'log' to a more generic field 'message'. This way the
          # fluent-plugin-google-cloud knows to flatten the field as textPayload
          # instead of jsonPayload after extracting 'time', 'severity' and
          # 'stream' from the record.
          message ${record['log']}
          # If 'severity' is not set, assume stderr is ERROR and stdout is INFO.
          severity ${record['severity'] || if record['stream'] == 'stderr' then 'ERROR' else 'INFO' end}
        </record>
        tag ${if record['stream'] == 'stderr' then 'raw.stderr' else 'raw.stdout' end}
        remove_keys stream,log
      </match>

      # Detect exceptions in the log output and forward them as one log entry.
      <match {raw.stderr,raw.stdout}>
        @type detect_exceptions

        remove_tag_prefix raw
        message message
        stream "logging.googleapis.com/local_resource_id"
        multiline_flush_interval 5
        max_bytes 500000
        max_lines 1000
      </match>
    monitoring.conf: |-
      # This source is used to acquire approximate process start timestamp,
      # which purpose is explained before the corresponding output plugin.
      <source>
        @type exec
        command /bin/sh -c 'date +%s'
        tag process_start
        time_format %Y-%m-%d %H:%M:%S
        keys process_start_timestamp
      </source>

      # This filter is used to convert process start timestamp to integer
      # value for correct ingestion in the prometheus output plugin.
      <filter process_start>
        @type record_transformer
        enable_ruby true
        auto_typecast true
        <record>
          process_start_timestamp ${record["process_start_timestamp"].to_i}
        </record>
      </filter>
    output.conf: |-
      # This match is placed before the all-matching output to provide metric
      # exporter with a process start timestamp for correct exporting of
      # cumulative metrics to Stackdriver.
      <match process_start>
        @type prometheus

        <metric>
          type gauge
          name process_start_time_seconds
          desc Timestamp of the process start in seconds
          key process_start_timestamp
        </metric>
      </match>

      # This filter allows to count the number of log entries read by fluentd
      # before they are processed by the output plugin. This in turn allows to
      # monitor the number of log entries that were read but never sent, e.g.
      # because of liveness probe removing buffer.
      <filter **>
        @type prometheus
        <metric>
          type counter
          name logging_entry_count
          desc Total number of log entries generated by either application containers or system components
        </metric>
      </filter>

      # This section is exclusive for k8s_container logs. Those come with
      # 'stderr'/'stdout' tags.
      # TODO(instrumentation): Reconsider this workaround later.
      # Trim the entries which exceed slightly less than 100KB, to avoid
      # dropping them. It is a necessity, because Stackdriver only supports
      # entries that are up to 100KB in size.
      <filter {stderr,stdout}>
        @type record_transformer
        enable_ruby true
        <record>
          message ${record['message'].length > 100000 ? "[Trimmed]#{record['message'][0..100000]}..." : record['message']}
        </record>
      </filter>

      # Do not collect fluentd's own logs to avoid infinite loops.
      <match fluent.**>
        @type null
      </match>

      # Add a unique insertId to each log entry that doesn't already have it.
      # This helps guarantee the order and prevent log duplication.
      <filter **>
        @type add_insert_ids
      </filter>

      # This section is exclusive for k8s_container logs. These logs come with
      # 'stderr'/'stdout' tags.
      # We use a separate output stanza for 'k8s_node' logs with a smaller buffer
      # because node logs are less important than user's container logs.
      <match {stderr,stdout}>
        @type google_cloud

        # Try to detect JSON formatted log entries.
        detect_json true
        # Collect metrics in Prometheus registry about plugin activity.
        enable_monitoring true
        monitoring_type prometheus
        # Allow log entries from multiple containers to be sent in the same request.
        split_logs_by_tag false
        # Set the buffer type to file to improve the reliability and reduce the memory consumption
        buffer_type file
        buffer_path /var/log/fluentd-buffers/kubernetes.containers.buffer
        # Set queue_full action to block because we want to pause gracefully
        # in case of the off-the-limits load instead of throwing an exception
        buffer_queue_full_action block
        # Set the chunk limit conservatively to avoid exceeding the recommended
        # chunk size of 5MB per write request.
        buffer_chunk_limit 512k
        # Cap the combined memory usage of this buffer and the one below to
        # 512KiB/chunk * (6 + 2) chunks = 4 MiB
        buffer_queue_limit 6
        # Never wait more than 5 seconds before flushing logs in the non-error case.
        flush_interval 5s
        # Never wait longer than 30 seconds between retries.
        max_retry_wait 30
        # Disable the limit on the number of retries (retry forever).
        disable_retry_limit
        # Use multiple threads for processing.
        num_threads 2
        use_grpc true
        # Skip timestamp adjustment as this is in a controlled environment with
        # known timestamp format. This helps with CPU usage.
        adjust_invalid_timestamps false
      </match>

      # Attach local_resource_id for 'k8s_node' monitored resource.
      <filter **>
        @type record_transformer
        enable_ruby true
        <record>
          "logging.googleapis.com/local_resource_id" ${"k8s_node.#{ENV['NODE_NAME']}"}
        </record>
      </filter>

      # This section is exclusive for 'k8s_node' logs. These logs come with tags
      # that are neither 'stderr' or 'stdout'.
      # We use a separate output stanza for 'k8s_container' logs with a larger
      # buffer because user's container logs are more important than node logs.
      <match **>
        @type google_cloud

        detect_json true
        enable_monitoring true
        monitoring_type prometheus
        # Allow entries from multiple system logs to be sent in the same request.
        split_logs_by_tag false
        detect_subservice false
        buffer_type file
        buffer_path /var/log/fluentd-buffers/kubernetes.system.buffer
        buffer_queue_full_action block
        buffer_chunk_limit 512k
        buffer_queue_limit 2
        flush_interval 5s
        max_retry_wait 30
        disable_retry_limit
        num_threads 2
        use_grpc true
        # Skip timestamp adjustment as this is in a controlled environment with
        # known timestamp format. This helps with CPU usage.
        adjust_invalid_timestamps false
      </match>
    system.input.conf: |-
      # Example:
      # Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script /var/run/google.startup.script
      <source>
        @type tail
        format syslog
        path /var/log/startupscript.log
        pos_file /var/log/gcp-startupscript.log.pos
        tag startupscript
      </source>

      # Examples:
      # time="2016-02-04T06:51:03.053580605Z" level=info msg="GET /containers/json"
      # time="2016-02-04T07:53:57.505612354Z" level=error msg="HTTP Error" err="No such image: -f" statusCode=404
      # TODO(random-liu): Remove this after cri container runtime rolls out.
      <source>
        @type tail
        format /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
        path /var/log/docker.log
        pos_file /var/log/gcp-docker.log.pos
        tag docker
      </source>

      # Example:
      # 2016/02/04 06:52:38 filePurge: successfully removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal
      <source>
        @type tail
        # Not parsing this, because it doesn't have anything particularly useful to
        # parse out of it (like severities).
        format none
        path /var/log/etcd.log
        pos_file /var/log/gcp-etcd.log.pos
        tag etcd
      </source>

      # Multi-line parsing is required for all the kube logs because very large log
      # statements, such as those that include entire object bodies, get split into
      # multiple lines by glog.

      # Example:
      # I0204 07:32:30.020537    3368 server.go:1048] POST /stats/container/: (13.972191ms) 200 [[Go-http-client/1.1] 10.244.1.3:40537]
      <source>
        @type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/kubelet.log
        pos_file /var/log/gcp-kubelet.log.pos
        tag kubelet
      </source>

      # Example:
      # I1118 21:26:53.975789       6 proxier.go:1096] Port "nodePort for kube-system/default-http-backend:http" (:31429/tcp) was open before and is still needed
      <source>
        @type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/kube-proxy.log
        pos_file /var/log/gcp-kube-proxy.log.pos
        tag kube-proxy
      </source>

      # Example:
      # I0204 07:00:19.604280       5 handlers.go:131] GET /api/v1/nodes: (1.624207ms) 200 [[kube-controller-manager/v1.1.3 (linux/amd64) kubernetes/6a81b50] 127.0.0.1:38266]
      <source>
        @type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/kube-apiserver.log
        pos_file /var/log/gcp-kube-apiserver.log.pos
        tag kube-apiserver
      </source>

      # Example:
      # I0204 06:55:31.872680       5 servicecontroller.go:277] LB already exists and doesn't need update for service kube-system/kube-ui
      <source>
        @type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/kube-controller-manager.log
        pos_file /var/log/gcp-kube-controller-manager.log.pos
        tag kube-controller-manager
      </source>

      # Example:
      # W0204 06:49:18.239674       7 reflector.go:245] pkg/scheduler/factory/factory.go:193: watch of *api.Service ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [2578313/2577886]) [2579312]
      <source>
        @type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/kube-scheduler.log
        pos_file /var/log/gcp-kube-scheduler.log.pos
        tag kube-scheduler
      </source>

      # Example:
      # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
      <source>
        @type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/glbc.log
        pos_file /var/log/gcp-glbc.log.pos
        tag glbc
      </source>

      # Example:
      # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
      <source>
        @type tail
        format multiline
        multiline_flush_interval 5s
        format_firstline /^\w\d{4}/
        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
        time_format %m%d %H:%M:%S.%N
        path /var/log/cluster-autoscaler.log
        pos_file /var/log/gcp-cluster-autoscaler.log.pos
        tag cluster-autoscaler
      </source>

      # Logs from systemd-journal for interesting services.
      # TODO(random-liu): Keep this for compatibility, remove this after
      # cri container runtime rolls out.
      <source>
        @type systemd
        filters [{ "_SYSTEMD_UNIT": "docker.service" }]
        pos_file /var/log/gcp-journald-docker.pos
        read_from_head true
        tag docker
      </source>

      <source>
        @type systemd
        filters [{ "_SYSTEMD_UNIT": "{{ fluentd_container_runtime_service }}.service" }]
        pos_file /var/log/gcp-journald-container-runtime.pos
        read_from_head true
        tag container-runtime
      </source>

      <source>
        @type systemd
        filters [{ "_SYSTEMD_UNIT": "kubelet.service" }]
        pos_file /var/log/gcp-journald-kubelet.pos
        read_from_head true
        tag kubelet
      </source>

      <source>
        @type systemd
        filters [{ "_SYSTEMD_UNIT": "node-problem-detector.service" }]
        pos_file /var/log/gcp-journald-node-problem-detector.pos
        read_from_head true
        tag node-problem-detector
      </source>

      # BEGIN_NODE_JOURNAL
      # Whether to include node-journal or not is determined when starting the
      # cluster. It is not changed when the cluster is already running.
      <source>
        @type systemd
        pos_file /var/log/gcp-journald.pos
        read_from_head true
        tag node-journal
      </source>

      <filter node-journal>
        @type grep
        <exclude>
          key _SYSTEMD_UNIT
          pattern ^(docker|{{ fluentd_container_runtime_service }}|kubelet|node-problem-detector)\.service$
        </exclude>
      </filter>
      # END_NODE_JOURNAL
  kind: ConfigMap
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","data":{"containers.input.conf":"# This configuration file for Fluentd is used\n# to watch changes to Docker log files that live in the\n# directory /var/lib/docker/containers/ and are symbolically\n# linked to from the /var/log/containers directory using names that capture the\n# pod name and container name. These logs are then submitted to\n# Google Cloud Logging which assumes the installation of the cloud-logging plug-in.\n#\n# Example\n# =======\n# A line in the Docker log file might look like this JSON:\n#\n# {\"log\":\"2014/09/25 21:15:03 Got request with path wombat\\\\n\",\n#  \"stream\":\"stderr\",\n#   \"time\":\"2014-09-25T21:15:03.499185026Z\"}\n#\n# The original tag is derived from the log file's location.\n# For example a Docker container's logs might be in the directory:\n#  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b\n# and in the file:\n#  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log\n# where 997599971ee6... is the Docker ID of the running container.\n# The Kubernetes kubelet makes a symbolic link to this file on the host\n# machine in the /var/log/containers directory which includes the pod name,\n# the namespace name and the Kubernetes container name:\n#    synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n#    -\u003e\n#    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log\n# The /var/log directory on the host is mapped to the /var/log directory in the container\n# running this instance of Fluentd and we end up collecting the file:\n#   /var/log/containers/synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n# This results in the tag:\n#  var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log\n# where 'synthetic-logger-0.25lps-pod' is the pod name, 'default' is the\n# namespace name, 'synth-lgr' is the container name and '997599971ee6..' is\n# the container ID.\n# The record reformer is used to extract pod_name, namespace_name and\n# container_name from the tag and set them in a local_resource_id in the\n# format of:\n# 'k8s_container.\u003cNAMESPACE_NAME\u003e.\u003cPOD_NAME\u003e.\u003cCONTAINER_NAME\u003e'.\n# The reformer also changes the tags to 'stderr' or 'stdout' based on the\n# value of 'stream'.\n# local_resource_id is later used by google_cloud plugin to determine the\n# monitored resource to ingest logs against.\n\n# Json Log Example:\n# {\"log\":\"[info:2016-02-16T16:04:05.930-08:00] Some log text here\\n\",\"stream\":\"stdout\",\"time\":\"2016-02-17T00:04:05.931087621Z\"}\n# CRI Log Example:\n# 2016-02-17T00:04:05.931087621Z stdout F [info:2016-02-16T16:04:05.930-08:00] Some log text here\n\u003csource\u003e\n  @type tail\n  path /var/log/containers/*.log\n  pos_file /var/log/gcp-containers.log.pos\n  # Tags at this point are in the format of:\n  # reform.var.log.containers.\u003cPOD_NAME\u003e_\u003cNAMESPACE_NAME\u003e_\u003cCONTAINER_NAME\u003e-\u003cCONTAINER_ID\u003e.log\n  tag reform.*\n  read_from_head true\n  \u003cparse\u003e\n    @type multi_format\n    \u003cpattern\u003e\n      format json\n      time_key time\n      time_format %Y-%m-%dT%H:%M:%S.%NZ\n    \u003c/pattern\u003e\n    \u003cpattern\u003e\n      format /^(?\u003ctime\u003e.+) (?\u003cstream\u003estdout|stderr) [^ ]* (?\u003clog\u003e.*)$/\n      time_format %Y-%m-%dT%H:%M:%S.%N%:z\n    \u003c/pattern\u003e\n  \u003c/parse\u003e\n\u003c/source\u003e\n\n\u003cfilter reform.**\u003e\n  @type parser\n  format /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003clog\u003e.*)/\n  reserve_data true\n  suppress_parse_error_log true\n  emit_invalid_record_to_error false\n  key_name log\n\u003c/filter\u003e\n\n\u003cmatch reform.**\u003e\n  @type record_reformer\n  enable_ruby true\n  \u003crecord\u003e\n    # Extract local_resource_id from tag for 'k8s_container' monitored\n    # resource. The format is:\n    # 'k8s_container.\u003cnamespace_name\u003e.\u003cpod_name\u003e.\u003ccontainer_name\u003e'.\n    \"logging.googleapis.com/local_resource_id\" ${\"k8s_container.#{tag_suffix[4].rpartition('.')[0].split('_')[1]}.#{tag_suffix[4].rpartition('.')[0].split('_')[0]}.#{tag_suffix[4].rpartition('.')[0].split('_')[2].rpartition('-')[0]}\"}\n    # Rename the field 'log' to a more generic field 'message'. This way the\n    # fluent-plugin-google-cloud knows to flatten the field as textPayload\n    # instead of jsonPayload after extracting 'time', 'severity' and\n    # 'stream' from the record.\n    message ${record['log']}\n    # If 'severity' is not set, assume stderr is ERROR and stdout is INFO.\n    severity ${record['severity'] || if record['stream'] == 'stderr' then 'ERROR' else 'INFO' end}\n  \u003c/record\u003e\n  tag ${if record['stream'] == 'stderr' then 'raw.stderr' else 'raw.stdout' end}\n  remove_keys stream,log\n\u003c/match\u003e\n\n# Detect exceptions in the log output and forward them as one log entry.\n\u003cmatch {raw.stderr,raw.stdout}\u003e\n  @type detect_exceptions\n\n  remove_tag_prefix raw\n  message message\n  stream \"logging.googleapis.com/local_resource_id\"\n  multiline_flush_interval 5\n  max_bytes 500000\n  max_lines 1000\n\u003c/match\u003e","monitoring.conf":"# This source is used to acquire approximate process start timestamp,\n# which purpose is explained before the corresponding output plugin.\n\u003csource\u003e\n  @type exec\n  command /bin/sh -c 'date +%s'\n  tag process_start\n  time_format %Y-%m-%d %H:%M:%S\n  keys process_start_timestamp\n\u003c/source\u003e\n\n# This filter is used to convert process start timestamp to integer\n# value for correct ingestion in the prometheus output plugin.\n\u003cfilter process_start\u003e\n  @type record_transformer\n  enable_ruby true\n  auto_typecast true\n  \u003crecord\u003e\n    process_start_timestamp ${record[\"process_start_timestamp\"].to_i}\n  \u003c/record\u003e\n\u003c/filter\u003e","output.conf":"# This match is placed before the all-matching output to provide metric\n# exporter with a process start timestamp for correct exporting of\n# cumulative metrics to Stackdriver.\n\u003cmatch process_start\u003e\n  @type prometheus\n\n  \u003cmetric\u003e\n    type gauge\n    name process_start_time_seconds\n    desc Timestamp of the process start in seconds\n    key process_start_timestamp\n  \u003c/metric\u003e\n\u003c/match\u003e\n\n# This filter allows to count the number of log entries read by fluentd\n# before they are processed by the output plugin. This in turn allows to\n# monitor the number of log entries that were read but never sent, e.g.\n# because of liveness probe removing buffer.\n\u003cfilter **\u003e\n  @type prometheus\n  \u003cmetric\u003e\n    type counter\n    name logging_entry_count\n    desc Total number of log entries generated by either application containers or system components\n  \u003c/metric\u003e\n\u003c/filter\u003e\n\n# This section is exclusive for k8s_container logs. Those come with\n# 'stderr'/'stdout' tags.\n# TODO(instrumentation): Reconsider this workaround later.\n# Trim the entries which exceed slightly less than 100KB, to avoid\n# dropping them. It is a necessity, because Stackdriver only supports\n# entries that are up to 100KB in size.\n\u003cfilter {stderr,stdout}\u003e\n  @type record_transformer\n  enable_ruby true\n  \u003crecord\u003e\n    message ${record['message'].length \u003e 100000 ? \"[Trimmed]#{record['message'][0..100000]}...\" : record['message']}\n  \u003c/record\u003e\n\u003c/filter\u003e\n\n# Do not collect fluentd's own logs to avoid infinite loops.\n\u003cmatch fluent.**\u003e\n  @type null\n\u003c/match\u003e\n\n# Add a unique insertId to each log entry that doesn't already have it.\n# This helps guarantee the order and prevent log duplication.\n\u003cfilter **\u003e\n  @type add_insert_ids\n\u003c/filter\u003e\n\n# This section is exclusive for k8s_container logs. These logs come with\n# 'stderr'/'stdout' tags.\n# We use a separate output stanza for 'k8s_node' logs with a smaller buffer\n# because node logs are less important than user's container logs.\n\u003cmatch {stderr,stdout}\u003e\n  @type google_cloud\n\n  # Try to detect JSON formatted log entries.\n  detect_json true\n  # Collect metrics in Prometheus registry about plugin activity.\n  enable_monitoring true\n  monitoring_type prometheus\n  # Allow log entries from multiple containers to be sent in the same request.\n  split_logs_by_tag false\n  # Set the buffer type to file to improve the reliability and reduce the memory consumption\n  buffer_type file\n  buffer_path /var/log/fluentd-buffers/kubernetes.containers.buffer\n  # Set queue_full action to block because we want to pause gracefully\n  # in case of the off-the-limits load instead of throwing an exception\n  buffer_queue_full_action block\n  # Set the chunk limit conservatively to avoid exceeding the recommended\n  # chunk size of 5MB per write request.\n  buffer_chunk_limit 512k\n  # Cap the combined memory usage of this buffer and the one below to\n  # 512KiB/chunk * (6 + 2) chunks = 4 MiB\n  buffer_queue_limit 6\n  # Never wait more than 5 seconds before flushing logs in the non-error case.\n  flush_interval 5s\n  # Never wait longer than 30 seconds between retries.\n  max_retry_wait 30\n  # Disable the limit on the number of retries (retry forever).\n  disable_retry_limit\n  # Use multiple threads for processing.\n  num_threads 2\n  use_grpc true\n  # Skip timestamp adjustment as this is in a controlled environment with\n  # known timestamp format. This helps with CPU usage.\n  adjust_invalid_timestamps false\n\u003c/match\u003e\n\n# Attach local_resource_id for 'k8s_node' monitored resource.\n\u003cfilter **\u003e\n  @type record_transformer\n  enable_ruby true\n  \u003crecord\u003e\n    \"logging.googleapis.com/local_resource_id\" ${\"k8s_node.#{ENV['NODE_NAME']}\"}\n  \u003c/record\u003e\n\u003c/filter\u003e\n\n# This section is exclusive for 'k8s_node' logs. These logs come with tags\n# that are neither 'stderr' or 'stdout'.\n# We use a separate output stanza for 'k8s_container' logs with a larger\n# buffer because user's container logs are more important than node logs.\n\u003cmatch **\u003e\n  @type google_cloud\n\n  detect_json true\n  enable_monitoring true\n  monitoring_type prometheus\n  # Allow entries from multiple system logs to be sent in the same request.\n  split_logs_by_tag false\n  detect_subservice false\n  buffer_type file\n  buffer_path /var/log/fluentd-buffers/kubernetes.system.buffer\n  buffer_queue_full_action block\n  buffer_chunk_limit 512k\n  buffer_queue_limit 2\n  flush_interval 5s\n  max_retry_wait 30\n  disable_retry_limit\n  num_threads 2\n  use_grpc true\n  # Skip timestamp adjustment as this is in a controlled environment with\n  # known timestamp format. This helps with CPU usage.\n  adjust_invalid_timestamps false\n\u003c/match\u003e","system.input.conf":"# Example:\n# Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script /var/run/google.startup.script\n\u003csource\u003e\n  @type tail\n  format syslog\n  path /var/log/startupscript.log\n  pos_file /var/log/gcp-startupscript.log.pos\n  tag startupscript\n\u003c/source\u003e\n\n# Examples:\n# time=\"2016-02-04T06:51:03.053580605Z\" level=info msg=\"GET /containers/json\"\n# time=\"2016-02-04T07:53:57.505612354Z\" level=error msg=\"HTTP Error\" err=\"No such image: -f\" statusCode=404\n# TODO(random-liu): Remove this after cri container runtime rolls out.\n\u003csource\u003e\n  @type tail\n  format /^time=\"(?\u003ctime\u003e[^)]*)\" level=(?\u003cseverity\u003e[^ ]*) msg=\"(?\u003cmessage\u003e[^\"]*)\"( err=\"(?\u003cerror\u003e[^\"]*)\")?( statusCode=($\u003cstatus_code\u003e\\d+))?/\n  path /var/log/docker.log\n  pos_file /var/log/gcp-docker.log.pos\n  tag docker\n\u003c/source\u003e\n\n# Example:\n# 2016/02/04 06:52:38 filePurge: successfully removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal\n\u003csource\u003e\n  @type tail\n  # Not parsing this, because it doesn't have anything particularly useful to\n  # parse out of it (like severities).\n  format none\n  path /var/log/etcd.log\n  pos_file /var/log/gcp-etcd.log.pos\n  tag etcd\n\u003c/source\u003e\n\n# Multi-line parsing is required for all the kube logs because very large log\n# statements, such as those that include entire object bodies, get split into\n# multiple lines by glog.\n\n# Example:\n# I0204 07:32:30.020537    3368 server.go:1048] POST /stats/container/: (13.972191ms) 200 [[Go-http-client/1.1] 10.244.1.3:40537]\n\u003csource\u003e\n  @type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/kubelet.log\n  pos_file /var/log/gcp-kubelet.log.pos\n  tag kubelet\n\u003c/source\u003e\n\n# Example:\n# I1118 21:26:53.975789       6 proxier.go:1096] Port \"nodePort for kube-system/default-http-backend:http\" (:31429/tcp) was open before and is still needed\n\u003csource\u003e\n  @type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/kube-proxy.log\n  pos_file /var/log/gcp-kube-proxy.log.pos\n  tag kube-proxy\n\u003c/source\u003e\n\n# Example:\n# I0204 07:00:19.604280       5 handlers.go:131] GET /api/v1/nodes: (1.624207ms) 200 [[kube-controller-manager/v1.1.3 (linux/amd64) kubernetes/6a81b50] 127.0.0.1:38266]\n\u003csource\u003e\n  @type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/kube-apiserver.log\n  pos_file /var/log/gcp-kube-apiserver.log.pos\n  tag kube-apiserver\n\u003c/source\u003e\n\n# Example:\n# I0204 06:55:31.872680       5 servicecontroller.go:277] LB already exists and doesn't need update for service kube-system/kube-ui\n\u003csource\u003e\n  @type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/kube-controller-manager.log\n  pos_file /var/log/gcp-kube-controller-manager.log.pos\n  tag kube-controller-manager\n\u003c/source\u003e\n\n# Example:\n# W0204 06:49:18.239674       7 reflector.go:245] pkg/scheduler/factory/factory.go:193: watch of *api.Service ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [2578313/2577886]) [2579312]\n\u003csource\u003e\n  @type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/kube-scheduler.log\n  pos_file /var/log/gcp-kube-scheduler.log.pos\n  tag kube-scheduler\n\u003c/source\u003e\n\n# Example:\n# I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf\n\u003csource\u003e\n  @type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/glbc.log\n  pos_file /var/log/gcp-glbc.log.pos\n  tag glbc\n\u003c/source\u003e\n\n# Example:\n# I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf\n\u003csource\u003e\n  @type tail\n  format multiline\n  multiline_flush_interval 5s\n  format_firstline /^\\w\\d{4}/\n  format1 /^(?\u003cseverity\u003e\\w)(?\u003ctime\u003e\\d{4} [^\\s]*)\\s+(?\u003cpid\u003e\\d+)\\s+(?\u003csource\u003e[^ \\]]+)\\] (?\u003cmessage\u003e.*)/\n  time_format %m%d %H:%M:%S.%N\n  path /var/log/cluster-autoscaler.log\n  pos_file /var/log/gcp-cluster-autoscaler.log.pos\n  tag cluster-autoscaler\n\u003c/source\u003e\n\n# Logs from systemd-journal for interesting services.\n# TODO(random-liu): Keep this for compatibility, remove this after\n# cri container runtime rolls out.\n\u003csource\u003e\n  @type systemd\n  filters [{ \"_SYSTEMD_UNIT\": \"docker.service\" }]\n  pos_file /var/log/gcp-journald-docker.pos\n  read_from_head true\n  tag docker\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type systemd\n  filters [{ \"_SYSTEMD_UNIT\": \"{{ fluentd_container_runtime_service }}.service\" }]\n  pos_file /var/log/gcp-journald-container-runtime.pos\n  read_from_head true\n  tag container-runtime\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type systemd\n  filters [{ \"_SYSTEMD_UNIT\": \"kubelet.service\" }]\n  pos_file /var/log/gcp-journald-kubelet.pos\n  read_from_head true\n  tag kubelet\n\u003c/source\u003e\n\n\u003csource\u003e\n  @type systemd\n  filters [{ \"_SYSTEMD_UNIT\": \"node-problem-detector.service\" }]\n  pos_file /var/log/gcp-journald-node-problem-detector.pos\n  read_from_head true\n  tag node-problem-detector\n\u003c/source\u003e\n\n# BEGIN_NODE_JOURNAL\n# Whether to include node-journal or not is determined when starting the\n# cluster. It is not changed when the cluster is already running.\n\u003csource\u003e\n  @type systemd\n  pos_file /var/log/gcp-journald.pos\n  read_from_head true\n  tag node-journal\n\u003c/source\u003e\n\n\u003cfilter node-journal\u003e\n  @type grep\n  \u003cexclude\u003e\n    key _SYSTEMD_UNIT\n    pattern ^(docker|{{ fluentd_container_runtime_service }}|kubelet|node-problem-detector)\\.service$\n  \u003c/exclude\u003e\n\u003c/filter\u003e\n# END_NODE_JOURNAL"},"kind":"ConfigMap","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile"},"name":"fluentd-gcp-config-v1.2.5","namespace":"kube-system"}}
    creationTimestamp: "2020-01-11T06:01:13Z"
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
    name: fluentd-gcp-config-v1.2.5
    namespace: kube-system
    resourceVersion: "474"
    selfLink: /api/v1/namespaces/kube-system/configmaps/fluentd-gcp-config-v1.2.5
    uid: c60ea81b-3437-11ea-9cdc-42010a8001cf
- apiVersion: v1
  kind: ConfigMap
  metadata:
    annotations:
      control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"gke-f005108ec19aa6d238be-199c-4e3f-vm_e9039","leaseDurationSeconds":15,"acquireTime":"2020-04-02T13:50:28Z","renewTime":"2020-04-10T23:54:11Z","leaderTransitions":11}'
    creationTimestamp: "2020-01-11T06:00:42Z"
    name: gke-common-webhook-lock
    namespace: kube-system
    resourceVersion: "37202447"
    selfLink: /api/v1/namespaces/kube-system/configmaps/gke-common-webhook-lock
    uid: b370ebe1-3437-11ea-9cdc-42010a8001cf
- apiVersion: v1
  data:
    NannyConfiguration: |-
      apiVersion: nannyconfig/v1alpha1
      kind: NannyConfiguration
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-11T06:01:00Z"
    labels:
      addonmanager.kubernetes.io/mode: EnsureExists
      kubernetes.io/cluster-service: "true"
    name: heapster-config
    namespace: kube-system
    resourceVersion: "296"
    selfLink: /api/v1/namespaces/kube-system/configmaps/heapster-config
    uid: be5c9b85-3437-11ea-9cdc-42010a8001cf
- apiVersion: v1
  kind: ConfigMap
  metadata:
    annotations:
      control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"gke-f005108ec19aa6d238be-199c-4e3f-vm_4d3ba","leaseDurationSeconds":15,"acquireTime":"2020-04-02T13:50:30Z","renewTime":"2020-04-10T23:54:11Z","leaderTransitions":12}'
    creationTimestamp: "2020-01-11T06:01:04Z"
    name: ingress-gce-lock
    namespace: kube-system
    resourceVersion: "37202446"
    selfLink: /api/v1/namespaces/kube-system/configmaps/ingress-gce-lock
    uid: c02f9e07-3437-11ea-9cdc-42010a8001cf
- apiVersion: v1
  data:
    provider-uid: ab64a690ed0b76be
    uid: ab64a690ed0b76be
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-11T06:00:35Z"
    name: ingress-uid
    namespace: kube-system
    resourceVersion: "92"
    selfLink: /api/v1/namespaces/kube-system/configmaps/ingress-uid
    uid: af53c428-3437-11ea-9cdc-42010a8001cf
- apiVersion: v1
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-11T06:01:00Z"
    labels:
      addonmanager.kubernetes.io/mode: EnsureExists
    name: kube-dns
    namespace: kube-system
    resourceVersion: "294"
    selfLink: /api/v1/namespaces/kube-system/configmaps/kube-dns
    uid: be0f8b17-3437-11ea-9cdc-42010a8001cf
- apiVersion: v1
  data:
    linear: '{"coresPerReplica":256,"nodesPerReplica":16,"preventSinglePointFailure":true}'
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-11T06:01:32Z"
    name: kube-dns-autoscaler
    namespace: kube-system
    resourceVersion: "713"
    selfLink: /api/v1/namespaces/kube-system/configmaps/kube-dns-autoscaler
    uid: d1238036-3437-11ea-9cdc-42010a8001cf
- apiVersion: v1
  data:
    "1": '{"Key":{"Namespace":"kubevious","Name":"kubevious-certificate"},"Value":{"ExcludedFromSLO":false,"SoftDeleted":false,"SslCertificateName":"mcrt-c2644db5-4590-42be-8d97-544f25635953","SslCertificateBindingReported":true,"SslCertificateCreationReported":true}}'
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-28T21:17:23Z"
    name: managed-certificate-config
    namespace: kube-system
    resourceVersion: "31987851"
    selfLink: /api/v1/namespaces/kube-system/configmaps/managed-certificate-config
    uid: 9351998f-4213-11ea-96d3-42010a80017a
- apiVersion: v1
  data:
    NannyConfiguration: |-
      apiVersion: nannyconfig/v1alpha1
      kind: NannyConfiguration
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-11T06:01:00Z"
    labels:
      addonmanager.kubernetes.io/mode: EnsureExists
      kubernetes.io/cluster-service: "true"
    name: metrics-server-config
    namespace: kube-system
    resourceVersion: "297"
    selfLink: /api/v1/namespaces/kube-system/configmaps/metrics-server-config
    uid: be5f35a6-3437-11ea-9cdc-42010a8001cf
- apiVersion: v1
  data:
    MYSQL_HOST: kubevious-mysql-0.kubevious-mysql-svc.kubevious.svc.cluster.local
    MYSQL_PORT: "3306"
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-03-27T23:06:15Z"
    name: kubevious-mysql-client
    namespace: kubevious
    resourceVersion: "31987745"
    selfLink: /api/v1/namespaces/kubevious/configmaps/kubevious-mysql-client
    uid: 8f5c105f-707f-11ea-8ebf-42010a800207
- apiVersion: v1
  data:
    master.cnf: |
      # Apply this config only on the master.
      [mysqld]
      log-bin
      expire_logs_days=3
    slave.cnf: |
      # Apply this config only on slaves.
      [mysqld]
      super-read-only
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-03-27T23:06:15Z"
    name: kubevious-mysql-conf
    namespace: kubevious
    resourceVersion: "31987744"
    selfLink: /api/v1/namespaces/kubevious/configmaps/kubevious-mysql-conf
    uid: 8f5bdaf8-707f-11ea-8ebf-42010a800207
- apiVersion: v1
  data:
    initdb.sql: |
      USE kubevious;
      SET NAMES utf8mb4;

      SET GLOBAL expire_logs_days = 0;
      SET GLOBAL binlog_expire_logs_seconds = 259200;

      CREATE TABLE IF NOT EXISTS `config` (
        `key` varchar(64) NOT NULL DEFAULT '',
        `value` json NOT NULL,
        PRIMARY KEY (`key`)
      ) ENGINE=InnoDB DEFAULT CHARSET=latin1;

      CREATE TABLE IF NOT EXISTS `config_hashes` (
        `key` BINARY(32) NOT NULL DEFAULT '',
        `value` json NOT NULL,
        PRIMARY KEY (`key`)
      ) ENGINE=InnoDB DEFAULT CHARSET=latin1;

      CREATE TABLE IF NOT EXISTS `snapshots` (
        `id` int unsigned NOT NULL AUTO_INCREMENT,
        `date` datetime NOT NULL,
        PRIMARY KEY (`id`),
        KEY `date` (`date`)
      ) ENGINE=InnoDB DEFAULT CHARSET=latin1;

      CREATE TABLE IF NOT EXISTS `snap_items` (
        `id` int unsigned NOT NULL AUTO_INCREMENT,
        `snapshot_id` int unsigned NOT NULL,
        `dn` varchar(1024) NOT NULL DEFAULT '',
        `kind` varchar(128) NOT NULL DEFAULT '',
        `config_kind` varchar(128) NOT NULL DEFAULT '',
        `name` varchar(128) NULL DEFAULT '',
        `config_hash` BINARY(32) NOT NULL,
        PRIMARY KEY (`id`),
        KEY `snapshot_id` (`snapshot_id`),
        KEY `dn` (`dn`),
        KEY `kind` (`kind`),
        KEY `config_kind` (`config_kind`),
        CONSTRAINT `snap_item_snapshot_id` FOREIGN KEY (`snapshot_id`) REFERENCES `snapshots` (`id`),
        CONSTRAINT `snap_item_config_hash` FOREIGN KEY (`config_hash`) REFERENCES `config_hashes` (`key`)
      ) ENGINE=InnoDB DEFAULT CHARSET=latin1;

      CREATE TABLE IF NOT EXISTS `diffs` (
        `id` int unsigned NOT NULL AUTO_INCREMENT,
        `snapshot_id` int unsigned NOT NULL,
        `date` datetime NOT NULL,
        `in_snapshot` tinyint(1) NOT NULL,
        `summary` json NOT NULL,
        PRIMARY KEY (`id`),
        KEY `snapshot_id` (`snapshot_id`),
        KEY `date` (`date`),
        CONSTRAINT `diff_snapshot_id` FOREIGN KEY (`snapshot_id`) REFERENCES `snapshots` (`id`)
      ) ENGINE=InnoDB DEFAULT CHARSET=latin1;

      CREATE TABLE IF NOT EXISTS `diff_items` (
        `id` int unsigned NOT NULL AUTO_INCREMENT,
        `diff_id` int unsigned NOT NULL,
        `dn` varchar(1024) NOT NULL DEFAULT '',
        `kind` varchar(128) NOT NULL DEFAULT '',
        `config_kind` varchar(128) NOT NULL DEFAULT '',
        `name` varchar(128) NULL DEFAULT '',
        `present` tinyint(1) NOT NULL,
        `config_hash` BINARY(32) NULL,
        PRIMARY KEY (`id`),
        KEY `diff_id` (`diff_id`),
        KEY `dn` (`dn`),
        KEY `kind` (`kind`),
        KEY `config_kind` (`config_kind`),
        CONSTRAINT `diff_item_diff_id` FOREIGN KEY (`diff_id`) REFERENCES `diffs` (`id`),
        CONSTRAINT `diff_item_config_hash` FOREIGN KEY (`config_hash`) REFERENCES `config_hashes` (`key`)
      ) ENGINE=InnoDB DEFAULT CHARSET=latin1;

      INSERT IGNORE INTO `config`(`key`, `value`)
      VALUES('DB_SCHEMA', '{ "version": 2 }')
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-03-27T23:06:15Z"
    name: kubevious-mysql-init-script
    namespace: kubevious
    resourceVersion: "31987746"
    selfLink: /api/v1/namespaces/kubevious/configmaps/kubevious-mysql-init-script
    uid: 8f5c6db7-707f-11ea-8ebf-42010a800207
- apiVersion: v1
  data:
    head.html: |-
      <!-- Google Tag Manager -->
      <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-5Q25Q6P');</script>
      <!-- End Google Tag Manager -->
  kind: ConfigMap
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","data":{"head.html":"\u003c!-- Google Tag Manager --\u003e\n\u003cscript\u003e(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\nnew Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\nj=d.createElement(s),dl=l!='dataLayer'?'\u0026l='+l:'';j.async=true;j.src=\n'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);\n})(window,document,'script','dataLayer','GTM-5Q25Q6P');\u003c/script\u003e\n\u003c!-- End Google Tag Manager --\u003e"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"kubevious-ui-header","namespace":"kubevious"}}
    creationTimestamp: "2020-03-27T23:06:05Z"
    name: kubevious-ui-header
    namespace: kubevious
    resourceVersion: "31987688"
    selfLink: /api/v1/namespaces/kubevious/configmaps/kubevious-ui-header
    uid: 894b2812-707f-11ea-8ebf-42010a800207
- apiVersion: v1
  data:
    alertmanager.yml: |
      route:
        group_by: ['alertname', 'cluster', 'service']
        group_wait: 5s
        group_interval: 10s
        repeat_interval: 30s
        receiver: scale-up
        routes:
        - match:
            service: gateway
            receiver: scale-up
            severity: major

      inhibit_rules:
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        equal: ['alertname', 'cluster', 'service']

      receivers:
      - name: 'scale-up'
        webhook_configs:
          - url: http://gateway.openfaas:8080/system/alert
            send_resolved: true
            http_config:
              basic_auth:
                username: admin
                password_file: /var/secrets/basic-auth-password
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:05:48Z"
    labels:
      app: openfaas
      chart: openfaas-5.4.1
      component: alertmanager-config
      heritage: Helm
      release: openfaas
    name: alertmanager-config
    namespace: openfaas
    resourceVersion: "2007013"
    selfLink: /api/v1/namespaces/openfaas/configmaps/alertmanager-config
    uid: 48044016-3986-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    alert.rules.yml: |
      groups:
        - name: openfaas
          rules:
          - alert: service_down
            expr: up == 0
          - alert: APIHighInvocationRate
            expr: sum(rate(gateway_function_invocation_total{code="200"}[10s])) BY (function_name) > 5
            for: 5s
            labels:
              service: gateway
              severity: major
            annotations:
              description: High invocation total on "{{$labels.function_name}}"
              summary: High invocation total on "{{$labels.function_name}}"
    prometheus.yml: |
      global:
        scrape_interval:     15s
        evaluation_interval: 15s
        external_labels:
            monitor: 'faas-monitor'

      rule_files:
          - 'alert.rules.yml'

      scrape_configs:
        - job_name: 'prometheus'
          scrape_interval: 5s
          static_configs:
            - targets: ['localhost:9090']

        - job_name: 'kubernetes-pods'
          scrape_interval: 5s
          honor_labels: false
          kubernetes_sd_configs:
            - role: pod
              namespaces:
                names:
                  - openfaas
          relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__

      alerting:
        alertmanagers:
        - static_configs:
          - targets:
            - alertmanager:9093
  kind: ConfigMap
  metadata:
    creationTimestamp: "2020-01-18T00:05:48Z"
    labels:
      app: openfaas
      chart: openfaas-5.4.1
      component: prometheus-config
      heritage: Helm
      release: openfaas
    name: prometheus-config
    namespace: openfaas
    resourceVersion: "2007012"
    selfLink: /api/v1/namespaces/openfaas/configmaps/prometheus-config
    uid: 4803a1e8-3986-11ea-b115-42010a8001d6
- apiVersion: v1
  data:
    config.yaml: |
      resources:
        cpuRequestsMissing: warning
        cpuLimitsMissing: warning
        memoryRequestsMissing: warning
        memoryLimitsMissing: warning
      images:
        tagNotSpecified: error
      healthChecks:
        readinessProbeMissing: warning
        livenessProbeMissing: warning
      networking:
        hostNetworkSet: warning
        hostPortSet: warning
      security:
        hostIPCSet: error
        hostPIDSet: error
        notReadOnlyRootFileSystem: warning
        privilegeEscalationAllowed: error
        runAsRootAllowed: warning
        runAsPrivileged: error
        capabilities:
          error:
            ifAnyAdded:
              - SYS_ADMIN
              - NET_ADMIN
              - ALL
          warning:
            ifAnyAddedBeyond:
              - CHOWN
              - DAC_OVERRIDE
              - FSETID
              - FOWNER
              - MKNOD
              - NET_RAW
              - SETGID
              - SETUID
              - SETFCAP
              - SETPCAP
              - NET_BIND_SERVICE
              - SYS_CHROOT
              - KILL
              - AUDIT_WRITE
      controllers_to_scan:
        - Deployments
        - StatefulSets
        - DaemonSets
        - Jobs
        - CronJobs
        - ReplicationControllers
  kind: ConfigMap
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","data":{"config.yaml":"resources:\n  cpuRequestsMissing: warning\n  cpuLimitsMissing: warning\n  memoryRequestsMissing: warning\n  memoryLimitsMissing: warning\nimages:\n  tagNotSpecified: error\nhealthChecks:\n  readinessProbeMissing: warning\n  livenessProbeMissing: warning\nnetworking:\n  hostNetworkSet: warning\n  hostPortSet: warning\nsecurity:\n  hostIPCSet: error\n  hostPIDSet: error\n  notReadOnlyRootFileSystem: warning\n  privilegeEscalationAllowed: error\n  runAsRootAllowed: warning\n  runAsPrivileged: error\n  capabilities:\n    error:\n      ifAnyAdded:\n        - SYS_ADMIN\n        - NET_ADMIN\n        - ALL\n    warning:\n      ifAnyAddedBeyond:\n        - CHOWN\n        - DAC_OVERRIDE\n        - FSETID\n        - FOWNER\n        - MKNOD\n        - NET_RAW\n        - SETGID\n        - SETUID\n        - SETFCAP\n        - SETPCAP\n        - NET_BIND_SERVICE\n        - SYS_CHROOT\n        - KILL\n        - AUDIT_WRITE\ncontrollers_to_scan:\n  - Deployments\n  - StatefulSets\n  - DaemonSets\n  - Jobs\n  - CronJobs\n  - ReplicationControllers\n"},"kind":"ConfigMap","metadata":{"annotations":{},"labels":{"app":"polaris"},"name":"polaris","namespace":"polaris"}}
    creationTimestamp: "2020-02-27T04:07:08Z"
    labels:
      app: polaris
    name: polaris
    namespace: polaris
    resourceVersion: "19359237"
    selfLink: /api/v1/namespaces/polaris/configmaps/polaris
    uid: 9f39fc94-5916-11ea-8ab4-42010a8000a7
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
