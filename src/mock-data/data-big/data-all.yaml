apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.90/32
    creationTimestamp: "2020-03-31T18:40:21Z"
    generateName: gitlab-cainjector-5d757b9fdd-
    labels:
      app: cainjector
      app.kubernetes.io/instance: gitlab
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      helm.sh/chart: cainjector-v0.10.1
      pod-template-hash: 5d757b9fdd
    name: gitlab-cainjector-5d757b9fdd-zd7sf
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-cainjector-5d757b9fdd
      uid: 99716a60-398b-11ea-b115-42010a8001d6
    resourceVersion: "37202421"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-cainjector-5d757b9fdd-zd7sf
    uid: 13c62e8a-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - args:
      - --v=2
      - --leader-election-namespace=$(POD_NAMESPACE)
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-cainjector:v0.10.1
      imagePullPolicy: IfNotPresent
      name: cainjector
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: gitlab-cainjector-token-j4ppg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: gitlab-cainjector
    serviceAccountName: gitlab-cainjector
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: gitlab-cainjector-token-j4ppg
      secret:
        defaultMode: 420
        secretName: gitlab-cainjector-token-j4ppg
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:21Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T23:54:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T23:54:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:21Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://a493fd739ba642cb541961a0fce8f9225a523d29f7cecce10c29fe880cac209f
      image: quay.io/jetstack/cert-manager-cainjector:v0.10.1
      imageID: docker-pullable://quay.io/jetstack/cert-manager-cainjector@sha256:aaa0d125234ccb2ccab729f7a553dd10c90b9079c25c56263aca80effab6d958
      lastState:
        terminated:
          containerID: docker://a344d48064ee7ea5aca48c3f1f679e87bae874a571dfddad9c0c7db741088364
          exitCode: 255
          finishedAt: "2020-04-10T23:50:26Z"
          reason: Error
          startedAt: "2020-04-10T23:44:43Z"
      name: cainjector
      ready: true
      restartCount: 1208
      state:
        running:
          startedAt: "2020-04-10T23:54:04Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.90
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:21Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.108/32
      prometheus.io/path: /metrics
      prometheus.io/port: "9402"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-03-31T18:40:21Z"
    generateName: gitlab-cert-manager-5ffcc7f99f-
    labels:
      app: cert-manager
      app.kubernetes.io/instance: gitlab
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      helm.sh/chart: certmanager-v0.10.1
      pod-template-hash: 5ffcc7f99f
    name: gitlab-cert-manager-5ffcc7f99f-b5h99
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-cert-manager-5ffcc7f99f
      uid: 99816455-398b-11ea-b115-42010a8001d6
    resourceVersion: "37198415"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-cert-manager-5ffcc7f99f-b5h99
    uid: 13c68edd-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - args:
      - --v=2
      - --cluster-resource-namespace=$(POD_NAMESPACE)
      - --leader-election-namespace=$(POD_NAMESPACE)
      - --webhook-namespace=$(POD_NAMESPACE)
      - --webhook-ca-secret=gitlab-cert-manager-webhook-ca
      - --webhook-serving-secret=gitlab-cert-manager-webhook-tls
      - --webhook-dns-names=gitlab-cert-manager-webhook,gitlab-cert-manager-webhook.gitlab,gitlab-cert-manager-webhook.gitlab.svc
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-controller:v0.10.1
      imagePullPolicy: IfNotPresent
      name: certmanager
      ports:
      - containerPort: 9402
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: gitlab-cert-manager-token-bqpjt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: gitlab-cert-manager
    serviceAccountName: gitlab-cert-manager
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: gitlab-cert-manager-token-bqpjt
      secret:
        defaultMode: 420
        secretName: gitlab-cert-manager-token-bqpjt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:21Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T23:36:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T23:36:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:21Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://3847005effe94292141748a24d991af6b017df458ee59baf5296009b7630e626
      image: quay.io/jetstack/cert-manager-controller:v0.10.1
      imageID: docker-pullable://quay.io/jetstack/cert-manager-controller@sha256:57521bab22044e65ec73aedc0d405b13246785022a6445605e52fe2a75e6e437
      lastState:
        terminated:
          containerID: docker://874aaecfcba0969c547d4985fcbbe5944d4ca0d1b0640b0c5ee899055ecc0c86
          exitCode: 1
          finishedAt: "2020-04-10T23:36:11Z"
          reason: Error
          startedAt: "2020-04-10T22:56:25Z"
      name: certmanager
      ready: true
      restartCount: 103
      state:
        running:
          startedAt: "2020-04-10T23:36:22Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.108
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:21Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.69/32
    creationTimestamp: "2020-03-31T18:40:21Z"
    generateName: gitlab-cert-manager-webhook-76d9d9cc69-
    labels:
      app: webhook
      app.kubernetes.io/instance: gitlab
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      helm.sh/chart: certmanager-v0.10.1
      pod-template-hash: 76d9d9cc69
    name: gitlab-cert-manager-webhook-76d9d9cc69-6f65j
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-cert-manager-webhook-76d9d9cc69
      uid: 9992e00f-398b-11ea-b115-42010a8001d6
    resourceVersion: "33571686"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-cert-manager-webhook-76d9d9cc69-6f65j
    uid: 13c9e16b-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - args:
      - --v=2
      - --secure-port=6443
      - --tls-cert-file=/certs/tls.crt
      - --tls-private-key-file=/certs/tls.key
      env:
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/jetstack/cert-manager-webhook:v0.10.1
      imagePullPolicy: IfNotPresent
      name: certmanager
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /certs
        name: certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: gitlab-cert-manager-webhook-token-nx7k6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: gitlab-cert-manager-webhook
    serviceAccountName: gitlab-cert-manager-webhook
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: certs
      secret:
        defaultMode: 420
        secretName: gitlab-cert-manager-webhook-tls
    - name: gitlab-cert-manager-webhook-token-nx7k6
      secret:
        defaultMode: 420
        secretName: gitlab-cert-manager-webhook-token-nx7k6
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:21Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:21Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://edcf1b525388bfdcdfe11b651c42bddf480b63bd067475aed75d60e3a8f33045
      image: quay.io/jetstack/cert-manager-webhook:v0.10.1
      imageID: docker-pullable://quay.io/jetstack/cert-manager-webhook@sha256:8db898648fe921ce3d4c49a71672d608084b062c03c6295a20d26030ab6077ff
      lastState: {}
      name: certmanager
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:48:44Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.69
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:21Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 164952b2fb54733dd4a2c06a3f1429e87c5410ecfcd72236bb546ddea4bf277a
      cni.projectcalico.org/podIP: 10.8.1.102/32
    creationTimestamp: "2020-03-30T19:15:37Z"
    generateName: gitlab-gitaly-
    labels:
      app: gitaly
      controller-revision-hash: gitlab-gitaly-7cf784db64
      release: gitlab
      statefulset.kubernetes.io/pod-name: gitlab-gitaly-0
    name: gitlab-gitaly-0
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: gitlab-gitaly
      uid: 999dceb7-398b-11ea-b115-42010a8001d6
    resourceVersion: "33572289"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-gitaly-0
    uid: d646f8dc-72ba-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: gitaly
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /etc/gitaly/templates
      - name: CONFIG_DIRECTORY
        value: /etc/gitaly
      - name: GITALY_CONFIG_FILE
        value: /etc/gitaly/config.toml
      - name: SSL_CERT_DIR
        value: /etc/ssl/certs
      - name: GITALY_PROMETHEUS_LISTEN_ADDR
        value: :9236
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      image: registry.gitlab.com/gitlab-org/build/cng/gitaly:v1.77.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: gitaly
      ports:
      - containerPort: 8075
        protocol: TCP
      - containerPort: 9236
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        requests:
          cpu: 50m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /etc/gitaly/templates
        name: gitaly-config
      - mountPath: /etc/gitlab-secrets
        name: gitaly-secrets
        readOnly: true
      - mountPath: /home/git/repositories
        name: repo-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: gitlab-gitaly-0
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - command:
      - sh
      - /config/configure
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: gitaly-config
        readOnly: true
      - mountPath: /init-config
        name: init-gitaly-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: gitaly-secrets
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    subdomain: gitlab-gitaly
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: repo-data
      persistentVolumeClaim:
        claimName: repo-data-gitlab-gitaly-0
    - configMap:
        defaultMode: 420
        name: gitlab-gitaly
      name: gitaly-config
    - emptyDir:
        medium: Memory
      name: gitaly-secrets
    - name: init-gitaly-secrets
      projected:
        defaultMode: 288
        sources:
        - secret:
            items:
            - key: token
              path: gitaly_token
            name: gitlab-gitaly-secret
        - secret:
            items:
            - key: secret
              path: .gitlab_shell_secret
            name: gitlab-gitlab-shell-secret
        - secret:
            items:
            - key: secret
              path: redis_password
            name: gitlab-redis-secret
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:17:57Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:51:42Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:51:42Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://85830c77ac0ed70c5267c9a298587ccef0ae1d8f55e1166f334197befc69d05f
      image: registry.gitlab.com/gitlab-org/build/cng/gitaly:v1.77.1
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/gitaly@sha256:39cba22d0752c351c71d81d5f256e4ab15775c99e1384bc44b9e49df2a87315a
      lastState:
        terminated:
          containerID: docker://c613f6601229ba278d0897117126a7a743ab9f33ac60c963c4b7dd368bbcfd50
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:18:53Z"
      name: gitaly
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:50:36Z"
    hostIP: 10.128.0.37
    initContainerStatuses:
    - containerID: docker://f84178f7c288de1e737da3482f63a5a3027697ce257874e33d0b0597d3b573a5
      image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/alpine-certificates@sha256:00ce9a585179e6b22c9bfea9ba82552630eab0bd25da4f13282b588b2ad022dc
      lastState: {}
      name: certificates
      ready: true
      restartCount: 1
      state:
        terminated:
          containerID: docker://f84178f7c288de1e737da3482f63a5a3027697ce257874e33d0b0597d3b573a5
          exitCode: 0
          finishedAt: "2020-04-01T13:49:30Z"
          reason: Completed
          startedAt: "2020-04-01T13:49:31Z"
    - containerID: docker://ba6ab7c3863cd7c09674c2ad8b1435908af86bb1fd88de9f99c71d825839205f
      image: busybox:latest
      imageID: docker-pullable://busybox@sha256:afe605d272837ce1732f390966166c2afff5391208ddd57de10942748694049d
      lastState: {}
      name: configure
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://ba6ab7c3863cd7c09674c2ad8b1435908af86bb1fd88de9f99c71d825839205f
          exitCode: 0
          finishedAt: "2020-04-01T13:50:26Z"
          reason: Completed
          startedAt: "2020-04-01T13:50:26Z"
    phase: Running
    podIP: 10.8.1.102
    qosClass: Burstable
    startTime: "2020-03-30T19:16:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 1aebe5270e7534dc7510737e5fcfb26b9b6ca0f289d95563a9cdddae80ad7e24
      cni.projectcalico.org/podIP: 10.8.0.171/32
      prometheus.io/path: /metrics
      prometheus.io/port: "9168"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-03-31T18:40:30Z"
    generateName: gitlab-gitlab-exporter-868bc56dd8-
    labels:
      app: gitlab-exporter
      pod-template-hash: 868bc56dd8
      release: gitlab
    name: gitlab-gitlab-exporter-868bc56dd8-qb57h
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-gitlab-exporter-868bc56dd8
      uid: 9981cadc-398b-11ea-b115-42010a8001d6
    resourceVersion: "33303020"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-gitlab-exporter-868bc56dd8-qb57h
    uid: 18da9757-737f-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: gitlab-exporter
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab-exporter/templates
      - name: CONFIG_DIRECTORY
        value: /etc/gitlab-exporter
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-exporter:5.1.0
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -f 'gitlab-exporter'
      livenessProbe:
        exec:
          command:
          - pgrep
          - -f
          - gitlab-exporter
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: gitlab-exporter
      ports:
      - containerPort: 9168
        name: gitlab-exporter
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - pgrep
          - -f
          - gitlab-exporter
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 50m
          memory: 100M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab-exporter/templates/gitlab-exporter.yml.erb
        name: gitlab-exporter-config
        subPath: gitlab-exporter.yml.erb
      - mountPath: /etc/gitlab
        name: gitlab-exporter-secrets
        readOnly: true
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - command:
      - sh
      - /config/configure
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: gitlab-exporter-config
        readOnly: true
      - mountPath: /init-config
        name: init-gitlab-exporter-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: gitlab-exporter-secrets
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: gitlab-gitlab-exporter
      name: gitlab-exporter-config
    - name: init-gitlab-exporter-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: postgres-password
              path: postgres/psql-password
            name: gitlab-postgresql-password
        - secret:
            items:
            - key: secret
              path: redis/password
            name: gitlab-redis-secret
    - emptyDir:
        medium: Memory
      name: gitlab-exporter-secrets
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:51Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:51Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:25Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://d52c50bbe859af0b5abd4fbf5925a96004783d401581b7d0e5268a19985660b4
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-exporter:5.1.0
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/gitlab-exporter@sha256:a1f234d28dc2978067d5bd25a72895bd0161e34467d5bd9e2b12e6cf334f8de2
      lastState: {}
      name: gitlab-exporter
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-03-31T18:42:46Z"
    hostIP: 10.128.0.35
    initContainerStatuses:
    - containerID: docker://1b5e1f50e0bfad70d2b88a166fa454f9752a3e7790116783e36a5408270c1d64
      image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/alpine-certificates@sha256:00ce9a585179e6b22c9bfea9ba82552630eab0bd25da4f13282b588b2ad022dc
      lastState: {}
      name: certificates
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://1b5e1f50e0bfad70d2b88a166fa454f9752a3e7790116783e36a5408270c1d64
          exitCode: 0
          finishedAt: "2020-03-31T18:42:35Z"
          reason: Completed
          startedAt: "2020-03-31T18:42:34Z"
    - containerID: docker://f67aad87976715bf4c551e79dea281188799fd051a50741f2ff4e1251d799c8e
      image: busybox:latest
      imageID: docker-pullable://busybox@sha256:afe605d272837ce1732f390966166c2afff5391208ddd57de10942748694049d
      lastState: {}
      name: configure
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://f67aad87976715bf4c551e79dea281188799fd051a50741f2ff4e1251d799c8e
          exitCode: 0
          finishedAt: "2020-03-31T18:42:40Z"
          reason: Completed
          startedAt: "2020-03-31T18:42:40Z"
    phase: Running
    podIP: 10.8.0.171
    qosClass: Burstable
    startTime: "2020-03-31T18:42:25Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configmap: b6037b659634c6ed64ec4ba6a1ff81db0af76d05c0b3be594c981a6274cb551d
      checksum/secrets: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
      cni.projectcalico.org/podIP: 10.8.1.113/32
      prometheus.io/port: "9252"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-03-31T18:40:22Z"
    generateName: gitlab-gitlab-runner-65ff67bdd8-
    labels:
      app: gitlab-gitlab-runner
      chart: gitlab-runner-0.12.0
      heritage: Helm
      pod-template-hash: 65ff67bdd8
      release: gitlab
    name: gitlab-gitlab-runner-65ff67bdd8-mnsbp
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-gitlab-runner-65ff67bdd8
      uid: 997138d9-398b-11ea-b115-42010a8001d6
    resourceVersion: "37201637"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-gitlab-runner-65ff67bdd8-mnsbp
    uid: 13f5e140-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - command:
      - /bin/bash
      - /scripts/entrypoint
      env:
      - name: CI_SERVER_URL
        value: https://gitlab.example.com
      - name: CLONE_URL
      - name: RUNNER_REQUEST_CONCURRENCY
        value: "1"
      - name: RUNNER_EXECUTOR
        value: kubernetes
      - name: REGISTER_LOCKED
        value: "false"
      - name: RUNNER_TAG_LIST
      - name: RUNNER_OUTPUT_LIMIT
        value: "4096"
      - name: KUBERNETES_IMAGE
        value: ubuntu:16.04
      - name: KUBERNETES_NAMESPACE
        value: gitlab
      - name: KUBERNETES_POLL_TIMEOUT
        value: "180"
      - name: KUBERNETES_CPU_LIMIT
      - name: KUBERNETES_MEMORY_LIMIT
      - name: KUBERNETES_CPU_REQUEST
      - name: KUBERNETES_MEMORY_REQUEST
      - name: KUBERNETES_SERVICE_ACCOUNT
      - name: KUBERNETES_SERVICE_CPU_LIMIT
      - name: KUBERNETES_SERVICE_MEMORY_LIMIT
      - name: KUBERNETES_SERVICE_CPU_REQUEST
      - name: KUBERNETES_SERVICE_MEMORY_REQUEST
      - name: KUBERNETES_HELPER_CPU_LIMIT
      - name: KUBERNETES_HELPER_MEMORY_LIMIT
      - name: KUBERNETES_HELPER_CPU_REQUEST
      - name: KUBERNETES_HELPER_MEMORY_REQUEST
      - name: KUBERNETES_HELPER_IMAGE
      - name: KUBERNETES_PULL_POLICY
      - name: CACHE_TYPE
        value: s3
      - name: CACHE_PATH
        value: gitlab-runner
      - name: CACHE_SHARED
        value: "true"
      - name: CACHE_S3_SERVER_ADDRESS
        value: minio.example.com
      - name: CACHE_S3_BUCKET_NAME
        value: runner-cache
      - name: CACHE_S3_BUCKET_LOCATION
        value: us-east-1
      image: gitlab/gitlab-runner:alpine-v12.6.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/bash
          - /scripts/check-live
        failureThreshold: 3
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: gitlab-gitlab-runner
      ports:
      - containerPort: 9252
        name: metrics
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /usr/bin/pgrep
          - gitlab.*runner
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /secrets
        name: runner-secrets
      - mountPath: /home/gitlab-runner/.gitlab-runner
        name: etc-gitlab-runner
      - mountPath: /scripts
        name: scripts
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: gitlab-gitlab-runner-token-c692q
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - sh
      - /config/configure
      env:
      - name: CI_SERVER_URL
        value: https://gitlab.example.com
      - name: CLONE_URL
      - name: RUNNER_REQUEST_CONCURRENCY
        value: "1"
      - name: RUNNER_EXECUTOR
        value: kubernetes
      - name: REGISTER_LOCKED
        value: "false"
      - name: RUNNER_TAG_LIST
      - name: RUNNER_OUTPUT_LIMIT
        value: "4096"
      - name: KUBERNETES_IMAGE
        value: ubuntu:16.04
      - name: KUBERNETES_NAMESPACE
        value: gitlab
      - name: KUBERNETES_POLL_TIMEOUT
        value: "180"
      - name: KUBERNETES_CPU_LIMIT
      - name: KUBERNETES_MEMORY_LIMIT
      - name: KUBERNETES_CPU_REQUEST
      - name: KUBERNETES_MEMORY_REQUEST
      - name: KUBERNETES_SERVICE_ACCOUNT
      - name: KUBERNETES_SERVICE_CPU_LIMIT
      - name: KUBERNETES_SERVICE_MEMORY_LIMIT
      - name: KUBERNETES_SERVICE_CPU_REQUEST
      - name: KUBERNETES_SERVICE_MEMORY_REQUEST
      - name: KUBERNETES_HELPER_CPU_LIMIT
      - name: KUBERNETES_HELPER_MEMORY_LIMIT
      - name: KUBERNETES_HELPER_CPU_REQUEST
      - name: KUBERNETES_HELPER_MEMORY_REQUEST
      - name: KUBERNETES_HELPER_IMAGE
      - name: KUBERNETES_PULL_POLICY
      - name: CACHE_TYPE
        value: s3
      - name: CACHE_PATH
        value: gitlab-runner
      - name: CACHE_SHARED
        value: "true"
      - name: CACHE_S3_SERVER_ADDRESS
        value: minio.example.com
      - name: CACHE_S3_BUCKET_NAME
        value: runner-cache
      - name: CACHE_S3_BUCKET_LOCATION
        value: us-east-1
      image: gitlab/gitlab-runner:alpine-v12.6.0
      imagePullPolicy: IfNotPresent
      name: configure
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /secrets
        name: runner-secrets
      - mountPath: /config
        name: scripts
        readOnly: true
      - mountPath: /init-secrets
        name: init-runner-secrets
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: gitlab-gitlab-runner-token-c692q
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65533
      runAsUser: 100
    serviceAccount: gitlab-gitlab-runner
    serviceAccountName: gitlab-gitlab-runner
    terminationGracePeriodSeconds: 3600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: runner-secrets
    - emptyDir:
        medium: Memory
      name: etc-gitlab-runner
    - name: init-runner-secrets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: gitlab-minio-secret
        - secret:
            items:
            - key: runner-registration-token
              path: runner-registration-token
            - key: runner-token
              path: runner-token
            name: gitlab-gitlab-runner-secret
    - configMap:
        defaultMode: 420
        name: gitlab-gitlab-runner
      name: scripts
    - name: gitlab-gitlab-runner-token-c692q
      secret:
        defaultMode: 420
        secretName: gitlab-gitlab-runner-token-c692q
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T23:50:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T23:50:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://fa41bdcc5e46594a5e9442e6432040a0785a00663d3b13d2b5d4cb4a25b9a056
      image: gitlab/gitlab-runner:alpine-v12.6.0
      imageID: docker-pullable://gitlab/gitlab-runner@sha256:b2638eafdd22b1e3166206838143a44468fd87d33ed76958563630301238852f
      lastState:
        terminated:
          containerID: docker://70170110d18bc435df7bd0b5e00216ef6b9b1d36af935c1000e0aca5812a685b
          exitCode: 1
          finishedAt: "2020-04-10T23:49:26Z"
          reason: Error
          startedAt: "2020-04-10T23:36:38Z"
      name: gitlab-gitlab-runner
      ready: true
      restartCount: 1806
      state:
        running:
          startedAt: "2020-04-10T23:49:31Z"
    hostIP: 10.128.0.37
    initContainerStatuses:
    - containerID: docker://242cc4cb4acd4b133800a08d96da7e5a0509acd22c6dbaed7e412be9a76098e5
      image: gitlab/gitlab-runner:alpine-v12.6.0
      imageID: docker-pullable://gitlab/gitlab-runner@sha256:b2638eafdd22b1e3166206838143a44468fd87d33ed76958563630301238852f
      lastState: {}
      name: configure
      ready: true
      restartCount: 1
      state:
        terminated:
          containerID: docker://242cc4cb4acd4b133800a08d96da7e5a0509acd22c6dbaed7e412be9a76098e5
          exitCode: 0
          finishedAt: "2020-04-01T13:49:58Z"
          reason: Completed
          startedAt: "2020-04-01T13:49:58Z"
    phase: Running
    podIP: 10.8.1.113
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: ba8fbd9408a25a13f7c9d3682c4bed3a4e3645216c471e54e71035dd3c0d5bad
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/podIP: 10.8.0.196/32
    creationTimestamp: "2020-04-01T13:37:37Z"
    generateName: gitlab-gitlab-shell-7b89877f4c-
    labels:
      app: gitlab-shell
      pod-template-hash: 7b89877f4c
      release: gitlab
    name: gitlab-gitlab-shell-7b89877f4c-hbd29
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-gitlab-shell-7b89877f4c
      uid: 99a6c2f9-398b-11ea-b115-42010a8001d6
    resourceVersion: "33568700"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-gitlab-shell-7b89877f4c-hbd29
    uid: f358239e-741d-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: gitlab-shell
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /etc/gitlab-shell
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab-shell
      - name: KEYS_DIRECTORY
        value: /etc/gitlab-secrets/ssh
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-shell:v10.3.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: gitlab-shell
      ports:
      - containerPort: 2222
        name: ssh
        protocol: TCP
      resources:
        requests:
          cpu: "0"
          memory: 6M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/gitlab-shell
        name: shell-config
      - mountPath: /etc/gitlab-secrets
        name: shell-secrets
        readOnly: true
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - command:
      - sh
      - /config/configure
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: shell-config
        readOnly: true
      - mountPath: /init-config
        name: shell-init-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: shell-secrets
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: gitlab-gitlab-shell
      name: shell-config
    - name: shell-init-secrets
      projected:
        defaultMode: 288
        sources:
        - secret:
            name: gitlab-gitlab-shell-host-keys
        - secret:
            items:
            - key: secret
              path: shell/.gitlab_shell_secret
            name: gitlab-gitlab-shell-secret
        - secret:
            items:
            - key: secret
              path: redis/password
            name: gitlab-redis-secret
    - emptyDir:
        medium: Memory
      name: shell-secrets
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:37:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:37:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:37:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:37:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://4a3c322d91766992687a0f58f7f2e61912f6b0dc88483d72fc3da6606f669200
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-shell:v10.3.0
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/gitlab-shell@sha256:3bf341ee201c88d919c89e28c54c25ed9e1c2a0ab86ef7e0f197251e3fa13880
      lastState: {}
      name: gitlab-shell
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-04-01T13:37:43Z"
    hostIP: 10.128.0.35
    initContainerStatuses:
    - containerID: docker://91e2c5509bdf9e73999967fd535b70b8ec9dfa872ab62887cb5167b348a12f2d
      image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/alpine-certificates@sha256:00ce9a585179e6b22c9bfea9ba82552630eab0bd25da4f13282b588b2ad022dc
      lastState: {}
      name: certificates
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://91e2c5509bdf9e73999967fd535b70b8ec9dfa872ab62887cb5167b348a12f2d
          exitCode: 0
          finishedAt: "2020-04-01T13:37:40Z"
          reason: Completed
          startedAt: "2020-04-01T13:37:39Z"
    - containerID: docker://ee6dea40148af2e2de1cb191eed8cf7d4a26486a10588db0b9e6d05a22fdfbb7
      image: busybox:latest
      imageID: docker-pullable://busybox@sha256:afe605d272837ce1732f390966166c2afff5391208ddd57de10942748694049d
      lastState: {}
      name: configure
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://ee6dea40148af2e2de1cb191eed8cf7d4a26486a10588db0b9e6d05a22fdfbb7
          exitCode: 0
          finishedAt: "2020-04-01T13:37:41Z"
          reason: Completed
          startedAt: "2020-04-01T13:37:41Z"
    phase: Running
    podIP: 10.8.0.196
    qosClass: Burstable
    startTime: "2020-04-01T13:37:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.101/32
    creationTimestamp: "2020-03-31T18:40:22Z"
    generateName: gitlab-minio-79db4985c4-
    labels:
      app: minio
      chart: minio-0.4.3
      component: app
      heritage: Helm
      pod-template-hash: 79db4985c4
      release: gitlab
    name: gitlab-minio-79db4985c4-k5bmw
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-minio-79db4985c4
      uid: 997377fd-398b-11ea-b115-42010a8001d6
    resourceVersion: "33571989"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-minio-79db4985c4-k5bmw
    uid: 13f1eccd-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - args:
      - -C
      - /tmp/.minio
      - --quiet
      - server
      - /export
      image: minio/minio:RELEASE.2017-12-28T01-21-00Z
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: 9000
        timeoutSeconds: 1
      name: minio
      ports:
      - containerPort: 9000
        name: service
        protocol: TCP
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /export
        name: export
      - mountPath: /tmp/.minio
        name: minio-server-config
      - mountPath: /podinfo
        name: podinfo
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - sh
      - /config/configure
      image: busybox:latest
      imagePullPolicy: IfNotPresent
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: minio-configuration
      - mountPath: /minio
        name: minio-server-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - downwardAPI:
        defaultMode: 420
        items:
        - fieldRef:
            apiVersion: v1
            fieldPath: metadata.labels
          path: labels
      name: podinfo
    - name: export
      persistentVolumeClaim:
        claimName: gitlab-minio
    - name: minio-configuration
      projected:
        defaultMode: 420
        sources:
        - configMap:
            name: gitlab-minio-config-cm
        - secret:
            name: gitlab-minio-secret
    - emptyDir:
        medium: Memory
      name: minio-server-config
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:44:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:31Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://6934f44f35a038f7b75bbbc7d8c94c4ed081a3c1cd0a6c4e9183aa83a587cfd9
      image: minio/minio:RELEASE.2017-12-28T01-21-00Z
      imageID: docker-pullable://minio/minio@sha256:3611f1644cf5447e2f3639b4212ade26155f6a0632bef155c4d6510811c1fe1d
      lastState:
        terminated:
          containerID: docker://ee1dcdabbfb4caf856fad40298fddf1d9e54c8195b82d9b20f466e2f1089dad8
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-31T18:46:54Z"
      name: minio
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:50:27Z"
    hostIP: 10.128.0.37
    initContainerStatuses:
    - containerID: docker://a5c4bd9ddda709b6d2d2f1a711fc395c86ed5d252506cbe72041af3dea095c66
      image: busybox:latest
      imageID: docker-pullable://busybox@sha256:afe605d272837ce1732f390966166c2afff5391208ddd57de10942748694049d
      lastState: {}
      name: configure
      ready: true
      restartCount: 1
      state:
        terminated:
          containerID: docker://a5c4bd9ddda709b6d2d2f1a711fc395c86ed5d252506cbe72041af3dea095c66
          exitCode: 0
          finishedAt: "2020-04-01T13:49:30Z"
          reason: Completed
          startedAt: "2020-04-01T13:49:30Z"
    phase: Running
    podIP: 10.8.1.101
    qosClass: Burstable
    startTime: "2020-03-31T18:40:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.72/32
    creationTimestamp: "2020-03-31T18:40:22Z"
    generateName: gitlab-nginx-ingress-controller-7db754f856-
    labels:
      app: nginx-ingress
      component: controller
      pod-template-hash: 7db754f856
      release: gitlab
    name: gitlab-nginx-ingress-controller-7db754f856-c68sq
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-nginx-ingress-controller-7db754f856
      uid: 2e1a3417-470e-11ea-96d3-42010a80017a
    resourceVersion: "33571826"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-nginx-ingress-controller-7db754f856-c68sq
    uid: 1400464d-737f-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: nginx-ingress
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - args:
      - /nginx-ingress-controller
      - --default-backend-service=gitlab/gitlab-nginx-ingress-default-backend
      - --publish-service=gitlab/gitlab-nginx-ingress-controller
      - --election-id=ingress-controller-leader
      - --ingress-class=gitlab-nginx
      - --configmap=gitlab/gitlab-nginx-ingress-controller
      - --tcp-services-configmap=gitlab/gitlab-nginx-ingress-tcp
      - --watch-namespace=gitlab
      - --force-namespace-isolation
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: nginx-ingress-controller
      ports:
      - containerPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        name: https
        protocol: TCP
      - containerPort: 18080
        name: stats
        protocol: TCP
      - containerPort: 10254
        name: metrics
        protocol: TCP
      - containerPort: 22
        name: gitlab-shell
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 15m
          memory: 15Mi
      securityContext:
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        runAsUser: 33
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: gitlab-nginx-ingress-token-czp6w
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: gitlab-nginx-ingress
    serviceAccountName: gitlab-nginx-ingress
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: gitlab-nginx-ingress-token-czp6w
      secret:
        defaultMode: 420
        secretName: gitlab-nginx-ingress-token-czp6w
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:14Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:14Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://76ccc6f2c8c49c8c2eb4e92ae937b8dbe7bef8ae30963c5d5d5c9631cc35478b
      image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
      imageID: docker-pullable://quay.io/kubernetes-ingress-controller/nginx-ingress-controller@sha256:617076c3e3d4d0638a4927702530d8456bda64c67194f6daed272a59e93b992f
      lastState: {}
      name: nginx-ingress-controller
      ready: true
      restartCount: 23
      state:
        running:
          startedAt: "2020-04-01T13:48:45Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.72
    qosClass: Burstable
    startTime: "2020-03-31T18:40:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.92/32
    creationTimestamp: "2020-03-31T18:40:22Z"
    generateName: gitlab-nginx-ingress-controller-7db754f856-
    labels:
      app: nginx-ingress
      component: controller
      pod-template-hash: 7db754f856
      release: gitlab
    name: gitlab-nginx-ingress-controller-7db754f856-gwp94
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-nginx-ingress-controller-7db754f856
      uid: 2e1a3417-470e-11ea-96d3-42010a80017a
    resourceVersion: "37062127"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-nginx-ingress-controller-7db754f856-gwp94
    uid: 1437d3c8-737f-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: nginx-ingress
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - args:
      - /nginx-ingress-controller
      - --default-backend-service=gitlab/gitlab-nginx-ingress-default-backend
      - --publish-service=gitlab/gitlab-nginx-ingress-controller
      - --election-id=ingress-controller-leader
      - --ingress-class=gitlab-nginx
      - --configmap=gitlab/gitlab-nginx-ingress-controller
      - --tcp-services-configmap=gitlab/gitlab-nginx-ingress-tcp
      - --watch-namespace=gitlab
      - --force-namespace-isolation
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: nginx-ingress-controller
      ports:
      - containerPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        name: https
        protocol: TCP
      - containerPort: 18080
        name: stats
        protocol: TCP
      - containerPort: 10254
        name: metrics
        protocol: TCP
      - containerPort: 22
        name: gitlab-shell
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 15m
          memory: 15Mi
      securityContext:
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        runAsUser: 33
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: gitlab-nginx-ingress-token-czp6w
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: gitlab-nginx-ingress
    serviceAccountName: gitlab-nginx-ingress
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: gitlab-nginx-ingress-token-czp6w
      secret:
        defaultMode: 420
        secretName: gitlab-nginx-ingress-token-czp6w
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T14:23:06Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T14:23:06Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://872d19535f147a9e20de6393f55bf69bd76d605fb9c60e4003e1da0d7546a0cd
      image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
      imageID: docker-pullable://quay.io/kubernetes-ingress-controller/nginx-ingress-controller@sha256:617076c3e3d4d0638a4927702530d8456bda64c67194f6daed272a59e93b992f
      lastState:
        terminated:
          containerID: docker://48c3272343761be6a06ac20673231a43d9c9ede6665388b173fcbe75846b309b
          exitCode: 143
          finishedAt: "2020-04-10T14:22:41Z"
          reason: Error
          startedAt: "2020-04-10T14:22:08Z"
      name: nginx-ingress-controller
      ready: true
      restartCount: 30
      state:
        running:
          startedAt: "2020-04-10T14:22:44Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.92
    qosClass: Burstable
    startTime: "2020-03-31T18:40:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.89/32
    creationTimestamp: "2020-03-31T18:40:22Z"
    generateName: gitlab-nginx-ingress-controller-7db754f856-
    labels:
      app: nginx-ingress
      component: controller
      pod-template-hash: 7db754f856
      release: gitlab
    name: gitlab-nginx-ingress-controller-7db754f856-qj84g
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-nginx-ingress-controller-7db754f856
      uid: 2e1a3417-470e-11ea-96d3-42010a80017a
    resourceVersion: "33571852"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-nginx-ingress-controller-7db754f856-qj84g
    uid: 1415d030-737f-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: nginx-ingress
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - args:
      - /nginx-ingress-controller
      - --default-backend-service=gitlab/gitlab-nginx-ingress-default-backend
      - --publish-service=gitlab/gitlab-nginx-ingress-controller
      - --election-id=ingress-controller-leader
      - --ingress-class=gitlab-nginx
      - --configmap=gitlab/gitlab-nginx-ingress-controller
      - --tcp-services-configmap=gitlab/gitlab-nginx-ingress-tcp
      - --watch-namespace=gitlab
      - --force-namespace-isolation
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: nginx-ingress-controller
      ports:
      - containerPort: 80
        name: http
        protocol: TCP
      - containerPort: 443
        name: https
        protocol: TCP
      - containerPort: 18080
        name: stats
        protocol: TCP
      - containerPort: 10254
        name: metrics
        protocol: TCP
      - containerPort: 22
        name: gitlab-shell
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 10254
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 15m
          memory: 15Mi
      securityContext:
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - ALL
        runAsUser: 33
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: gitlab-nginx-ingress-token-czp6w
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: gitlab-nginx-ingress
    serviceAccountName: gitlab-nginx-ingress
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: gitlab-nginx-ingress-token-czp6w
      secret:
        defaultMode: 420
        secretName: gitlab-nginx-ingress-token-czp6w
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://d834283cdce82e2d524a49ee6bbd7b021f1a7353225890cb6999913aff9c79d0
      image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
      imageID: docker-pullable://quay.io/kubernetes-ingress-controller/nginx-ingress-controller@sha256:617076c3e3d4d0638a4927702530d8456bda64c67194f6daed272a59e93b992f
      lastState:
        terminated:
          containerID: docker://9c358d5a4d69662b61dac7b72b4519eecdbf9d455a7409493888a7f4f07c6761
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-04-01T13:27:08Z"
      name: nginx-ingress-controller
      ready: true
      restartCount: 15
      state:
        running:
          startedAt: "2020-04-01T13:49:19Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.89
    qosClass: Burstable
    startTime: "2020-03-31T18:40:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.82/32
    creationTimestamp: "2020-03-31T18:40:22Z"
    generateName: gitlab-nginx-ingress-default-backend-7f87d67c8-
    labels:
      app: nginx-ingress
      component: default-backend
      pod-template-hash: 7f87d67c8
      release: gitlab
    name: gitlab-nginx-ingress-default-backend-7f87d67c8-2fznh
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-nginx-ingress-default-backend-7f87d67c8
      uid: 9981338c-398b-11ea-b115-42010a8001d6
    resourceVersion: "33571812"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-nginx-ingress-default-backend-7f87d67c8-2fznh
    uid: 1425c2c9-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - image: k8s.gcr.io/defaultbackend:1.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: nginx-ingress-default-backend
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      resources:
        requests:
          cpu: 5m
          memory: 5Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://4f47b6ee84f4de76619ffc523bc074c3b5c1795a586d8885fb24c5dbad34af34
      image: k8s.gcr.io/defaultbackend:1.4
      imageID: docker-pullable://k8s.gcr.io/defaultbackend@sha256:865b0c35e6da393b8e80b7e3799f777572399a4cff047eb02a81fa6e7a48ed4b
      lastState:
        terminated:
          containerID: docker://179ad3f70da6781da6168a9492de0d817d21c195bf49247fbc3de7c922f40e4a
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-31T18:42:19Z"
      name: nginx-ingress-default-backend
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:48:51Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.82
    qosClass: Burstable
    startTime: "2020-03-31T18:40:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.67/32
    creationTimestamp: "2020-03-31T18:40:22Z"
    generateName: gitlab-nginx-ingress-default-backend-7f87d67c8-
    labels:
      app: nginx-ingress
      component: default-backend
      pod-template-hash: 7f87d67c8
      release: gitlab
    name: gitlab-nginx-ingress-default-backend-7f87d67c8-zv7jx
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-nginx-ingress-default-backend-7f87d67c8
      uid: 9981338c-398b-11ea-b115-42010a8001d6
    resourceVersion: "33571657"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-nginx-ingress-default-backend-7f87d67c8-zv7jx
    uid: 14459da5-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - image: k8s.gcr.io/defaultbackend:1.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: nginx-ingress-default-backend
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      resources:
        requests:
          cpu: 5m
          memory: 5Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://25e60efb15eb500a4c8c901bc2dd68c9a23705c047fc3cc19461f76003e7bb5b
      image: k8s.gcr.io/defaultbackend:1.4
      imageID: docker-pullable://k8s.gcr.io/defaultbackend@sha256:865b0c35e6da393b8e80b7e3799f777572399a4cff047eb02a81fa6e7a48ed4b
      lastState: {}
      name: nginx-ingress-default-backend
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:48:21Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.67
    qosClass: Burstable
    startTime: "2020-03-31T18:40:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.84/32
    creationTimestamp: "2020-03-31T18:40:22Z"
    generateName: gitlab-postgresql-8466fc49b6-
    labels:
      app: postgresql
      pod-template-hash: 8466fc49b6
      release: gitlab
    name: gitlab-postgresql-8466fc49b6-tp87g
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-postgresql-8466fc49b6
      uid: 19bcff3a-470e-11ea-96d3-42010a80017a
    resourceVersion: "33571955"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-postgresql-8466fc49b6-tp87g
    uid: 144d8d4c-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: POSTGRES_USER
        value: gitlab
      - name: PGUSER
        value: gitlab
      - name: POSTGRES_DB
        value: gitlabhq_production
      - name: POSTGRES_INITDB_ARGS
      - name: PGDATA
        value: /var/lib/postgresql/data/pgdata
      - name: POSTGRES_PASSWORD_FILE
        value: /conf/postgres-password
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: postgres:9.6.8
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - exec pg_isready --host $POD_IP
        failureThreshold: 6
        initialDelaySeconds: 120
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: gitlab-postgresql
      ports:
      - containerPort: 5432
        name: postgresql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - exec pg_isready --host $POD_IP
        failureThreshold: 3
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        requests:
          cpu: 10m
          memory: 25Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/postgresql/data/pgdata
        name: data
        subPath: postgresql-db
      - mountPath: /conf
        name: password-file
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - env:
      - name: DATA_SOURCE_NAME
        value: postgresql://gitlab@127.0.0.1:5432?sslmode=disable
      image: wrouesnel/postgres_exporter:v0.1.1
      imagePullPolicy: IfNotPresent
      name: metrics
      ports:
      - containerPort: 9187
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 12m
          memory: 25Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: gitlab-postgresql
    - name: password-file
      secret:
        defaultMode: 420
        items:
        - key: postgres-password
          path: postgres-password
        secretName: gitlab-postgresql-password
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:25Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:25Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://7d82ae37156b160e46b58a61c05331bff569a5a456031d16d527fea1665b8072
      image: postgres:9.6.8
      imageID: docker-pullable://postgres@sha256:6e1efcd656386fc0c0a9893fda0c0ec6704868fbaad5ad5be3732c4f81cf21c9
      lastState:
        terminated:
          containerID: docker://7108c69ba805dccb893ba3fecdd7b58fa2d672bc4372af718265bca851eb69c1
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-31T18:47:35Z"
      name: gitlab-postgresql
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:06Z"
    - containerID: docker://5849d18b4899576a8cd4a2b72245de9d115ef82b36ebabb23405b73dc5d2b9ae
      image: wrouesnel/postgres_exporter:v0.1.1
      imageID: docker-pullable://wrouesnel/postgres_exporter@sha256:d8bc6221112d77b2d7b7746b729f848b0db60823eb385355636943934c09d822
      lastState: {}
      name: metrics
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:37Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.84
    qosClass: Burstable
    startTime: "2020-03-31T18:40:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.88/32
    creationTimestamp: "2020-03-31T18:40:22Z"
    generateName: gitlab-prometheus-server-cf7649bb9-
    labels:
      app: prometheus
      chart: prometheus-9.0.0
      component: server
      heritage: Helm
      pod-template-hash: cf7649bb9
      release: gitlab
    name: gitlab-prometheus-server-cf7649bb9-z95n4
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-prometheus-server-cf7649bb9
      uid: 9a191393-398b-11ea-b115-42010a8001d6
    resourceVersion: "37200144"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-prometheus-server-cf7649bb9-z95n4
    uid: 145447e6-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - args:
      - --volume-dir=/etc/config
      - --webhook-url=http://127.0.0.1:9090/-/reload
      image: jimmidyson/configmap-reload:v0.2.2
      imagePullPolicy: IfNotPresent
      name: prometheus-server-configmap-reload
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: gitlab-prometheus-server-token-dx4t6
        readOnly: true
    - args:
      - --storage.tsdb.retention.time=15d
      - --config.file=/etc/config/prometheus.yml
      - --storage.tsdb.path=/data
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --web.console.templates=/etc/prometheus/consoles
      - --web.enable-lifecycle
      image: prom/prometheus:v2.11.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/healthy
          port: 9090
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: prometheus-server
      ports:
      - containerPort: 9090
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: 9090
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
      - mountPath: /data
        name: storage-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: gitlab-prometheus-server-token-dx4t6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: gitlab-prometheus-server
    serviceAccountName: gitlab-prometheus-server
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: gitlab-prometheus-server
      name: config-volume
    - name: storage-volume
      persistentVolumeClaim:
        claimName: gitlab-prometheus-server
    - name: gitlab-prometheus-server-token-dx4t6
      secret:
        defaultMode: 420
        secretName: gitlab-prometheus-server-token-dx4t6
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T23:44:04Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T23:44:04Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://2e202c747e3ad82e479d6074d14fb2de0ffc67f1219fa22f518fb737e385d033
      image: prom/prometheus:v2.11.1
      imageID: docker-pullable://prom/prometheus@sha256:8f34c18cf2ccaf21e361afd18e92da2602d0fa23a8917f759f906219242d8572
      lastState:
        terminated:
          containerID: docker://c326302374b89f9c4c560d7c6017c8afce965bd7860efeb86c1a6ef6fa480fda
          exitCode: 137
          finishedAt: "2020-04-10T23:36:07Z"
          reason: Error
          startedAt: "2020-04-10T23:09:57Z"
      name: prometheus-server
      ready: true
      restartCount: 110
      state:
        running:
          startedAt: "2020-04-10T23:36:14Z"
    - containerID: docker://a4710ac1d2eea66355a1039ba2a9b03d5060a3d41e25b6d1e8d5571f33c66144
      image: jimmidyson/configmap-reload:v0.2.2
      imageID: docker-pullable://jimmidyson/configmap-reload@sha256:befec9f23d2a9da86a298d448cc9140f56a457362a7d9eecddba192db1ab489e
      lastState: {}
      name: prometheus-server-configmap-reload
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:42Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.88
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.93/32
    creationTimestamp: "2020-03-31T18:40:22Z"
    generateName: gitlab-redis-sentinel-69879b577-
    labels:
      app: redis
      chart: redis-ha-0.1.0
      heritage: Helm
      name: redis-sentinel
      pod-template-hash: 69879b577
      podIP: 10.8.1.93
      redis-role: sentinel
      release: gitlab
    name: gitlab-redis-sentinel-69879b577-jdspq
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-redis-sentinel-69879b577
      uid: 0061c6c4-470e-11ea-96d3-42010a80017a
    resourceVersion: "33571938"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-redis-sentinel-69879b577-jdspq
    uid: 146aadfb-737f-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - redis
              - key: release
                operator: In
                values:
                - gitlab
              - key: redis-role
                operator: In
                values:
                - sentinel
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: SENTINEL
        value: "true"
      - name: REDIS_CHART_PREFIX
        value: gitlab-redis
      - name: REDIS_PASSWORD_FILE
        value: /config/password
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-redis-ha:732704f18e34ba469df34b10c3b2465e0469d484
      imagePullPolicy: Always
      name: sentinel
      ports:
      - containerPort: 26379
        protocol: TCP
      resources:
        limits:
          memory: 30Mi
        requests:
          cpu: 13m
          memory: 30Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config/
        name: gitlab-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: gitlab-redis-token-78ffj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: gitlab-redis
    serviceAccountName: gitlab-redis
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: gitlab-config
      projected:
        defaultMode: 420
        sources:
        - secret:
            items:
            - key: secret
              path: password
            name: gitlab-redis-secret
    - name: gitlab-redis-token-78ffj
      secret:
        defaultMode: 420
        secretName: gitlab-redis-token-78ffj
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://37e1a51c9b34ec79bc1b3769c23ac3eb1242cd8115b00f41cc47a1d987a268ef
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-redis-ha:732704f18e34ba469df34b10c3b2465e0469d484
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/gitlab-redis-ha@sha256:fd0cc46056de7f542953ac852265919552285a388fe2616ca593fbc147d86c4c
      lastState:
        terminated:
          containerID: docker://9d08ca81c1041d59e17b728d57785365d377809fb95d40fdf302e011c3a8b05e
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-31T18:41:15Z"
      name: sentinel
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:30Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.93
    qosClass: Burstable
    startTime: "2020-03-31T18:40:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.71/32
    creationTimestamp: "2020-03-31T18:40:23Z"
    generateName: gitlab-redis-sentinel-69879b577-
    labels:
      app: redis
      chart: redis-ha-0.1.0
      heritage: Helm
      name: redis-sentinel
      pod-template-hash: 69879b577
      podIP: 10.8.1.71
      redis-role: sentinel
      release: gitlab
    name: gitlab-redis-sentinel-69879b577-thjjr
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-redis-sentinel-69879b577
      uid: 0061c6c4-470e-11ea-96d3-42010a80017a
    resourceVersion: "33571665"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-redis-sentinel-69879b577-thjjr
    uid: 14c5351f-737f-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - redis
              - key: release
                operator: In
                values:
                - gitlab
              - key: redis-role
                operator: In
                values:
                - sentinel
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: SENTINEL
        value: "true"
      - name: REDIS_CHART_PREFIX
        value: gitlab-redis
      - name: REDIS_PASSWORD_FILE
        value: /config/password
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-redis-ha:732704f18e34ba469df34b10c3b2465e0469d484
      imagePullPolicy: Always
      name: sentinel
      ports:
      - containerPort: 26379
        protocol: TCP
      resources:
        limits:
          memory: 30Mi
        requests:
          cpu: 13m
          memory: 30Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config/
        name: gitlab-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: gitlab-redis-token-78ffj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: gitlab-redis
    serviceAccountName: gitlab-redis
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: gitlab-config
      projected:
        defaultMode: 420
        sources:
        - secret:
            items:
            - key: secret
              path: password
            name: gitlab-redis-secret
    - name: gitlab-redis-token-78ffj
      secret:
        defaultMode: 420
        secretName: gitlab-redis-token-78ffj
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://7e40048190a36a93f23d50aa316c8c34aa0d9ae5f12a9263d9d4365043618bb9
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-redis-ha:732704f18e34ba469df34b10c3b2465e0469d484
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/gitlab-redis-ha@sha256:fd0cc46056de7f542953ac852265919552285a388fe2616ca593fbc147d86c4c
      lastState: {}
      name: sentinel
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:48:37Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.71
    qosClass: Burstable
    startTime: "2020-03-31T18:40:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: a17ca01b2bd51c3d701b455723a4d2324df05a7ec7436349a928b62f74fbee96
      cni.projectcalico.org/podIP: 10.8.1.66/32
    creationTimestamp: "2020-03-30T19:15:38Z"
    generateName: gitlab-redis-server-
    labels:
      app: redis
      chart: redis-ha-0.1.0
      controller-revision-hash: gitlab-redis-server-6df9884d78
      heritage: Helm
      name: redis-server
      podIP: 10.8.1.66
      redis-node: "true"
      redis-role: master
      release: gitlab
      statefulset.kubernetes.io/pod-name: gitlab-redis-server-0
    name: gitlab-redis-server-0
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: gitlab-redis-server
      uid: 999be56c-398b-11ea-b115-42010a8001d6
    resourceVersion: "33572219"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-redis-server-0
    uid: d6fed3ab-72ba-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - redis
              - key: release
                operator: In
                values:
                - gitlab
              - key: redis-role
                operator: In
                values:
                - master
                - slave
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: REDIS_SENTINEL_SERVICE_HOST
        value: redis-sentinel
      - name: REDIS_CHART_PREFIX
        value: gitlab-redis
      - name: REDIS_PASSWORD_FILE
        value: /etc/redis/pass
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-redis-ha:732704f18e34ba469df34b10c3b2465e0469d484
      imagePullPolicy: Always
      name: redis
      ports:
      - containerPort: 6379
        protocol: TCP
      resources:
        requests:
          cpu: 25m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /redis-master-data
        name: data
      - mountPath: /etc/redis/
        name: gitlab-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: gitlab-redis-token-78ffj
        readOnly: true
    - env:
      - name: REDIS_FILE
        value: /metrics/redis
      image: oliver006/redis_exporter:v0.34.1-alpine
      imagePullPolicy: IfNotPresent
      name: metrics
      ports:
      - containerPort: 9121
        name: metrics
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /metrics
        name: gitlab-metrics
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: gitlab-redis-token-78ffj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: gitlab-redis-server-0
    initContainers:
    - command:
      - sh
      - /config/configure
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: gitlab
        readOnly: true
      - mountPath: /redis
        name: gitlab-config
      - mountPath: /metrics
        name: gitlab-metrics
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: gitlab-redis-token-78ffj
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsUser: 2000
    serviceAccount: gitlab-redis
    serviceAccountName: gitlab-redis
    subdomain: gitlab-redis
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-gitlab-redis-server-0
    - emptyDir:
        medium: Memory
      name: gitlab-config
    - emptyDir:
        medium: Memory
      name: gitlab-metrics
    - name: gitlab
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: redis-master.conf
              path: redis-master.conf
            - key: redis-slave.conf
              path: redis-slave.conf
            - key: configure
              path: configure
            name: gitlab-redis
        - secret:
            items:
            - key: secret
              path: password
            name: gitlab-redis-secret
    - name: gitlab-redis-token-78ffj
      secret:
        defaultMode: 420
        secretName: gitlab-redis-token-78ffj
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:17:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:51:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:51:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9e2522c6b761035acea1b353ef6057a0063d087d13915504f94b294d4f650944
      image: oliver006/redis_exporter:v0.34.1-alpine
      imageID: docker-pullable://oliver006/redis_exporter@sha256:5a2ddfdd9cec0a473086dc433a38c28ca93915fac9f5a62a019eab397dd22ff5
      lastState:
        terminated:
          containerID: docker://c02d7585cda254b9526cd3663c63d44eaf00ab184e8c93a726b6cffdfd94fbcc
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:18:00Z"
      name: metrics
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:50:11Z"
    - containerID: docker://00613135e9e1b7bd8f8298454802f4a7fc60e1a7e5461d9263da714f31a5129c
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-redis-ha:732704f18e34ba469df34b10c3b2465e0469d484
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/gitlab-redis-ha@sha256:fd0cc46056de7f542953ac852265919552285a388fe2616ca593fbc147d86c4c
      lastState:
        terminated:
          containerID: docker://319a42a8bec951b9747cbadc7468afdcccaf3d94f40a9b4d6e5b4af247beff46
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:17:56Z"
      name: redis
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:50:07Z"
    hostIP: 10.128.0.37
    initContainerStatuses:
    - containerID: docker://dd1de897900c6b8126887dc3a3aea435de37d6f3345afcd1874521780c8b9e5a
      image: busybox:latest
      imageID: docker-pullable://busybox@sha256:afe605d272837ce1732f390966166c2afff5391208ddd57de10942748694049d
      lastState: {}
      name: configure
      ready: true
      restartCount: 1
      state:
        terminated:
          containerID: docker://dd1de897900c6b8126887dc3a3aea435de37d6f3345afcd1874521780c8b9e5a
          exitCode: 0
          finishedAt: "2020-04-01T13:48:30Z"
          reason: Completed
          startedAt: "2020-04-01T13:48:34Z"
    phase: Running
    podIP: 10.8.1.66
    qosClass: Burstable
    startTime: "2020-03-30T19:16:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configmap: 286c8cfd6e68b01cac791db6c7970b093f059b17c007ea126964d5c8f5cc9288
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/podIP: 10.8.1.104/32
    creationTimestamp: "2020-03-31T01:16:27Z"
    generateName: gitlab-registry-74c959fc8-
    labels:
      app: registry
      pod-template-hash: 74c959fc8
      release: gitlab
    name: gitlab-registry-74c959fc8-2krpd
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-registry-74c959fc8
      uid: 99924f2e-398b-11ea-b115-42010a8001d6
    resourceVersion: "33572003"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-registry-74c959fc8-2krpd
    uid: 3ec1e3d3-72ed-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: registry
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - image: registry:2.7.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /debug/health
          port: 5001
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: registry
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /debug/health
          port: 5001
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 50m
          memory: 32Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/docker/registry/
        name: registry-server-config
        readOnly: true
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - command:
      - sh
      - /config/configure
      image: busybox:latest
      imagePullPolicy: IfNotPresent
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: registry-secrets
      - mountPath: /registry
        name: registry-server-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: registry-server-config
    - name: registry-secrets
      projected:
        defaultMode: 420
        sources:
        - configMap:
            name: gitlab-registry
        - secret:
            items:
            - key: registry-auth.crt
              path: certificate.crt
            name: gitlab-registry-secret
        - secret:
            items:
            - key: secret
              path: httpSecret
            name: gitlab-registry-httpsecret
        - secret:
            name: gitlab-minio-secret
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T01:16:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:33Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:33Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T01:16:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://5406c219663bbd53657b302503270853959575111cee6c337e01b8b520d7b1e0
      image: registry:2.7.1
      imageID: docker-pullable://registry@sha256:7d081088e4bfd632a88e3f3bcd9e007ef44a796fddfe3261407a3f9f04abe1e7
      lastState:
        terminated:
          containerID: docker://dab00ebe562355f0cd6f8fbe25a53605a13c17d1e9171b55a961a8e0296d8b8d
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-04-01T13:00:40Z"
      name: registry
      ready: true
      restartCount: 3
      state:
        running:
          startedAt: "2020-04-01T13:50:14Z"
    hostIP: 10.128.0.37
    initContainerStatuses:
    - containerID: docker://80926a64d7b9dce3478c17ac3213aa8b746bcc6d138b0863a3f15e1a4de387cd
      image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/alpine-certificates@sha256:00ce9a585179e6b22c9bfea9ba82552630eab0bd25da4f13282b588b2ad022dc
      lastState: {}
      name: certificates
      ready: true
      restartCount: 1
      state:
        terminated:
          containerID: docker://80926a64d7b9dce3478c17ac3213aa8b746bcc6d138b0863a3f15e1a4de387cd
          exitCode: 0
          finishedAt: "2020-04-01T13:49:30Z"
          reason: Completed
          startedAt: "2020-04-01T13:49:30Z"
    - containerID: docker://946eb7f07a6ed88ac29d41f77e0cd34666fa6dc40a16a48e05f4922f1d7c3824
      image: busybox:latest
      imageID: docker-pullable://busybox@sha256:afe605d272837ce1732f390966166c2afff5391208ddd57de10942748694049d
      lastState: {}
      name: configure
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://946eb7f07a6ed88ac29d41f77e0cd34666fa6dc40a16a48e05f4922f1d7c3824
          exitCode: 0
          finishedAt: "2020-04-01T13:49:57Z"
          reason: Completed
          startedAt: "2020-04-01T13:49:57Z"
    phase: Running
    podIP: 10.8.1.104
    qosClass: Burstable
    startTime: "2020-03-31T01:16:27Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/configmap: 3a9f53018b334d9fcb2222de4dabc983f5b6eed7c75910d315de61d1d88a0d91
      checksum/configmap-pod: 1c9250b5fb48aeff1c2d2554f323f88f9db01c254bdaef83cf2dc26ec784ebea
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/podIP: 10.8.0.197/32
      prometheus.io/port: "3807"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-04-01T13:52:23Z"
    generateName: gitlab-sidekiq-all-in-1-649889b7c6-
    labels:
      app: sidekiq
      pod-template-hash: 649889b7c6
      release: gitlab
    name: gitlab-sidekiq-all-in-1-649889b7c6-v7ml6
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-sidekiq-all-in-1-649889b7c6
      uid: 977edeb1-4709-11ea-96d3-42010a80017a
    resourceVersion: "33573023"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-sidekiq-all-in-1-649889b7c6-v7ml6
    uid: 03b71e09-7420-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: sidekiq
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: prometheus_multiproc_dir
        value: /metrics
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: SIDEKIQ_CONCURRENCY
        value: "25"
      - name: SIDEKIQ_TIMEOUT
        value: "5"
      - name: SIDEKIQ_DAEMON_MEMORY_KILLER
        value: "0"
      - name: SIDEKIQ_MEMORY_KILLER_CHECK_INTERVAL
        value: "3"
      - name: SIDEKIQ_MEMORY_KILLER_MAX_RSS
        value: "2000000"
      - name: SIDEKIQ_MEMORY_KILLER_GRACE_TIME
        value: "900"
      - name: SIDEKIQ_MEMORY_KILLER_SHUTDOWN_WAIT
        value: "30"
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-sidekiq-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -f 'sidekiq'
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /liveness
          port: 3807
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: sidekiq
      ports:
      - containerPort: 3807
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 3807
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 50M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /metrics
        name: sidekiq-metrics
      - mountPath: /var/opt/gitlab/templates
        name: sidekiq-config
        readOnly: true
      - mountPath: /etc/gitlab
        name: sidekiq-secrets
        readOnly: true
      - mountPath: /srv/gitlab/config/secrets.yml
        name: sidekiq-secrets
        subPath: rails-secrets/secrets.yml
      - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
        name: sidekiq-config
        subPath: smtp_settings.rb
      - mountPath: /srv/gitlab/INSTALLATION_TYPE
        name: sidekiq-config
        subPath: installation_type
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - command:
      - sh
      - /config/configure
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: sidekiq-config
        readOnly: true
      - mountPath: /init-config
        name: init-sidekiq-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: sidekiq-secrets
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - /scripts/wait-for-deps
      env:
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: SIDEKIQ_CONCURRENCY
        value: "25"
      - name: SIDEKIQ_TIMEOUT
        value: "5"
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-sidekiq-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      name: dependencies
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: sidekiq-config
        readOnly: true
      - mountPath: /etc/gitlab
        name: sidekiq-secrets
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: sidekiq-metrics
    - name: sidekiq-config
      projected:
        defaultMode: 420
        sources:
        - configMap:
            name: gitlab-sidekiq
        - configMap:
            name: gitlab-sidekiq-all-in-1
    - name: init-sidekiq-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: secrets.yml
              path: rails-secrets/secrets.yml
            name: gitlab-rails-secret
        - secret:
            items:
            - key: token
              path: gitaly/gitaly_token
            name: gitlab-gitaly-secret
        - secret:
            items:
            - key: secret
              path: redis/password
            name: gitlab-redis-secret
        - secret:
            items:
            - key: postgres-password
              path: postgres/psql-password
            name: gitlab-postgresql-password
        - secret:
            items:
            - key: registry-auth.key
              path: registry/gitlab-registry.key
            name: gitlab-registry-secret
        - secret:
            items:
            - key: accesskey
              path: minio/accesskey
            - key: secretkey
              path: minio/secretkey
            name: gitlab-minio-secret
    - emptyDir:
        medium: Memory
      name: sidekiq-secrets
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:53:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:54:51Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:54:51Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:52:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://e529ad5964b9c972d4969f3e10402ddd892a3506bdc586523451da56ee0f5938
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-sidekiq-ee:v12.6.4
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/gitlab-sidekiq-ee@sha256:fe237c2c5e64dade77d841a034a16b84ce5afa9f619f8519fa0d846dfed5289b
      lastState: {}
      name: sidekiq
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-04-01T13:53:11Z"
    hostIP: 10.128.0.35
    initContainerStatuses:
    - containerID: docker://57b58b06064895d1fb16b37ecde140e4245381863cc80d7f7f94327aff2d77b0
      image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/alpine-certificates@sha256:00ce9a585179e6b22c9bfea9ba82552630eab0bd25da4f13282b588b2ad022dc
      lastState: {}
      name: certificates
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://57b58b06064895d1fb16b37ecde140e4245381863cc80d7f7f94327aff2d77b0
          exitCode: 0
          finishedAt: "2020-04-01T13:52:26Z"
          reason: Completed
          startedAt: "2020-04-01T13:52:25Z"
    - containerID: docker://1c8a366a22e5829fde4840746fcfa61a4cd4d9cffc10dd01499a82e98bc5f468
      image: busybox:latest
      imageID: docker-pullable://busybox@sha256:afe605d272837ce1732f390966166c2afff5391208ddd57de10942748694049d
      lastState: {}
      name: configure
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://1c8a366a22e5829fde4840746fcfa61a4cd4d9cffc10dd01499a82e98bc5f468
          exitCode: 0
          finishedAt: "2020-04-01T13:52:27Z"
          reason: Completed
          startedAt: "2020-04-01T13:52:27Z"
    - containerID: docker://e52b0f9994bd6358f1c18e7da16ed40a3de638bed5516159b076d64a0111a9c1
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-sidekiq-ee:v12.6.4
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/gitlab-sidekiq-ee@sha256:fe237c2c5e64dade77d841a034a16b84ce5afa9f619f8519fa0d846dfed5289b
      lastState: {}
      name: dependencies
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://e52b0f9994bd6358f1c18e7da16ed40a3de638bed5516159b076d64a0111a9c1
          exitCode: 0
          finishedAt: "2020-04-01T13:53:07Z"
          reason: Completed
          startedAt: "2020-04-01T13:52:28Z"
    phase: Running
    podIP: 10.8.0.197
    qosClass: Burstable
    startTime: "2020-04-01T13:52:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: f51dac24c133254fe6c63906a579c8d3d58b44036d8737eb8744b99cef3feb8a
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/podIP: 10.8.0.157/32
    creationTimestamp: "2020-03-31T18:40:28Z"
    generateName: gitlab-task-runner-9f9cf668f-
    labels:
      app: task-runner
      pod-template-hash: 9f9cf668f
      release: gitlab
    name: gitlab-task-runner-9f9cf668f-q9q67
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-task-runner-9f9cf668f
      uid: 999f2071-398b-11ea-b115-42010a8001d6
    resourceVersion: "33302858"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-task-runner-9f9cf668f-q9q67
    uid: 17c0507e-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - args:
      - /bin/bash
      - -c
      - cp -v -r -L /etc/gitlab/.s3cfg $HOME/.s3cfg && while sleep 3600; do :; done
      env:
      - name: ARTIFACTS_BUCKET_NAME
        value: gitlab-artifacts
      - name: REGISTRY_BUCKET_NAME
        value: registry
      - name: LFS_BUCKET_NAME
        value: git-lfs
      - name: UPLOADS_BUCKET_NAME
        value: gitlab-uploads
      - name: PACKAGES_BUCKET_NAME
        value: gitlab-packages
      - name: BACKUP_BUCKET_NAME
        value: gitlab-backups
      - name: BACKUP_BACKEND
        value: s3
      - name: TMP_BUCKET_NAME
        value: tmp
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: ENABLE_BOOTSNAP
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-task-runner-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      name: task-runner
      resources:
        requests:
          cpu: 50m
          memory: 350M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: task-runner-config
      - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
        name: task-runner-config
        subPath: smtp_settings.rb
      - mountPath: /etc/gitlab
        name: task-runner-secrets
        readOnly: true
      - mountPath: /srv/gitlab/config/secrets.yml
        name: task-runner-secrets
        subPath: rails-secrets/secrets.yml
      - mountPath: /srv/gitlab/tmp
        name: task-runner-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - command:
      - sh
      - /config/configure
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config
        name: task-runner-config
        readOnly: true
      - mountPath: /init-config
        name: init-task-runner-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: task-runner-secrets
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: task-runner-config
      projected:
        defaultMode: 420
        sources:
        - configMap:
            name: gitlab-task-runner
    - emptyDir: {}
      name: task-runner-tmp
    - name: init-task-runner-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: secrets.yml
              path: rails-secrets/secrets.yml
            name: gitlab-rails-secret
        - secret:
            items:
            - key: secret
              path: shell/.gitlab_shell_secret
            name: gitlab-gitlab-shell-secret
        - secret:
            items:
            - key: token
              path: gitaly/gitaly_token
            name: gitlab-gitaly-secret
        - secret:
            items:
            - key: secret
              path: redis/password
            name: gitlab-redis-secret
        - secret:
            items:
            - key: postgres-password
              path: postgres/psql-password
            name: gitlab-postgresql-password
        - secret:
            items:
            - key: registry-auth.key
              path: registry/gitlab-registry.key
            name: gitlab-registry-secret
        - secret:
            items:
            - key: accesskey
              path: minio/accesskey
            - key: secretkey
              path: minio/secretkey
            name: gitlab-minio-secret
    - emptyDir:
        medium: Memory
      name: task-runner-secrets
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://bb8b1db5cdfdb41d8aaf32c3a4aef20b341f3678a0134af00c50f8dcb9d94cc3
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-task-runner-ee:v12.6.4
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/gitlab-task-runner-ee@sha256:d8d8050ef9af602f0f8b402a0335e2f24a0b01d4a0d8742ec14fe126b300a5d4
      lastState: {}
      name: task-runner
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-03-31T18:42:29Z"
    hostIP: 10.128.0.35
    initContainerStatuses:
    - containerID: docker://f94eda3065cbea2181fd8a51cbcc5ec3041fa0fa6cf8e459343376c2b3400764
      image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/alpine-certificates@sha256:00ce9a585179e6b22c9bfea9ba82552630eab0bd25da4f13282b588b2ad022dc
      lastState: {}
      name: certificates
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://f94eda3065cbea2181fd8a51cbcc5ec3041fa0fa6cf8e459343376c2b3400764
          exitCode: 0
          finishedAt: "2020-03-31T18:42:01Z"
          reason: Completed
          startedAt: "2020-03-31T18:42:01Z"
    - containerID: docker://603fe8f0aaa92dd81374d9a4fadedb34bca10705d15a4a185079e43a8e7d41f1
      image: busybox:latest
      imageID: docker-pullable://busybox@sha256:afe605d272837ce1732f390966166c2afff5391208ddd57de10942748694049d
      lastState: {}
      name: configure
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://603fe8f0aaa92dd81374d9a4fadedb34bca10705d15a4a185079e43a8e7d41f1
          exitCode: 0
          finishedAt: "2020-03-31T18:42:16Z"
          reason: Completed
          startedAt: "2020-03-31T18:42:15Z"
    phase: Running
    podIP: 10.8.0.157
    qosClass: Burstable
    startTime: "2020-03-31T18:40:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: d021e46742b5d6dd76265a6b1dd9c78cdeb6db74842f2cdaa710f411a97fbabb
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/podIP: 10.8.1.62/32
      prometheus.io/path: /-/metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-03-31T23:01:10Z"
    generateName: gitlab-unicorn-84d7dc6557-
    labels:
      app: unicorn
      pod-template-hash: 84d7dc6557
      release: gitlab
    name: gitlab-unicorn-84d7dc6557-2smmj
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-unicorn-84d7dc6557
      uid: b3bedb01-4709-11ea-96d3-42010a80017a
    resourceVersion: "33431485"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-unicorn-84d7dc6557-2smmj
    uid: 83176b6b-73a3-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: unicorn
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: prometheus_multiproc_dir
        value: /metrics
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -SIGQUIT -f 'unicorn master'
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/liveness
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: unicorn
      ports:
      - containerPort: 8080
        name: unicorn
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/readiness
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /metrics
        name: unicorn-metrics
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /srv/gitlab/config/secrets.yml
        name: unicorn-secrets
        subPath: rails-secrets/secrets.yml
      - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
        name: unicorn-config
        subPath: smtp_settings.rb
      - mountPath: /srv/gitlab/INSTALLATION_TYPE
        name: unicorn-config
        subPath: installation_type
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITLAB_WORKHORSE_EXTRA_ARGS
      - name: GITLAB_WORKHORSE_LISTEN_PORT
        value: "8181"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: gitlab-workhorse
      ports:
      - containerPort: 8181
        name: workhorse
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: workhorse-config
      - mountPath: /etc/gitlab
        name: workhorse-secrets
        readOnly: true
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - -c
      - sh -x /config-unicorn/configure ; sh -x /config-workhorse/configure ; mkdir
        -p -m 3770 /tmp/gitlab
      command:
      - sh
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config-unicorn
        name: unicorn-config
        readOnly: true
      - mountPath: /config-workhorse
        name: workhorse-config
        readOnly: true
      - mountPath: /init-config
        name: init-unicorn-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: unicorn-secrets
      - mountPath: /init-secrets-workhorse
        name: workhorse-secrets
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - /scripts/wait-for-deps
      env:
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: WORKHORSE_ARCHIVE_CACHE_DISABLED
        value: "1"
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      name: dependencies
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: shared-tmp
    - emptyDir:
        medium: Memory
      name: unicorn-metrics
    - configMap:
        defaultMode: 420
        name: gitlab-unicorn
      name: unicorn-config
    - configMap:
        defaultMode: 420
        name: gitlab-workhorse-config
      name: workhorse-config
    - name: init-unicorn-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: secrets.yml
              path: rails-secrets/secrets.yml
            name: gitlab-rails-secret
        - secret:
            items:
            - key: secret
              path: shell/.gitlab_shell_secret
            name: gitlab-gitlab-shell-secret
        - secret:
            items:
            - key: token
              path: gitaly/gitaly_token
            name: gitlab-gitaly-secret
        - secret:
            items:
            - key: secret
              path: redis/password
            name: gitlab-redis-secret
        - secret:
            items:
            - key: postgres-password
              path: postgres/psql-password
            name: gitlab-postgresql-password
        - secret:
            items:
            - key: registry-auth.key
              path: registry/gitlab-registry.key
            name: gitlab-registry-secret
        - secret:
            items:
            - key: shared_secret
              path: gitlab-workhorse/secret
            name: gitlab-gitlab-workhorse-secret
        - secret:
            items:
            - key: accesskey
              path: minio/accesskey
            - key: secretkey
              path: minio/secretkey
            name: gitlab-minio-secret
    - emptyDir:
        medium: Memory
      name: unicorn-secrets
    - emptyDir:
        medium: Memory
      name: workhorse-secrets
    - emptyDir: {}
      name: shared-upload-directory
    - emptyDir: {}
      name: shared-artifact-directory
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    message: 'The node was low on resource: memory. Container unicorn was using 886328Ki,
      which exceeds its request of 40M. '
    phase: Failed
    reason: Evicted
    startTime: "2020-03-31T23:01:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: d021e46742b5d6dd76265a6b1dd9c78cdeb6db74842f2cdaa710f411a97fbabb
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/podIP: 10.8.1.64/32
      prometheus.io/path: /-/metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-04-01T06:01:53Z"
    generateName: gitlab-unicorn-84d7dc6557-
    labels:
      app: unicorn
      pod-template-hash: 84d7dc6557
      release: gitlab
    name: gitlab-unicorn-84d7dc6557-8hbrb
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-unicorn-84d7dc6557
      uid: b3bedb01-4709-11ea-96d3-42010a80017a
    resourceVersion: "33534064"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-unicorn-84d7dc6557-8hbrb
    uid: 4956ca11-73de-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: unicorn
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: prometheus_multiproc_dir
        value: /metrics
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -SIGQUIT -f 'unicorn master'
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/liveness
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: unicorn
      ports:
      - containerPort: 8080
        name: unicorn
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/readiness
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /metrics
        name: unicorn-metrics
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /srv/gitlab/config/secrets.yml
        name: unicorn-secrets
        subPath: rails-secrets/secrets.yml
      - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
        name: unicorn-config
        subPath: smtp_settings.rb
      - mountPath: /srv/gitlab/INSTALLATION_TYPE
        name: unicorn-config
        subPath: installation_type
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITLAB_WORKHORSE_EXTRA_ARGS
      - name: GITLAB_WORKHORSE_LISTEN_PORT
        value: "8181"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: gitlab-workhorse
      ports:
      - containerPort: 8181
        name: workhorse
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: workhorse-config
      - mountPath: /etc/gitlab
        name: workhorse-secrets
        readOnly: true
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - -c
      - sh -x /config-unicorn/configure ; sh -x /config-workhorse/configure ; mkdir
        -p -m 3770 /tmp/gitlab
      command:
      - sh
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config-unicorn
        name: unicorn-config
        readOnly: true
      - mountPath: /config-workhorse
        name: workhorse-config
        readOnly: true
      - mountPath: /init-config
        name: init-unicorn-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: unicorn-secrets
      - mountPath: /init-secrets-workhorse
        name: workhorse-secrets
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - /scripts/wait-for-deps
      env:
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: WORKHORSE_ARCHIVE_CACHE_DISABLED
        value: "1"
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      name: dependencies
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: shared-tmp
    - emptyDir:
        medium: Memory
      name: unicorn-metrics
    - configMap:
        defaultMode: 420
        name: gitlab-unicorn
      name: unicorn-config
    - configMap:
        defaultMode: 420
        name: gitlab-workhorse-config
      name: workhorse-config
    - name: init-unicorn-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: secrets.yml
              path: rails-secrets/secrets.yml
            name: gitlab-rails-secret
        - secret:
            items:
            - key: secret
              path: shell/.gitlab_shell_secret
            name: gitlab-gitlab-shell-secret
        - secret:
            items:
            - key: token
              path: gitaly/gitaly_token
            name: gitlab-gitaly-secret
        - secret:
            items:
            - key: secret
              path: redis/password
            name: gitlab-redis-secret
        - secret:
            items:
            - key: postgres-password
              path: postgres/psql-password
            name: gitlab-postgresql-password
        - secret:
            items:
            - key: registry-auth.key
              path: registry/gitlab-registry.key
            name: gitlab-registry-secret
        - secret:
            items:
            - key: shared_secret
              path: gitlab-workhorse/secret
            name: gitlab-gitlab-workhorse-secret
        - secret:
            items:
            - key: accesskey
              path: minio/accesskey
            - key: secretkey
              path: minio/secretkey
            name: gitlab-minio-secret
    - emptyDir:
        medium: Memory
      name: unicorn-secrets
    - emptyDir:
        medium: Memory
      name: workhorse-secrets
    - emptyDir: {}
      name: shared-upload-directory
    - emptyDir: {}
      name: shared-artifact-directory
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    message: 'The node was low on resource: memory. '
    phase: Failed
    reason: Evicted
    startTime: "2020-04-01T06:01:54Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: d021e46742b5d6dd76265a6b1dd9c78cdeb6db74842f2cdaa710f411a97fbabb
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/podIP: 10.8.0.174/32
      prometheus.io/path: /-/metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-03-31T19:23:34Z"
    generateName: gitlab-unicorn-84d7dc6557-
    labels:
      app: unicorn
      pod-template-hash: 84d7dc6557
      release: gitlab
    name: gitlab-unicorn-84d7dc6557-bbcvd
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-unicorn-84d7dc6557
      uid: b3bedb01-4709-11ea-96d3-42010a80017a
    resourceVersion: "33313349"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-unicorn-84d7dc6557-bbcvd
    uid: 1d186bc2-7385-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: unicorn
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: prometheus_multiproc_dir
        value: /metrics
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -SIGQUIT -f 'unicorn master'
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/liveness
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: unicorn
      ports:
      - containerPort: 8080
        name: unicorn
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/readiness
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /metrics
        name: unicorn-metrics
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /srv/gitlab/config/secrets.yml
        name: unicorn-secrets
        subPath: rails-secrets/secrets.yml
      - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
        name: unicorn-config
        subPath: smtp_settings.rb
      - mountPath: /srv/gitlab/INSTALLATION_TYPE
        name: unicorn-config
        subPath: installation_type
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITLAB_WORKHORSE_EXTRA_ARGS
      - name: GITLAB_WORKHORSE_LISTEN_PORT
        value: "8181"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: gitlab-workhorse
      ports:
      - containerPort: 8181
        name: workhorse
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: workhorse-config
      - mountPath: /etc/gitlab
        name: workhorse-secrets
        readOnly: true
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - -c
      - sh -x /config-unicorn/configure ; sh -x /config-workhorse/configure ; mkdir
        -p -m 3770 /tmp/gitlab
      command:
      - sh
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config-unicorn
        name: unicorn-config
        readOnly: true
      - mountPath: /config-workhorse
        name: workhorse-config
        readOnly: true
      - mountPath: /init-config
        name: init-unicorn-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: unicorn-secrets
      - mountPath: /init-secrets-workhorse
        name: workhorse-secrets
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - /scripts/wait-for-deps
      env:
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: WORKHORSE_ARCHIVE_CACHE_DISABLED
        value: "1"
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      name: dependencies
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: shared-tmp
    - emptyDir:
        medium: Memory
      name: unicorn-metrics
    - configMap:
        defaultMode: 420
        name: gitlab-unicorn
      name: unicorn-config
    - configMap:
        defaultMode: 420
        name: gitlab-workhorse-config
      name: workhorse-config
    - name: init-unicorn-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: secrets.yml
              path: rails-secrets/secrets.yml
            name: gitlab-rails-secret
        - secret:
            items:
            - key: secret
              path: shell/.gitlab_shell_secret
            name: gitlab-gitlab-shell-secret
        - secret:
            items:
            - key: token
              path: gitaly/gitaly_token
            name: gitlab-gitaly-secret
        - secret:
            items:
            - key: secret
              path: redis/password
            name: gitlab-redis-secret
        - secret:
            items:
            - key: postgres-password
              path: postgres/psql-password
            name: gitlab-postgresql-password
        - secret:
            items:
            - key: registry-auth.key
              path: registry/gitlab-registry.key
            name: gitlab-registry-secret
        - secret:
            items:
            - key: shared_secret
              path: gitlab-workhorse/secret
            name: gitlab-gitlab-workhorse-secret
        - secret:
            items:
            - key: accesskey
              path: minio/accesskey
            - key: secretkey
              path: minio/secretkey
            name: gitlab-minio-secret
    - emptyDir:
        medium: Memory
      name: unicorn-secrets
    - emptyDir:
        medium: Memory
      name: workhorse-secrets
    - emptyDir: {}
      name: shared-upload-directory
    - emptyDir: {}
      name: shared-artifact-directory
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T19:24:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T19:25:58Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T19:25:58Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T19:23:34Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://07e98cee2fd30a8cbaf844005db12a5212b4700cbbebc2685dc64939d046ecda
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee@sha256:6dba4d45e8a1f975604894e9fa12c28e5d30c126480ca4346ea449c25a69c4c5
      lastState: {}
      name: gitlab-workhorse
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-03-31T19:24:29Z"
    - containerID: docker://fadeed23de7c3052525a5d61b408e06582f101614c9c37c733364367961f8ba3
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee@sha256:b47d7878f286b55537822d6cf11ca4dd02796153ff0855cb5dac9f9d10991c72
      lastState: {}
      name: unicorn
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-03-31T19:24:28Z"
    hostIP: 10.128.0.35
    initContainerStatuses:
    - containerID: docker://5c9ec959502f1b5a9e3e2be496c4fa368a96f6d09bc268cbbddac319a267666a
      image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/alpine-certificates@sha256:00ce9a585179e6b22c9bfea9ba82552630eab0bd25da4f13282b588b2ad022dc
      lastState: {}
      name: certificates
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://5c9ec959502f1b5a9e3e2be496c4fa368a96f6d09bc268cbbddac319a267666a
          exitCode: 0
          finishedAt: "2020-03-31T19:23:36Z"
          reason: Completed
          startedAt: "2020-03-31T19:23:36Z"
    - containerID: docker://7e36a9c290e6fdbad584c8266004345a7a494e4a2193411ad2ef5ce04301dd4b
      image: busybox:latest
      imageID: docker-pullable://busybox@sha256:afe605d272837ce1732f390966166c2afff5391208ddd57de10942748694049d
      lastState: {}
      name: configure
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://7e36a9c290e6fdbad584c8266004345a7a494e4a2193411ad2ef5ce04301dd4b
          exitCode: 0
          finishedAt: "2020-03-31T19:23:37Z"
          reason: Completed
          startedAt: "2020-03-31T19:23:37Z"
    - containerID: docker://074c323a9a58a1a9c85d3cf6748f78c14126da95569740be09cef6df878e3566
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee@sha256:b47d7878f286b55537822d6cf11ca4dd02796153ff0855cb5dac9f9d10991c72
      lastState: {}
      name: dependencies
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://074c323a9a58a1a9c85d3cf6748f78c14126da95569740be09cef6df878e3566
          exitCode: 0
          finishedAt: "2020-03-31T19:24:26Z"
          reason: Completed
          startedAt: "2020-03-31T19:23:38Z"
    phase: Running
    podIP: 10.8.0.174
    qosClass: Burstable
    startTime: "2020-03-31T19:23:34Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: d021e46742b5d6dd76265a6b1dd9c78cdeb6db74842f2cdaa710f411a97fbabb
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/podIP: 10.8.1.28/32
      prometheus.io/path: /-/metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-03-31T18:40:24Z"
    generateName: gitlab-unicorn-84d7dc6557-
    labels:
      app: unicorn
      pod-template-hash: 84d7dc6557
      release: gitlab
    name: gitlab-unicorn-84d7dc6557-d75ck
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-unicorn-84d7dc6557
      uid: b3bedb01-4709-11ea-96d3-42010a80017a
    resourceVersion: "33323957"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-unicorn-84d7dc6557-d75ck
    uid: 15303c7f-737f-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: unicorn
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: prometheus_multiproc_dir
        value: /metrics
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -SIGQUIT -f 'unicorn master'
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/liveness
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: unicorn
      ports:
      - containerPort: 8080
        name: unicorn
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/readiness
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /metrics
        name: unicorn-metrics
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /srv/gitlab/config/secrets.yml
        name: unicorn-secrets
        subPath: rails-secrets/secrets.yml
      - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
        name: unicorn-config
        subPath: smtp_settings.rb
      - mountPath: /srv/gitlab/INSTALLATION_TYPE
        name: unicorn-config
        subPath: installation_type
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITLAB_WORKHORSE_EXTRA_ARGS
      - name: GITLAB_WORKHORSE_LISTEN_PORT
        value: "8181"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: gitlab-workhorse
      ports:
      - containerPort: 8181
        name: workhorse
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: workhorse-config
      - mountPath: /etc/gitlab
        name: workhorse-secrets
        readOnly: true
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - -c
      - sh -x /config-unicorn/configure ; sh -x /config-workhorse/configure ; mkdir
        -p -m 3770 /tmp/gitlab
      command:
      - sh
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config-unicorn
        name: unicorn-config
        readOnly: true
      - mountPath: /config-workhorse
        name: workhorse-config
        readOnly: true
      - mountPath: /init-config
        name: init-unicorn-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: unicorn-secrets
      - mountPath: /init-secrets-workhorse
        name: workhorse-secrets
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - /scripts/wait-for-deps
      env:
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: WORKHORSE_ARCHIVE_CACHE_DISABLED
        value: "1"
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      name: dependencies
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: shared-tmp
    - emptyDir:
        medium: Memory
      name: unicorn-metrics
    - configMap:
        defaultMode: 420
        name: gitlab-unicorn
      name: unicorn-config
    - configMap:
        defaultMode: 420
        name: gitlab-workhorse-config
      name: workhorse-config
    - name: init-unicorn-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: secrets.yml
              path: rails-secrets/secrets.yml
            name: gitlab-rails-secret
        - secret:
            items:
            - key: secret
              path: shell/.gitlab_shell_secret
            name: gitlab-gitlab-shell-secret
        - secret:
            items:
            - key: token
              path: gitaly/gitaly_token
            name: gitlab-gitaly-secret
        - secret:
            items:
            - key: secret
              path: redis/password
            name: gitlab-redis-secret
        - secret:
            items:
            - key: postgres-password
              path: postgres/psql-password
            name: gitlab-postgresql-password
        - secret:
            items:
            - key: registry-auth.key
              path: registry/gitlab-registry.key
            name: gitlab-registry-secret
        - secret:
            items:
            - key: shared_secret
              path: gitlab-workhorse/secret
            name: gitlab-gitlab-workhorse-secret
        - secret:
            items:
            - key: accesskey
              path: minio/accesskey
            - key: secretkey
              path: minio/secretkey
            name: gitlab-minio-secret
    - emptyDir:
        medium: Memory
      name: unicorn-secrets
    - emptyDir:
        medium: Memory
      name: workhorse-secrets
    - emptyDir: {}
      name: shared-upload-directory
    - emptyDir: {}
      name: shared-artifact-directory
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    message: 'The node was low on resource: memory. Container unicorn was using 901864Ki,
      which exceeds its request of 40M. '
    phase: Failed
    reason: Evicted
    startTime: "2020-03-31T18:40:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: d021e46742b5d6dd76265a6b1dd9c78cdeb6db74842f2cdaa710f411a97fbabb
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/podIP: 10.8.1.24/32
      prometheus.io/path: /-/metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-03-31T18:40:23Z"
    generateName: gitlab-unicorn-84d7dc6557-
    labels:
      app: unicorn
      pod-template-hash: 84d7dc6557
      release: gitlab
    name: gitlab-unicorn-84d7dc6557-k8zbh
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-unicorn-84d7dc6557
      uid: b3bedb01-4709-11ea-96d3-42010a80017a
    resourceVersion: "33312728"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-unicorn-84d7dc6557-k8zbh
    uid: 14ddc6bc-737f-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: unicorn
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: prometheus_multiproc_dir
        value: /metrics
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -SIGQUIT -f 'unicorn master'
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/liveness
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: unicorn
      ports:
      - containerPort: 8080
        name: unicorn
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/readiness
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /metrics
        name: unicorn-metrics
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /srv/gitlab/config/secrets.yml
        name: unicorn-secrets
        subPath: rails-secrets/secrets.yml
      - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
        name: unicorn-config
        subPath: smtp_settings.rb
      - mountPath: /srv/gitlab/INSTALLATION_TYPE
        name: unicorn-config
        subPath: installation_type
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITLAB_WORKHORSE_EXTRA_ARGS
      - name: GITLAB_WORKHORSE_LISTEN_PORT
        value: "8181"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: gitlab-workhorse
      ports:
      - containerPort: 8181
        name: workhorse
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: workhorse-config
      - mountPath: /etc/gitlab
        name: workhorse-secrets
        readOnly: true
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - -c
      - sh -x /config-unicorn/configure ; sh -x /config-workhorse/configure ; mkdir
        -p -m 3770 /tmp/gitlab
      command:
      - sh
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config-unicorn
        name: unicorn-config
        readOnly: true
      - mountPath: /config-workhorse
        name: workhorse-config
        readOnly: true
      - mountPath: /init-config
        name: init-unicorn-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: unicorn-secrets
      - mountPath: /init-secrets-workhorse
        name: workhorse-secrets
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - /scripts/wait-for-deps
      env:
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: WORKHORSE_ARCHIVE_CACHE_DISABLED
        value: "1"
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      name: dependencies
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: shared-tmp
    - emptyDir:
        medium: Memory
      name: unicorn-metrics
    - configMap:
        defaultMode: 420
        name: gitlab-unicorn
      name: unicorn-config
    - configMap:
        defaultMode: 420
        name: gitlab-workhorse-config
      name: workhorse-config
    - name: init-unicorn-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: secrets.yml
              path: rails-secrets/secrets.yml
            name: gitlab-rails-secret
        - secret:
            items:
            - key: secret
              path: shell/.gitlab_shell_secret
            name: gitlab-gitlab-shell-secret
        - secret:
            items:
            - key: token
              path: gitaly/gitaly_token
            name: gitlab-gitaly-secret
        - secret:
            items:
            - key: secret
              path: redis/password
            name: gitlab-redis-secret
        - secret:
            items:
            - key: postgres-password
              path: postgres/psql-password
            name: gitlab-postgresql-password
        - secret:
            items:
            - key: registry-auth.key
              path: registry/gitlab-registry.key
            name: gitlab-registry-secret
        - secret:
            items:
            - key: shared_secret
              path: gitlab-workhorse/secret
            name: gitlab-gitlab-workhorse-secret
        - secret:
            items:
            - key: accesskey
              path: minio/accesskey
            - key: secretkey
              path: minio/secretkey
            name: gitlab-minio-secret
    - emptyDir:
        medium: Memory
      name: unicorn-secrets
    - emptyDir:
        medium: Memory
      name: workhorse-secrets
    - emptyDir: {}
      name: shared-upload-directory
    - emptyDir: {}
      name: shared-artifact-directory
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    message: 'The node was low on resource: memory. Container unicorn was using 876280Ki,
      which exceeds its request of 40M. '
    phase: Failed
    reason: Evicted
    startTime: "2020-03-31T18:40:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: d021e46742b5d6dd76265a6b1dd9c78cdeb6db74842f2cdaa710f411a97fbabb
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/podIP: 10.8.1.63/32
      prometheus.io/path: /-/metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-04-01T03:40:43Z"
    generateName: gitlab-unicorn-84d7dc6557-
    labels:
      app: unicorn
      pod-template-hash: 84d7dc6557
      release: gitlab
    name: gitlab-unicorn-84d7dc6557-pc8qk
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-unicorn-84d7dc6557
      uid: b3bedb01-4709-11ea-96d3-42010a80017a
    resourceVersion: "33463905"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-unicorn-84d7dc6557-pc8qk
    uid: 90b86598-73ca-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: unicorn
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: prometheus_multiproc_dir
        value: /metrics
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -SIGQUIT -f 'unicorn master'
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/liveness
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: unicorn
      ports:
      - containerPort: 8080
        name: unicorn
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/readiness
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /metrics
        name: unicorn-metrics
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /srv/gitlab/config/secrets.yml
        name: unicorn-secrets
        subPath: rails-secrets/secrets.yml
      - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
        name: unicorn-config
        subPath: smtp_settings.rb
      - mountPath: /srv/gitlab/INSTALLATION_TYPE
        name: unicorn-config
        subPath: installation_type
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITLAB_WORKHORSE_EXTRA_ARGS
      - name: GITLAB_WORKHORSE_LISTEN_PORT
        value: "8181"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: gitlab-workhorse
      ports:
      - containerPort: 8181
        name: workhorse
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: workhorse-config
      - mountPath: /etc/gitlab
        name: workhorse-secrets
        readOnly: true
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - -c
      - sh -x /config-unicorn/configure ; sh -x /config-workhorse/configure ; mkdir
        -p -m 3770 /tmp/gitlab
      command:
      - sh
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config-unicorn
        name: unicorn-config
        readOnly: true
      - mountPath: /config-workhorse
        name: workhorse-config
        readOnly: true
      - mountPath: /init-config
        name: init-unicorn-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: unicorn-secrets
      - mountPath: /init-secrets-workhorse
        name: workhorse-secrets
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - /scripts/wait-for-deps
      env:
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: WORKHORSE_ARCHIVE_CACHE_DISABLED
        value: "1"
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      name: dependencies
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: shared-tmp
    - emptyDir:
        medium: Memory
      name: unicorn-metrics
    - configMap:
        defaultMode: 420
        name: gitlab-unicorn
      name: unicorn-config
    - configMap:
        defaultMode: 420
        name: gitlab-workhorse-config
      name: workhorse-config
    - name: init-unicorn-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: secrets.yml
              path: rails-secrets/secrets.yml
            name: gitlab-rails-secret
        - secret:
            items:
            - key: secret
              path: shell/.gitlab_shell_secret
            name: gitlab-gitlab-shell-secret
        - secret:
            items:
            - key: token
              path: gitaly/gitaly_token
            name: gitlab-gitaly-secret
        - secret:
            items:
            - key: secret
              path: redis/password
            name: gitlab-redis-secret
        - secret:
            items:
            - key: postgres-password
              path: postgres/psql-password
            name: gitlab-postgresql-password
        - secret:
            items:
            - key: registry-auth.key
              path: registry/gitlab-registry.key
            name: gitlab-registry-secret
        - secret:
            items:
            - key: shared_secret
              path: gitlab-workhorse/secret
            name: gitlab-gitlab-workhorse-secret
        - secret:
            items:
            - key: accesskey
              path: minio/accesskey
            - key: secretkey
              path: minio/secretkey
            name: gitlab-minio-secret
    - emptyDir:
        medium: Memory
      name: unicorn-secrets
    - emptyDir:
        medium: Memory
      name: workhorse-secrets
    - emptyDir: {}
      name: shared-upload-directory
    - emptyDir: {}
      name: shared-artifact-directory
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    message: 'The node was low on resource: memory. Container unicorn was using 962740Ki,
      which exceeds its request of 40M. '
    phase: Failed
    reason: Evicted
    startTime: "2020-04-01T03:40:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: d021e46742b5d6dd76265a6b1dd9c78cdeb6db74842f2cdaa710f411a97fbabb
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/podIP: 10.8.1.96/32
      prometheus.io/path: /-/metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-04-01T11:10:48Z"
    generateName: gitlab-unicorn-84d7dc6557-
    labels:
      app: unicorn
      pod-template-hash: 84d7dc6557
      release: gitlab
    name: gitlab-unicorn-84d7dc6557-q8gs4
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-unicorn-84d7dc6557
      uid: b3bedb01-4709-11ea-96d3-42010a80017a
    resourceVersion: "36398032"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-unicorn-84d7dc6557-q8gs4
    uid: 70a2d492-7409-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: unicorn
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: prometheus_multiproc_dir
        value: /metrics
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -SIGQUIT -f 'unicorn master'
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/liveness
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: unicorn
      ports:
      - containerPort: 8080
        name: unicorn
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/readiness
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /metrics
        name: unicorn-metrics
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /srv/gitlab/config/secrets.yml
        name: unicorn-secrets
        subPath: rails-secrets/secrets.yml
      - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
        name: unicorn-config
        subPath: smtp_settings.rb
      - mountPath: /srv/gitlab/INSTALLATION_TYPE
        name: unicorn-config
        subPath: installation_type
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITLAB_WORKHORSE_EXTRA_ARGS
      - name: GITLAB_WORKHORSE_LISTEN_PORT
        value: "8181"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: gitlab-workhorse
      ports:
      - containerPort: 8181
        name: workhorse
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: workhorse-config
      - mountPath: /etc/gitlab
        name: workhorse-secrets
        readOnly: true
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - -c
      - sh -x /config-unicorn/configure ; sh -x /config-workhorse/configure ; mkdir
        -p -m 3770 /tmp/gitlab
      command:
      - sh
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config-unicorn
        name: unicorn-config
        readOnly: true
      - mountPath: /config-workhorse
        name: workhorse-config
        readOnly: true
      - mountPath: /init-config
        name: init-unicorn-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: unicorn-secrets
      - mountPath: /init-secrets-workhorse
        name: workhorse-secrets
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - /scripts/wait-for-deps
      env:
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: WORKHORSE_ARCHIVE_CACHE_DISABLED
        value: "1"
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      name: dependencies
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: shared-tmp
    - emptyDir:
        medium: Memory
      name: unicorn-metrics
    - configMap:
        defaultMode: 420
        name: gitlab-unicorn
      name: unicorn-config
    - configMap:
        defaultMode: 420
        name: gitlab-workhorse-config
      name: workhorse-config
    - name: init-unicorn-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: secrets.yml
              path: rails-secrets/secrets.yml
            name: gitlab-rails-secret
        - secret:
            items:
            - key: secret
              path: shell/.gitlab_shell_secret
            name: gitlab-gitlab-shell-secret
        - secret:
            items:
            - key: token
              path: gitaly/gitaly_token
            name: gitlab-gitaly-secret
        - secret:
            items:
            - key: secret
              path: redis/password
            name: gitlab-redis-secret
        - secret:
            items:
            - key: postgres-password
              path: postgres/psql-password
            name: gitlab-postgresql-password
        - secret:
            items:
            - key: registry-auth.key
              path: registry/gitlab-registry.key
            name: gitlab-registry-secret
        - secret:
            items:
            - key: shared_secret
              path: gitlab-workhorse/secret
            name: gitlab-gitlab-workhorse-secret
        - secret:
            items:
            - key: accesskey
              path: minio/accesskey
            - key: secretkey
              path: minio/secretkey
            name: gitlab-minio-secret
    - emptyDir:
        medium: Memory
      name: unicorn-secrets
    - emptyDir:
        medium: Memory
      name: workhorse-secrets
    - emptyDir: {}
      name: shared-upload-directory
    - emptyDir: {}
      name: shared-artifact-directory
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    message: 'The node was low on resource: memory. Container unicorn was using 1088628Ki,
      which exceeds its request of 40M. '
    phase: Failed
    reason: Evicted
    startTime: "2020-04-01T11:10:48Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: d021e46742b5d6dd76265a6b1dd9c78cdeb6db74842f2cdaa710f411a97fbabb
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/podIP: 10.8.1.116/32
      prometheus.io/path: /-/metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-04-08T20:22:44Z"
    generateName: gitlab-unicorn-84d7dc6557-
    labels:
      app: unicorn
      pod-template-hash: 84d7dc6557
      release: gitlab
    name: gitlab-unicorn-84d7dc6557-vh6gk
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-unicorn-84d7dc6557
      uid: b3bedb01-4709-11ea-96d3-42010a80017a
    resourceVersion: "36422527"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-unicorn-84d7dc6557-vh6gk
    uid: b453a410-79d6-11ea-bb1c-42010a800057
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: unicorn
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: prometheus_multiproc_dir
        value: /metrics
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -SIGQUIT -f 'unicorn master'
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/liveness
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: unicorn
      ports:
      - containerPort: 8080
        name: unicorn
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/readiness
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /metrics
        name: unicorn-metrics
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /srv/gitlab/config/secrets.yml
        name: unicorn-secrets
        subPath: rails-secrets/secrets.yml
      - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
        name: unicorn-config
        subPath: smtp_settings.rb
      - mountPath: /srv/gitlab/INSTALLATION_TYPE
        name: unicorn-config
        subPath: installation_type
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITLAB_WORKHORSE_EXTRA_ARGS
      - name: GITLAB_WORKHORSE_LISTEN_PORT
        value: "8181"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: gitlab-workhorse
      ports:
      - containerPort: 8181
        name: workhorse
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: workhorse-config
      - mountPath: /etc/gitlab
        name: workhorse-secrets
        readOnly: true
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - -c
      - sh -x /config-unicorn/configure ; sh -x /config-workhorse/configure ; mkdir
        -p -m 3770 /tmp/gitlab
      command:
      - sh
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config-unicorn
        name: unicorn-config
        readOnly: true
      - mountPath: /config-workhorse
        name: workhorse-config
        readOnly: true
      - mountPath: /init-config
        name: init-unicorn-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: unicorn-secrets
      - mountPath: /init-secrets-workhorse
        name: workhorse-secrets
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - /scripts/wait-for-deps
      env:
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: WORKHORSE_ARCHIVE_CACHE_DISABLED
        value: "1"
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      name: dependencies
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: shared-tmp
    - emptyDir:
        medium: Memory
      name: unicorn-metrics
    - configMap:
        defaultMode: 420
        name: gitlab-unicorn
      name: unicorn-config
    - configMap:
        defaultMode: 420
        name: gitlab-workhorse-config
      name: workhorse-config
    - name: init-unicorn-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: secrets.yml
              path: rails-secrets/secrets.yml
            name: gitlab-rails-secret
        - secret:
            items:
            - key: secret
              path: shell/.gitlab_shell_secret
            name: gitlab-gitlab-shell-secret
        - secret:
            items:
            - key: token
              path: gitaly/gitaly_token
            name: gitlab-gitaly-secret
        - secret:
            items:
            - key: secret
              path: redis/password
            name: gitlab-redis-secret
        - secret:
            items:
            - key: postgres-password
              path: postgres/psql-password
            name: gitlab-postgresql-password
        - secret:
            items:
            - key: registry-auth.key
              path: registry/gitlab-registry.key
            name: gitlab-registry-secret
        - secret:
            items:
            - key: shared_secret
              path: gitlab-workhorse/secret
            name: gitlab-gitlab-workhorse-secret
        - secret:
            items:
            - key: accesskey
              path: minio/accesskey
            - key: secretkey
              path: minio/secretkey
            name: gitlab-minio-secret
    - emptyDir:
        medium: Memory
      name: unicorn-secrets
    - emptyDir:
        medium: Memory
      name: workhorse-secrets
    - emptyDir: {}
      name: shared-upload-directory
    - emptyDir: {}
      name: shared-artifact-directory
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-04-08T20:25:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-08T21:56:58Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-08T21:56:58Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-04-08T20:22:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://e0083fc3d3882c8517153eaa2910d21e2aa987e9f3f1339caf900f28aa953783
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee@sha256:6dba4d45e8a1f975604894e9fa12c28e5d30c126480ca4346ea449c25a69c4c5
      lastState: {}
      name: gitlab-workhorse
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-04-08T20:25:14Z"
    - containerID: docker://d3b3fd448cc460743ae3ce0cb0b841008380b38f2810b257fba7f9ab48b50a98
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee@sha256:b47d7878f286b55537822d6cf11ca4dd02796153ff0855cb5dac9f9d10991c72
      lastState:
        terminated:
          containerID: docker://45bc18ca22c5c6d902437a2a6516369c947d413f1bc602ebc2d92b74b1c74ade
          exitCode: 137
          finishedAt: "2020-04-08T21:53:25Z"
          reason: Error
          startedAt: "2020-04-08T21:49:59Z"
      name: unicorn
      ready: true
      restartCount: 17
      state:
        running:
          startedAt: "2020-04-08T21:53:09Z"
    hostIP: 10.128.0.37
    initContainerStatuses:
    - containerID: docker://7cd8a6323315b43c4abc6423feced1aa558d58b7cce987420804769e177f3655
      image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/alpine-certificates@sha256:00ce9a585179e6b22c9bfea9ba82552630eab0bd25da4f13282b588b2ad022dc
      lastState: {}
      name: certificates
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://7cd8a6323315b43c4abc6423feced1aa558d58b7cce987420804769e177f3655
          exitCode: 0
          finishedAt: "2020-04-08T20:23:17Z"
          reason: Completed
          startedAt: "2020-04-08T20:23:15Z"
    - containerID: docker://1afa6240536ae023d64d79a764dd622b34eaaf1030cc2b42596563c0ca4132b1
      image: busybox:latest
      imageID: docker-pullable://busybox@sha256:afe605d272837ce1732f390966166c2afff5391208ddd57de10942748694049d
      lastState: {}
      name: configure
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://1afa6240536ae023d64d79a764dd622b34eaaf1030cc2b42596563c0ca4132b1
          exitCode: 0
          finishedAt: "2020-04-08T20:23:24Z"
          reason: Completed
          startedAt: "2020-04-08T20:23:24Z"
    - containerID: docker://dce72145130266f394a4cec7c7f70a21031665b2c5eb69d1f3c28faa32a7a302
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imageID: docker-pullable://registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee@sha256:b47d7878f286b55537822d6cf11ca4dd02796153ff0855cb5dac9f9d10991c72
      lastState: {}
      name: dependencies
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://dce72145130266f394a4cec7c7f70a21031665b2c5eb69d1f3c28faa32a7a302
          exitCode: 0
          finishedAt: "2020-04-08T20:25:08Z"
          reason: Completed
          startedAt: "2020-04-08T20:23:27Z"
    phase: Running
    podIP: 10.8.1.116
    qosClass: Burstable
    startTime: "2020-04-08T20:22:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: d021e46742b5d6dd76265a6b1dd9c78cdeb6db74842f2cdaa710f411a97fbabb
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/podIP: 10.8.1.61/32
      prometheus.io/path: /-/metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-03-31T21:30:54Z"
    generateName: gitlab-unicorn-84d7dc6557-
    labels:
      app: unicorn
      pod-template-hash: 84d7dc6557
      release: gitlab
    name: gitlab-unicorn-84d7dc6557-wfcr5
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-unicorn-84d7dc6557
      uid: b3bedb01-4709-11ea-96d3-42010a80017a
    resourceVersion: "33364306"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-unicorn-84d7dc6557-wfcr5
    uid: e7183eb6-7396-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: unicorn
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: prometheus_multiproc_dir
        value: /metrics
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -SIGQUIT -f 'unicorn master'
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/liveness
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: unicorn
      ports:
      - containerPort: 8080
        name: unicorn
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/readiness
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /metrics
        name: unicorn-metrics
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /srv/gitlab/config/secrets.yml
        name: unicorn-secrets
        subPath: rails-secrets/secrets.yml
      - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
        name: unicorn-config
        subPath: smtp_settings.rb
      - mountPath: /srv/gitlab/INSTALLATION_TYPE
        name: unicorn-config
        subPath: installation_type
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITLAB_WORKHORSE_EXTRA_ARGS
      - name: GITLAB_WORKHORSE_LISTEN_PORT
        value: "8181"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: gitlab-workhorse
      ports:
      - containerPort: 8181
        name: workhorse
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: workhorse-config
      - mountPath: /etc/gitlab
        name: workhorse-secrets
        readOnly: true
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - -c
      - sh -x /config-unicorn/configure ; sh -x /config-workhorse/configure ; mkdir
        -p -m 3770 /tmp/gitlab
      command:
      - sh
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config-unicorn
        name: unicorn-config
        readOnly: true
      - mountPath: /config-workhorse
        name: workhorse-config
        readOnly: true
      - mountPath: /init-config
        name: init-unicorn-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: unicorn-secrets
      - mountPath: /init-secrets-workhorse
        name: workhorse-secrets
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - /scripts/wait-for-deps
      env:
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: WORKHORSE_ARCHIVE_CACHE_DISABLED
        value: "1"
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      name: dependencies
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: shared-tmp
    - emptyDir:
        medium: Memory
      name: unicorn-metrics
    - configMap:
        defaultMode: 420
        name: gitlab-unicorn
      name: unicorn-config
    - configMap:
        defaultMode: 420
        name: gitlab-workhorse-config
      name: workhorse-config
    - name: init-unicorn-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: secrets.yml
              path: rails-secrets/secrets.yml
            name: gitlab-rails-secret
        - secret:
            items:
            - key: secret
              path: shell/.gitlab_shell_secret
            name: gitlab-gitlab-shell-secret
        - secret:
            items:
            - key: token
              path: gitaly/gitaly_token
            name: gitlab-gitaly-secret
        - secret:
            items:
            - key: secret
              path: redis/password
            name: gitlab-redis-secret
        - secret:
            items:
            - key: postgres-password
              path: postgres/psql-password
            name: gitlab-postgresql-password
        - secret:
            items:
            - key: registry-auth.key
              path: registry/gitlab-registry.key
            name: gitlab-registry-secret
        - secret:
            items:
            - key: shared_secret
              path: gitlab-workhorse/secret
            name: gitlab-gitlab-workhorse-secret
        - secret:
            items:
            - key: accesskey
              path: minio/accesskey
            - key: secretkey
              path: minio/secretkey
            name: gitlab-minio-secret
    - emptyDir:
        medium: Memory
      name: unicorn-secrets
    - emptyDir:
        medium: Memory
      name: workhorse-secrets
    - emptyDir: {}
      name: shared-upload-directory
    - emptyDir: {}
      name: shared-artifact-directory
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    message: 'The node was low on resource: memory. Container unicorn was using 899824Ki,
      which exceeds its request of 40M. '
    phase: Failed
    reason: Evicted
    startTime: "2020-03-31T21:30:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: d021e46742b5d6dd76265a6b1dd9c78cdeb6db74842f2cdaa710f411a97fbabb
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/podIP: 10.8.1.60/32
      prometheus.io/path: /-/metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-03-31T20:52:18Z"
    generateName: gitlab-unicorn-84d7dc6557-
    labels:
      app: unicorn
      pod-template-hash: 84d7dc6557
      release: gitlab
    name: gitlab-unicorn-84d7dc6557-xkxs4
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-unicorn-84d7dc6557
      uid: b3bedb01-4709-11ea-96d3-42010a80017a
    resourceVersion: "33343001"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-unicorn-84d7dc6557-xkxs4
    uid: 82c0f109-7391-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: unicorn
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: prometheus_multiproc_dir
        value: /metrics
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -SIGQUIT -f 'unicorn master'
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/liveness
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: unicorn
      ports:
      - containerPort: 8080
        name: unicorn
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/readiness
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /metrics
        name: unicorn-metrics
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /srv/gitlab/config/secrets.yml
        name: unicorn-secrets
        subPath: rails-secrets/secrets.yml
      - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
        name: unicorn-config
        subPath: smtp_settings.rb
      - mountPath: /srv/gitlab/INSTALLATION_TYPE
        name: unicorn-config
        subPath: installation_type
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITLAB_WORKHORSE_EXTRA_ARGS
      - name: GITLAB_WORKHORSE_LISTEN_PORT
        value: "8181"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: gitlab-workhorse
      ports:
      - containerPort: 8181
        name: workhorse
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: workhorse-config
      - mountPath: /etc/gitlab
        name: workhorse-secrets
        readOnly: true
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - -c
      - sh -x /config-unicorn/configure ; sh -x /config-workhorse/configure ; mkdir
        -p -m 3770 /tmp/gitlab
      command:
      - sh
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config-unicorn
        name: unicorn-config
        readOnly: true
      - mountPath: /config-workhorse
        name: workhorse-config
        readOnly: true
      - mountPath: /init-config
        name: init-unicorn-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: unicorn-secrets
      - mountPath: /init-secrets-workhorse
        name: workhorse-secrets
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - /scripts/wait-for-deps
      env:
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: WORKHORSE_ARCHIVE_CACHE_DISABLED
        value: "1"
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      name: dependencies
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: shared-tmp
    - emptyDir:
        medium: Memory
      name: unicorn-metrics
    - configMap:
        defaultMode: 420
        name: gitlab-unicorn
      name: unicorn-config
    - configMap:
        defaultMode: 420
        name: gitlab-workhorse-config
      name: workhorse-config
    - name: init-unicorn-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: secrets.yml
              path: rails-secrets/secrets.yml
            name: gitlab-rails-secret
        - secret:
            items:
            - key: secret
              path: shell/.gitlab_shell_secret
            name: gitlab-gitlab-shell-secret
        - secret:
            items:
            - key: token
              path: gitaly/gitaly_token
            name: gitlab-gitaly-secret
        - secret:
            items:
            - key: secret
              path: redis/password
            name: gitlab-redis-secret
        - secret:
            items:
            - key: postgres-password
              path: postgres/psql-password
            name: gitlab-postgresql-password
        - secret:
            items:
            - key: registry-auth.key
              path: registry/gitlab-registry.key
            name: gitlab-registry-secret
        - secret:
            items:
            - key: shared_secret
              path: gitlab-workhorse/secret
            name: gitlab-gitlab-workhorse-secret
        - secret:
            items:
            - key: accesskey
              path: minio/accesskey
            - key: secretkey
              path: minio/secretkey
            name: gitlab-minio-secret
    - emptyDir:
        medium: Memory
      name: unicorn-secrets
    - emptyDir:
        medium: Memory
      name: workhorse-secrets
    - emptyDir: {}
      name: shared-upload-directory
    - emptyDir: {}
      name: shared-artifact-directory
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    message: 'The node was low on resource: memory. Container unicorn was using 888400Ki,
      which exceeds its request of 40M. '
    phase: Failed
    reason: Evicted
    startTime: "2020-03-31T20:52:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: d021e46742b5d6dd76265a6b1dd9c78cdeb6db74842f2cdaa710f411a97fbabb
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      cni.projectcalico.org/podIP: 10.8.1.59/32
      prometheus.io/path: /-/metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-03-31T20:10:54Z"
    generateName: gitlab-unicorn-84d7dc6557-
    labels:
      app: unicorn
      pod-template-hash: 84d7dc6557
      release: gitlab
    name: gitlab-unicorn-84d7dc6557-xw7hr
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gitlab-unicorn-84d7dc6557
      uid: b3bedb01-4709-11ea-96d3-42010a80017a
    resourceVersion: "33333621"
    selfLink: /api/v1/namespaces/gitlab/pods/gitlab-unicorn-84d7dc6557-xw7hr
    uid: ba1dc4cb-738b-11ea-8ebf-42010a800207
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app: unicorn
                release: gitlab
            topologyKey: kubernetes.io/hostname
          weight: 1
    containers:
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: prometheus_multiproc_dir
        value: /metrics
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/bash
            - -c
            - pkill -SIGQUIT -f 'unicorn master'
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/liveness
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: unicorn
      ports:
      - containerPort: 8080
        name: unicorn
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/readiness
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /metrics
        name: unicorn-metrics
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /srv/gitlab/config/secrets.yml
        name: unicorn-secrets
        subPath: rails-secrets/secrets.yml
      - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
        name: unicorn-config
        subPath: smtp_settings.rb
      - mountPath: /srv/gitlab/INSTALLATION_TYPE
        name: unicorn-config
        subPath: installation_type
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - env:
      - name: TMPDIR
        value: /tmp/gitlab
      - name: GITLAB_WORKHORSE_EXTRA_ARGS
      - name: GITLAB_WORKHORSE_LISTEN_PORT
        value: "8181"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        initialDelaySeconds: 20
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 30
      name: gitlab-workhorse
      ports:
      - containerPort: 8181
        name: workhorse
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /scripts/healthcheck
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 2
      resources:
        requests:
          cpu: 10m
          memory: 40M
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: workhorse-config
      - mountPath: /etc/gitlab
        name: workhorse-secrets
        readOnly: true
      - mountPath: /srv/gitlab/public/uploads/tmp
        name: shared-upload-directory
      - mountPath: /srv/gitlab/shared
        name: shared-artifact-directory
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /etc/ssl/certs/
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
      imagePullPolicy: IfNotPresent
      name: certificates
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - -c
      - sh -x /config-unicorn/configure ; sh -x /config-workhorse/configure ; mkdir
        -p -m 3770 /tmp/gitlab
      command:
      - sh
      image: busybox:latest
      imagePullPolicy: Always
      name: configure
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /config-unicorn
        name: unicorn-config
        readOnly: true
      - mountPath: /config-workhorse
        name: workhorse-config
        readOnly: true
      - mountPath: /init-config
        name: init-unicorn-secrets
        readOnly: true
      - mountPath: /init-secrets
        name: unicorn-secrets
      - mountPath: /init-secrets-workhorse
        name: workhorse-secrets
      - mountPath: /tmp
        name: shared-tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    - args:
      - /scripts/wait-for-deps
      env:
      - name: GITALY_FEATURE_DEFAULT_ON
        value: "1"
      - name: CONFIG_TEMPLATE_DIRECTORY
        value: /var/opt/gitlab/templates
      - name: CONFIG_DIRECTORY
        value: /srv/gitlab/config
      - name: WORKHORSE_ARCHIVE_CACHE_DISABLED
        value: "1"
      image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
      imagePullPolicy: IfNotPresent
      name: dependencies
      resources:
        requests:
          cpu: 50m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/opt/gitlab/templates
        name: unicorn-config
      - mountPath: /etc/gitlab
        name: unicorn-secrets
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jjkq5
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: shared-tmp
    - emptyDir:
        medium: Memory
      name: unicorn-metrics
    - configMap:
        defaultMode: 420
        name: gitlab-unicorn
      name: unicorn-config
    - configMap:
        defaultMode: 420
        name: gitlab-workhorse-config
      name: workhorse-config
    - name: init-unicorn-secrets
      projected:
        defaultMode: 256
        sources:
        - secret:
            items:
            - key: secrets.yml
              path: rails-secrets/secrets.yml
            name: gitlab-rails-secret
        - secret:
            items:
            - key: secret
              path: shell/.gitlab_shell_secret
            name: gitlab-gitlab-shell-secret
        - secret:
            items:
            - key: token
              path: gitaly/gitaly_token
            name: gitlab-gitaly-secret
        - secret:
            items:
            - key: secret
              path: redis/password
            name: gitlab-redis-secret
        - secret:
            items:
            - key: postgres-password
              path: postgres/psql-password
            name: gitlab-postgresql-password
        - secret:
            items:
            - key: registry-auth.key
              path: registry/gitlab-registry.key
            name: gitlab-registry-secret
        - secret:
            items:
            - key: shared_secret
              path: gitlab-workhorse/secret
            name: gitlab-gitlab-workhorse-secret
        - secret:
            items:
            - key: accesskey
              path: minio/accesskey
            - key: secretkey
              path: minio/secretkey
            name: gitlab-minio-secret
    - emptyDir:
        medium: Memory
      name: unicorn-secrets
    - emptyDir:
        medium: Memory
      name: workhorse-secrets
    - emptyDir: {}
      name: shared-upload-directory
    - emptyDir: {}
      name: shared-artifact-directory
    - emptyDir:
        medium: Memory
      name: etc-ssl-certs
    - name: default-token-jjkq5
      secret:
        defaultMode: 420
        secretName: default-token-jjkq5
  status:
    message: 'The node was low on resource: memory. Container unicorn was using 830064Ki,
      which exceeds its request of 40M. '
    phase: Failed
    reason: Evicted
    startTime: "2020-03-31T20:10:57Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.163/32
    creationTimestamp: "2020-03-31T18:40:28Z"
    generateName: adservice-84b8749d65-
    labels:
      app: adservice
      pod-template-hash: 84b8749d65
    name: adservice-84b8749d65-ngsw4
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: adservice-84b8749d65
      uid: 5d739cff-431c-11ea-96d3-42010a80017a
    resourceVersion: "37201481"
    selfLink: /api/v1/namespaces/hipster/pods/adservice-84b8749d65-ngsw4
    uid: 17c7b595-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: PORT
        value: "9555"
      image: gcr.io/google-samples/microservices-demo/adservice:v0.1.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:9555
        failureThreshold: 3
        initialDelaySeconds: 20
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 1
      name: server
      ports:
      - containerPort: 9555
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:9555
        failureThreshold: 3
        initialDelaySeconds: 20
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 100m
          memory: 300Mi
        requests:
          cpu: 50m
          memory: 180Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-pncck
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 5
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-pncck
      secret:
        defaultMode: 420
        secretName: default-token-pncck
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:10:28Z"
      message: 'containers with unready status: [server]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:10:28Z"
      message: 'containers with unready status: [server]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://1933910c42fa24b212fde5b1b9bc03160b09632a4c59ce08abc8fbfee9850108
      image: gcr.io/google-samples/microservices-demo/adservice:v0.1.3
      imageID: docker-pullable://gcr.io/google-samples/microservices-demo/adservice@sha256:5edd0b072ec9f582693a4de28a50721028868dad453910c867268c231af734e1
      lastState:
        terminated:
          containerID: docker://1933910c42fa24b212fde5b1b9bc03160b09632a4c59ce08abc8fbfee9850108
          exitCode: 143
          finishedAt: "2020-04-10T23:49:50Z"
          reason: Error
          startedAt: "2020-04-10T23:48:51Z"
      name: server
      ready: false
      restartCount: 486
      state:
        waiting:
          message: Back-off 5m0s restarting failed container=server pod=adservice-84b8749d65-ngsw4_hipster(17c7b595-737f-11ea-8ebf-42010a800207)
          reason: CrashLoopBackOff
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.163
    qosClass: Burstable
    startTime: "2020-03-31T18:40:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.164/32
    creationTimestamp: "2020-03-31T18:40:28Z"
    generateName: cartservice-55f8ccc8f4-
    labels:
      app: cartservice
      pod-template-hash: 55f8ccc8f4
    name: cartservice-55f8ccc8f4-c5flx
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cartservice-55f8ccc8f4
      uid: d8fc4c22-4390-11ea-96d3-42010a80017a
    resourceVersion: "36841166"
    selfLink: /api/v1/namespaces/hipster/pods/cartservice-55f8ccc8f4-c5flx
    uid: 17e679b4-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: REDIS_ADDR
        value: redis-cart:6379
      - name: PORT
        value: "7070"
      - name: LISTEN_ADDR
        value: 0.0.0.0
      image: gcr.io/google-samples/microservices-demo/cartservice:v0.1.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:7070
          - -rpc-timeout=5s
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: server
      ports:
      - containerPort: 7070
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:7070
          - -rpc-timeout=5s
        failureThreshold: 3
        initialDelaySeconds: 15
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 25m
          memory: 64Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-pncck
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 5
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-pncck
      secret:
        defaultMode: 420
        secretName: default-token-pncck
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:10:50Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:10:50Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://b25cf39a3b4de2ef4b8e61bd4ebc0d9633fcc9c965418601e580e43e587c6a94
      image: gcr.io/google-samples/microservices-demo/cartservice:v0.1.3
      imageID: docker-pullable://gcr.io/google-samples/microservices-demo/cartservice@sha256:4e9c6ad41328210e0a4b181add097d60900ecc1b990527a832a5d47e3f209d69
      lastState:
        terminated:
          containerID: docker://058b1a26bed15c5b7f0f5a87e18941559c0a6b4c0b23d82de1e72a6ca113bc84
          exitCode: 137
          finishedAt: "2020-04-10T00:10:14Z"
          reason: Error
          startedAt: "2020-04-10T00:08:01Z"
      name: server
      ready: true
      restartCount: 3
      state:
        running:
          startedAt: "2020-04-10T00:10:26Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.164
    qosClass: Burstable
    startTime: "2020-03-31T18:40:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.159/32
    creationTimestamp: "2020-03-31T18:40:29Z"
    generateName: checkoutservice-77d8f889b7-
    labels:
      app: checkoutservice
      pod-template-hash: 77d8f889b7
    name: checkoutservice-77d8f889b7-wkkcp
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: checkoutservice-77d8f889b7
      uid: 286e8b6c-431b-11ea-96d3-42010a80017a
    resourceVersion: "33302782"
    selfLink: /api/v1/namespaces/hipster/pods/checkoutservice-77d8f889b7-wkkcp
    uid: 1858ff94-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: PORT
        value: "5050"
      - name: PRODUCT_CATALOG_SERVICE_ADDR
        value: productcatalogservice:3550
      - name: SHIPPING_SERVICE_ADDR
        value: shippingservice:50051
      - name: PAYMENT_SERVICE_ADDR
        value: paymentservice:50051
      - name: EMAIL_SERVICE_ADDR
        value: emailservice:5000
      - name: CURRENCY_SERVICE_ADDR
        value: currencyservice:7000
      - name: CART_SERVICE_ADDR
        value: cartservice:7070
      image: gcr.io/google-samples/microservices-demo/checkoutservice:v0.1.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:5050
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: server
      ports:
      - containerPort: 5050
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:5050
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 20m
          memory: 128Mi
        requests:
          cpu: 5m
          memory: 64Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-pncck
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-pncck
      secret:
        defaultMode: 420
        secretName: default-token-pncck
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://2babf9ced9e14464405fa116437cba14fa0d59b35132708518034e972a1d7664
      image: gcr.io/google-samples/microservices-demo/checkoutservice:v0.1.3
      imageID: docker-pullable://gcr.io/google-samples/microservices-demo/checkoutservice@sha256:099c35a454d2610de6634865a2f765706a60351469eb2dcb067d271212649566
      lastState: {}
      name: server
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-03-31T18:42:02Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.159
    qosClass: Burstable
    startTime: "2020-03-31T18:40:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.160/32
    creationTimestamp: "2020-03-31T18:40:28Z"
    generateName: currencyservice-7775f7949-
    labels:
      app: currencyservice
      pod-template-hash: 7775f7949
    name: currencyservice-7775f7949-4bx5q
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: currencyservice-7775f7949
      uid: 5d09f261-431c-11ea-96d3-42010a80017a
    resourceVersion: "36841070"
    selfLink: /api/v1/namespaces/hipster/pods/currencyservice-7775f7949-4bx5q
    uid: 17fda23a-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: PORT
        value: "7000"
      image: gcr.io/google-samples/microservices-demo/currencyservice:v0.1.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:7000
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: server
      ports:
      - containerPort: 7000
        name: grpc
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:7000
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-pncck
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 5
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-pncck
      secret:
        defaultMode: 420
        secretName: default-token-pncck
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:10:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:10:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://11870ba241486eb99afa132ed397c1114394165f3b866ad297b3e55d5b7ec56a
      image: gcr.io/google-samples/microservices-demo/currencyservice:v0.1.3
      imageID: docker-pullable://gcr.io/google-samples/microservices-demo/currencyservice@sha256:52818623cf85b8a0f01709b06ce94bde44b9ee27203c35b832663f55cde1fe83
      lastState:
        terminated:
          containerID: docker://7a0fb1f98d802e50f511126b7694c652b5f6636e86ffe01c4e699bc59435f22a
          exitCode: 137
          finishedAt: "2020-04-10T00:09:15Z"
          reason: Error
          startedAt: "2020-04-10T00:08:00Z"
      name: server
      ready: true
      restartCount: 4
      state:
        running:
          startedAt: "2020-04-10T00:09:32Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.160
    qosClass: Burstable
    startTime: "2020-03-31T18:40:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.161/32
    creationTimestamp: "2020-03-31T18:40:28Z"
    generateName: emailservice-6545668f4f-
    labels:
      app: emailservice
      pod-template-hash: 6545668f4f
    name: emailservice-6545668f4f-x5lrn
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: emailservice-6545668f4f
      uid: 5bd1e06f-431c-11ea-96d3-42010a80017a
    resourceVersion: "36840780"
    selfLink: /api/v1/namespaces/hipster/pods/emailservice-6545668f4f-x5lrn
    uid: 1805ecb5-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: PORT
        value: "8080"
      - name: ENABLE_PROFILER
        value: "0"
      image: gcr.io/google-samples/microservices-demo/emailservice:v0.1.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:8080
        failureThreshold: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: server
      ports:
      - containerPort: 8080
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:8080
        failureThreshold: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-pncck
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 5
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-pncck
      secret:
        defaultMode: 420
        secretName: default-token-pncck
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:09:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:09:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://94691b724b1352bb8ac1ffee74605682b5971b8d870ec02c89aef1ce69da551c
      image: gcr.io/google-samples/microservices-demo/emailservice:v0.1.3
      imageID: docker-pullable://gcr.io/google-samples/microservices-demo/emailservice@sha256:d42ee712cbb4806a8b922e303a5e6734f342dfb6c92c81284a289912165b7314
      lastState:
        terminated:
          containerID: docker://68ade6d76287dd5dd6b961461e65864eccc2aef8d0fd56bbe5c472fc9c534494
          exitCode: 137
          finishedAt: "2020-03-31T18:42:41Z"
          reason: Error
          startedAt: "2020-03-31T18:42:01Z"
      name: server
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-03-31T18:42:44Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.161
    qosClass: Burstable
    startTime: "2020-03-31T18:40:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.169/32
      sidecar.istio.io/rewriteAppHTTPProbers: "true"
    creationTimestamp: "2020-03-31T18:40:29Z"
    generateName: frontend-7dbdf6c769-
    labels:
      app: frontend
      pod-template-hash: 7dbdf6c769
    name: frontend-7dbdf6c769-b2t8p
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: frontend-7dbdf6c769
      uid: 29bbf1e3-431b-11ea-96d3-42010a80017a
    resourceVersion: "36841020"
    selfLink: /api/v1/namespaces/hipster/pods/frontend-7dbdf6c769-b2t8p
    uid: 1860d02f-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: PORT
        value: "8080"
      - name: PRODUCT_CATALOG_SERVICE_ADDR
        value: productcatalogservice:3550
      - name: CURRENCY_SERVICE_ADDR
        value: currencyservice:7000
      - name: CART_SERVICE_ADDR
        value: cartservice:7070
      - name: RECOMMENDATION_SERVICE_ADDR
        value: recommendationservice:8080
      - name: SHIPPING_SERVICE_ADDR
        value: shippingservice:50051
      - name: CHECKOUT_SERVICE_ADDR
        value: checkoutservice:5050
      - name: AD_SERVICE_ADDR
        value: adservice:9555
      image: gcr.io/google-samples/microservices-demo/frontend:v0.1.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          httpHeaders:
          - name: Cookie
            value: shop_session-id=x-liveness-probe
          path: /_healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: server
      ports:
      - containerPort: 8080
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          httpHeaders:
          - name: Cookie
            value: shop_session-id=x-readiness-probe
          path: /_healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 20m
          memory: 128Mi
        requests:
          cpu: 5m
          memory: 64Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-pncck
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-pncck
      secret:
        defaultMode: 420
        secretName: default-token-pncck
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:10:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:10:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://6c8a8a5e8e85f929a871eb55c29197d1992e5ce4eabcf06f939037aa861e971d
      image: gcr.io/google-samples/microservices-demo/frontend:v0.1.3
      imageID: docker-pullable://gcr.io/google-samples/microservices-demo/frontend@sha256:0c72f37ed9aac1e65bccafc0ce7675ab9d1b6a407cdcefb2b9a608eec83490d5
      lastState: {}
      name: server
      ready: true
      restartCount: 2
      state:
        running:
          startedAt: "2020-04-10T00:10:02Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.169
    qosClass: Burstable
    startTime: "2020-03-31T18:40:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.158/32
      sidecar.istio.io/rewriteAppHTTPProbers: "true"
    creationTimestamp: "2020-03-31T18:40:29Z"
    generateName: loadgenerator-547598db87-
    labels:
      app: loadgenerator
      pod-template-hash: 547598db87
    name: loadgenerator-547598db87-4rj9d
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: loadgenerator-547598db87
      uid: 2d2e941d-431b-11ea-96d3-42010a80017a
    resourceVersion: "33303375"
    selfLink: /api/v1/namespaces/hipster/pods/loadgenerator-547598db87-4rj9d
    uid: 18776f0e-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: FRONTEND_ADDR
        value: frontend:80
      - name: USERS
        value: "10"
      image: gcr.io/google-samples/microservices-demo/loadgenerator:v0.1.3
      imagePullPolicy: IfNotPresent
      name: main
      resources:
        limits:
          cpu: 50m
          memory: 512Mi
        requests:
          cpu: 10m
          memory: 256Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-pncck
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 5
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-pncck
      secret:
        defaultMode: 420
        secretName: default-token-pncck
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:44:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:44:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://b52c6d22239ea10925f0d6bb2879d1ff65b4a0bd2d8e6196808989e835c12727
      image: gcr.io/google-samples/microservices-demo/loadgenerator:v0.1.3
      imageID: docker-pullable://gcr.io/google-samples/microservices-demo/loadgenerator@sha256:d0701c6a89b4b47b90932d59f84d8840f093e54ceec1ca2d4789ed8daac752e9
      lastState:
        terminated:
          containerID: docker://23e1c69c72f1ec6bb4150dc4660671b07eacc661f261a2a5d2a80ed68bcf090d
          exitCode: 7
          finishedAt: "2020-03-31T18:44:15Z"
          reason: Error
          startedAt: "2020-03-31T18:42:01Z"
      name: main
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-03-31T18:44:16Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.158
    qosClass: Burstable
    startTime: "2020-03-31T18:40:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.173/32
    creationTimestamp: "2020-03-31T18:40:28Z"
    generateName: paymentservice-6b9d88465f-
    labels:
      app: paymentservice
      pod-template-hash: 6b9d88465f
    name: paymentservice-6b9d88465f-6lflc
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: paymentservice-6b9d88465f
      uid: 5c7abd32-431c-11ea-96d3-42010a80017a
    resourceVersion: "36841076"
    selfLink: /api/v1/namespaces/hipster/pods/paymentservice-6b9d88465f-6lflc
    uid: 17a2e5df-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: PORT
        value: "50051"
      image: gcr.io/google-samples/microservices-demo/paymentservice:v0.1.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:50051
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: server
      ports:
      - containerPort: 50051
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:50051
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 100m
          memory: 128Mi
        requests:
          cpu: 50m
          memory: 64Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-pncck
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 5
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-pncck
      secret:
        defaultMode: 420
        secretName: default-token-pncck
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:10:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:10:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://ea05466dd13fabe5c8c846b5f99cb15f5c01d59c5a22d7e548653bd1e27a6700
      image: gcr.io/google-samples/microservices-demo/paymentservice:v0.1.3
      imageID: docker-pullable://gcr.io/google-samples/microservices-demo/paymentservice@sha256:c82409d85cd2006ee13537d3f404b0c85d9fbb55642450602e015f63abc571e1
      lastState:
        terminated:
          containerID: docker://e4f660ad6d43890e11b72210b4727ff6000738e844ed61440376c78fcb485c34
          exitCode: 137
          finishedAt: "2020-04-10T00:08:31Z"
          reason: Error
          startedAt: "2020-03-31T18:42:40Z"
      name: server
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-10T00:08:45Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.173
    qosClass: Burstable
    startTime: "2020-03-31T18:42:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.77/32
    creationTimestamp: "2020-03-31T18:40:25Z"
    generateName: productcatalogservice-7f5dc87d7-
    labels:
      app: productcatalogservice
      pod-template-hash: 7f5dc87d7
    name: productcatalogservice-7f5dc87d7-tsndv
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: productcatalogservice-7f5dc87d7
      uid: 2b65422c-431b-11ea-96d3-42010a80017a
    resourceVersion: "37185767"
    selfLink: /api/v1/namespaces/hipster/pods/productcatalogservice-7f5dc87d7-tsndv
    uid: 15e7ad50-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: PORT
        value: "3550"
      image: gcr.io/google-samples/microservices-demo/productcatalogservice:v0.1.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:3550
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: server
      ports:
      - containerPort: 3550
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:3550
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 20m
          memory: 128Mi
        requests:
          cpu: 5m
          memory: 64Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-pncck
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 5
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-pncck
      secret:
        defaultMode: 420
        secretName: default-token-pncck
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:25Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T22:40:33Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T22:40:33Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:25Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://4f7d3fb94c61b08a14c3547b604021662ace8121def9b4381556d47963d43ac2
      image: gcr.io/google-samples/microservices-demo/productcatalogservice:v0.1.3
      imageID: docker-pullable://gcr.io/google-samples/microservices-demo/productcatalogservice@sha256:cb4fe6bb148ef473e34b978c90df4249370b2a5a13bd789455aaa97cc367aaa1
      lastState:
        terminated:
          containerID: docker://0a728c251ef9b2d0e042a4f388a05e91034f70868b907748b10ecd7a31273f78
          exitCode: 2
          finishedAt: "2020-04-09T20:35:05Z"
          reason: Error
          startedAt: "2020-04-09T20:33:51Z"
      name: server
      ready: true
      restartCount: 29
      state:
        running:
          startedAt: "2020-04-09T20:35:19Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.77
    qosClass: Burstable
    startTime: "2020-03-31T18:40:25Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.162/32
    creationTimestamp: "2020-03-31T18:40:29Z"
    generateName: recommendationservice-744d5589c7-
    labels:
      app: recommendationservice
      pod-template-hash: 744d5589c7
    name: recommendationservice-744d5589c7-njg8v
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: recommendationservice-744d5589c7
      uid: 29151317-431b-11ea-96d3-42010a80017a
    resourceVersion: "37202372"
    selfLink: /api/v1/namespaces/hipster/pods/recommendationservice-744d5589c7-njg8v
    uid: 181c9c8d-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: PORT
        value: "8080"
      - name: PRODUCT_CATALOG_SERVICE_ADDR
        value: productcatalogservice:3550
      - name: ENABLE_PROFILER
        value: "0"
      image: gcr.io/google-samples/microservices-demo/recommendationservice:v0.1.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:8080
        failureThreshold: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: server
      ports:
      - containerPort: 8080
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:8080
        failureThreshold: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 20m
          memory: 450Mi
        requests:
          cpu: 5m
          memory: 220Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-pncck
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 5
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-pncck
      secret:
        defaultMode: 420
        secretName: default-token-pncck
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:10:36Z"
      message: 'containers with unready status: [server]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:10:36Z"
      message: 'containers with unready status: [server]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://567df6575d41fdcd81d6f75f27d1d40fc101195db5958b699d6517ffb9fe519e
      image: gcr.io/google-samples/microservices-demo/recommendationservice:v0.1.3
      imageID: docker-pullable://gcr.io/google-samples/microservices-demo/recommendationservice@sha256:4ad2659fbe0a68bdfebcf86b8d95c5734f44014d484c94d9ffe0d313eed57f4e
      lastState:
        terminated:
          containerID: docker://91b211ec0a1207568c71e275d1e7805c32b516ee289a19e1db3d46224b65676d
          exitCode: 137
          finishedAt: "2020-04-10T23:53:46Z"
          reason: Error
          startedAt: "2020-04-10T23:53:20Z"
      name: server
      ready: false
      restartCount: 4958
      state:
        running:
          startedAt: "2020-04-10T23:53:50Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.162
    qosClass: Burstable
    startTime: "2020-03-31T18:40:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.172/32
    creationTimestamp: "2020-03-31T18:40:27Z"
    generateName: redis-cart-58764b9d5d-
    labels:
      app: redis-cart
      pod-template-hash: 58764b9d5d
    name: redis-cart-58764b9d5d-t4fp8
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: redis-cart-58764b9d5d
      uid: 2f3a7fa6-431b-11ea-96d3-42010a80017a
    resourceVersion: "33302847"
    selfLink: /api/v1/namespaces/hipster/pods/redis-cart-58764b9d5d-t4fp8
    uid: 17475b44-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - image: redis:alpine
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        periodSeconds: 5
        successThreshold: 1
        tcpSocket:
          port: 6379
        timeoutSeconds: 1
      name: redis
      ports:
      - containerPort: 6379
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        periodSeconds: 5
        successThreshold: 1
        tcpSocket:
          port: 6379
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 75m
          memory: 256Mi
        requests:
          cpu: 40m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: redis-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-pncck
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: redis-data
    - name: default-token-pncck
      secret:
        defaultMode: 420
        secretName: default-token-pncck
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:47Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:47Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://988421149a66a3bf352187126a30fe6885ce9a44101686b6feefcbd1eaa0bc58
      image: redis:alpine
      imageID: docker-pullable://redis@sha256:cda5d02e4ea900a8d52e834bc3158e83b8a87a5b44ae081885aecf9b156dcff1
      lastState: {}
      name: redis
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-03-31T18:42:40Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.172
    qosClass: Burstable
    startTime: "2020-03-31T18:42:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.74/32
    creationTimestamp: "2020-03-31T18:40:26Z"
    generateName: shippingservice-5f96974545-
    labels:
      app: shippingservice
      pod-template-hash: 5f96974545
    name: shippingservice-5f96974545-q6dm9
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: shippingservice-5f96974545
      uid: 2e796d8d-431b-11ea-96d3-42010a80017a
    resourceVersion: "37192530"
    selfLink: /api/v1/namespaces/hipster/pods/shippingservice-5f96974545-q6dm9
    uid: 16dc34e0-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: PORT
        value: "50051"
      image: gcr.io/google-samples/microservices-demo/shippingservice:v0.1.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:50051
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: server
      ports:
      - containerPort: 50051
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/grpc_health_probe
          - -addr=:50051
        failureThreshold: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 20m
          memory: 128Mi
        requests:
          cpu: 10m
          memory: 64Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-pncck
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-pncck
      secret:
        defaultMode: 420
        secretName: default-token-pncck
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:41:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T23:10:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T23:10:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:41:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://74bcd44b891e36f2bc2b7b803d9b402c340150743e00c09fde7b39e08ebcb1ab
      image: gcr.io/google-samples/microservices-demo/shippingservice:v0.1.3
      imageID: docker-pullable://gcr.io/google-samples/microservices-demo/shippingservice@sha256:6440aabdb33ad7f8ec4bd0e8922902b5fe3ad81fb8a262705ecb262a39bb33de
      lastState:
        terminated:
          containerID: docker://986b45693190716a59eb109998f2dc3ab686779a8dccbce910c7e723049c94c2
          exitCode: 2
          finishedAt: "2020-04-10T23:10:10Z"
          reason: Error
          startedAt: "2020-04-10T23:09:02Z"
      name: server
      ready: true
      restartCount: 30
      state:
        running:
          startedAt: "2020-04-10T23:10:16Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.74
    qosClass: Burstable
    startTime: "2020-03-31T18:41:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2020-03-30T19:16:29Z"
    generateName: calico-node-
    labels:
      controller-revision-hash: 6c4f8d565c
      k8s-app: calico-node
      pod-template-generation: "4"
    name: calico-node-hnddt
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: c72c3f7d-3d3e-11ea-96d3-42010a80017a
    resourceVersion: "33957893"
    selfLink: /api/v1/namespaces/kube-system/pods/calico-node-hnddt
    uid: f5af10db-72ba-11ea-8ebf-42010a800207
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-kubevious-samples-pool-2-d17eaa99-95cn
    containers:
    - env:
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "true"
      - name: CALICO_NETWORKING_BACKEND
        value: none
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_HEALTHENABLED
        value: "true"
      - name: FELIX_IGNORELOOSERPF
        value: "true"
      - name: FELIX_IPTABLESMANGLEALLOWACTION
        value: RETURN
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: FELIX_LOGSEVERITYSYS
        value: none
      - name: FELIX_LOGSEVERITYSCREEN
        value: warning
      - name: FELIX_PROMETHEUSMETRICSENABLED
        value: "true"
      - name: FELIX_REPORTINGINTERVALSECS
        value: "0"
      - name: FELIX_TYPHAK8SSERVICENAME
        value: calico-typha
      - name: IP
      - name: NO_DEFAULT_POOLS
        value: "true"
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: WAIT_FOR_DATASTORE
        value: "true"
      image: gcr.io/projectcalico-org/node:v3.2.7
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          host: localhost
          path: /liveness
          port: 9099
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: calico-node
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /readiness
          port: 9099
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/calico
        name: etc-calico
        readOnly: true
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/lib/calico
        name: var-lib-calico
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-sa-token-wrppd
        readOnly: true
    - command:
      - /install-cni.sh
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: CNI_NETWORK_CONFIG
        value: |-
          {
            "name": "k8s-pod-network",
            "cniVersion": "0.3.0",
            "plugins": [
              {
                "type": "calico",
                "mtu": 1460,
                "log_level": "warning",
                "datastore_type": "kubernetes",
                "nodename": "__KUBERNETES_NODE_NAME__",
                "ipam": {
                  "type": "host-local",
                  "subnet": "usePodCidr"
                },
                "policy": {
                  "type": "k8s",
                  "k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"
                },
                "kubernetes": {
                  "k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",
                  "kubeconfig": "__KUBECONFIG_FILEPATH__"
                }
              },
              {
                "type": "portmap",
                "capabilities": {"portMappings": true},
                "snat": true
              }
            ]
          }
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: gcr.io/projectcalico-org/cni:v3.2.7
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-sa-token-wrppd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
      projectcalico.org/ds-ready: "true"
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-sa
    serviceAccountName: calico-sa
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /etc/calico
        type: ""
      name: etc-calico
    - hostPath:
        path: /home/kubernetes/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /var/lib/calico
        type: ""
      name: var-lib-calico
    - name: calico-sa-token-wrppd
      secret:
        defaultMode: 420
        secretName: calico-sa-token-wrppd
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-02T13:50:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-02T13:50:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://7458a895046a6177880e2c4bf6d9e1d0571d434f460cd06aeaf0c2517362c384
      image: gcr.io/projectcalico-org/node:v3.2.7
      imageID: docker-pullable://gcr.io/projectcalico-org/node@sha256:fb01c69450f0ba1e2a961184d2fcf0c041e1cb0d92865fdbe4aaf3f475e2786c
      lastState:
        terminated:
          containerID: docker://10d2f6d976cbb0eb7cda748598c24187ad53f853cac6fb95e441350c0e24c529
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:16:40Z"
      name: calico-node
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:47:31Z"
    - containerID: docker://f2896831131cc5d0ca374d507e08abd8edbd74da010638a6628a785ebfdbdcc6
      image: gcr.io/projectcalico-org/cni:v3.2.7
      imageID: docker-pullable://gcr.io/projectcalico-org/cni@sha256:7d562fe1832ee85b6c0e005a8af5da25c96b46f978ab3e3026371b38eb872e71
      lastState:
        terminated:
          containerID: docker://cf0f5f32ce5f8777d14edede95ea9a127926f677e3ee7a49e2cefaa1217c7b92
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:16:51Z"
      name: install-cni
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:47:34Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.128.0.37
    qosClass: Burstable
    startTime: "2020-03-30T19:16:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2020-03-27T23:11:50Z"
    generateName: calico-node-
    labels:
      controller-revision-hash: 6c4f8d565c
      k8s-app: calico-node
      pod-template-generation: "4"
    name: calico-node-q7bmz
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: calico-node
      uid: c72c3f7d-3d3e-11ea-96d3-42010a80017a
    resourceVersion: "33957876"
    selfLink: /api/v1/namespaces/kube-system/pods/calico-node-q7bmz
    uid: 56cd4da5-7080-11ea-8ebf-42010a800207
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-kubevious-samples-pool-2-d17eaa99-w6zl
    containers:
    - env:
      - name: CALICO_DISABLE_FILE_LOGGING
        value: "true"
      - name: CALICO_NETWORKING_BACKEND
        value: none
      - name: DATASTORE_TYPE
        value: kubernetes
      - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
        value: ACCEPT
      - name: FELIX_HEALTHENABLED
        value: "true"
      - name: FELIX_IGNORELOOSERPF
        value: "true"
      - name: FELIX_IPTABLESMANGLEALLOWACTION
        value: RETURN
      - name: FELIX_IPV6SUPPORT
        value: "false"
      - name: FELIX_LOGSEVERITYSYS
        value: none
      - name: FELIX_LOGSEVERITYSCREEN
        value: warning
      - name: FELIX_PROMETHEUSMETRICSENABLED
        value: "true"
      - name: FELIX_REPORTINGINTERVALSECS
        value: "0"
      - name: FELIX_TYPHAK8SSERVICENAME
        value: calico-typha
      - name: IP
      - name: NO_DEFAULT_POOLS
        value: "true"
      - name: NODENAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: WAIT_FOR_DATASTORE
        value: "true"
      image: gcr.io/projectcalico-org/node:v3.2.7
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          host: localhost
          path: /liveness
          port: 9099
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: calico-node
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /readiness
          port: 9099
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 100m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/calico
        name: etc-calico
        readOnly: true
      - mountPath: /var/run/calico
        name: var-run-calico
      - mountPath: /var/lib/calico
        name: var-lib-calico
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-sa-token-wrppd
        readOnly: true
    - command:
      - /install-cni.sh
      env:
      - name: CNI_CONF_NAME
        value: 10-calico.conflist
      - name: CNI_NETWORK_CONFIG
        value: |-
          {
            "name": "k8s-pod-network",
            "cniVersion": "0.3.0",
            "plugins": [
              {
                "type": "calico",
                "mtu": 1460,
                "log_level": "warning",
                "datastore_type": "kubernetes",
                "nodename": "__KUBERNETES_NODE_NAME__",
                "ipam": {
                  "type": "host-local",
                  "subnet": "usePodCidr"
                },
                "policy": {
                  "type": "k8s",
                  "k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"
                },
                "kubernetes": {
                  "k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",
                  "kubeconfig": "__KUBECONFIG_FILEPATH__"
                }
              },
              {
                "type": "portmap",
                "capabilities": {"portMappings": true},
                "snat": true
              }
            ]
          }
      - name: KUBERNETES_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: gcr.io/projectcalico-org/cni:v3.2.7
      imagePullPolicy: IfNotPresent
      name: install-cni
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/opt/cni/bin
        name: cni-bin-dir
      - mountPath: /host/etc/cni/net.d
        name: cni-net-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-sa-token-wrppd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    nodeSelector:
      beta.kubernetes.io/os: linux
      projectcalico.org/ds-ready: "true"
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-sa
    serviceAccountName: calico-sa
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /etc/calico
        type: ""
      name: etc-calico
    - hostPath:
        path: /home/kubernetes/bin
        type: ""
      name: cni-bin-dir
    - hostPath:
        path: /etc/cni/net.d
        type: ""
      name: cni-net-dir
    - hostPath:
        path: /var/run/calico
        type: ""
      name: var-run-calico
    - hostPath:
        path: /var/lib/calico
        type: ""
      name: var-lib-calico
    - name: calico-sa-token-wrppd
      secret:
        defaultMode: 420
        secretName: calico-sa-token-wrppd
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-27T23:11:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-02T13:50:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-02T13:50:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-27T23:11:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://dcedc97a728b8584f2ba8a68fe95c7377606e4092de1876e7c6c67fc8501060a
      image: gcr.io/projectcalico-org/node:v3.2.7
      imageID: docker-pullable://gcr.io/projectcalico-org/node@sha256:fb01c69450f0ba1e2a961184d2fcf0c041e1cb0d92865fdbe4aaf3f475e2786c
      lastState:
        terminated:
          containerID: docker://e20a612f8f17d858bbb3ac64c3669d6f96f88c5ad5751374b5aa2c7b7d3ba6dd
          exitCode: 255
          finishedAt: "2020-03-31T03:38:52Z"
          reason: Error
          startedAt: "2020-03-30T20:00:24Z"
      name: calico-node
      ready: true
      restartCount: 5
      state:
        running:
          startedAt: "2020-03-31T03:39:38Z"
    - containerID: docker://96ed772d562a677d80b2c695f53ad90b7edd94753f96b592ee54f4df808035eb
      image: gcr.io/projectcalico-org/cni:v3.2.7
      imageID: docker-pullable://gcr.io/projectcalico-org/cni@sha256:7d562fe1832ee85b6c0e005a8af5da25c96b46f978ab3e3026371b38eb872e71
      lastState:
        terminated:
          containerID: docker://86d5ef7925444ad0946396ce46cf795fc604dae12249eb0db635d55ac9cf1e94
          exitCode: 255
          finishedAt: "2020-03-31T03:38:52Z"
          reason: Error
          startedAt: "2020-03-30T20:00:28Z"
      name: install-cni
      ready: true
      restartCount: 4
      state:
        running:
          startedAt: "2020-03-31T03:39:43Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.128.0.35
    qosClass: Burstable
    startTime: "2020-03-27T23:11:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.70/32
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2020-03-31T18:40:25Z"
    generateName: calico-node-vertical-autoscaler-b889c775f-
    labels:
      k8s-app: calico-node-autoscaler
      pod-template-hash: b889c775f
    name: calico-node-vertical-autoscaler-b889c775f-bpvk7
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: calico-node-vertical-autoscaler-b889c775f
      uid: 191987f3-6f58-11ea-8ebf-42010a800207
    resourceVersion: "33571649"
    selfLink: /api/v1/namespaces/kube-system/pods/calico-node-vertical-autoscaler-b889c775f-bpvk7
    uid: 1645ba65-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - command:
      - /cpvpa
      - --target=daemonset/calico-node
      - --namespace=kube-system
      - --logtostderr=true
      - --poll-period-seconds=30
      - --v=2
      - --config-file=/etc/config/node-autoscaler
      image: gke.gcr.io/cpvpa-amd64:v0.7.1-gke.0
      imagePullPolicy: IfNotPresent
      name: autoscaler
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-cpva-token-l8qbm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-cpva
    serviceAccountName: calico-cpva
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: calico-node-vertical-autoscaler
      name: config
    - name: calico-cpva-token-l8qbm
      secret:
        defaultMode: 420
        secretName: calico-cpva-token-l8qbm
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://826831df75f9950ffe9fab9c130b994e27b6401a4c3cb2ba6bbdbdf0effec74a
      image: asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.7.1-gke.0
      imageID: docker-pullable://asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:5e56182e651b57d4c2f861214e32ec9fac063694d7222c59ee915203bdfb5181
      lastState: {}
      name: autoscaler
      ready: true
      restartCount: 3
      state:
        running:
          startedAt: "2020-04-01T13:49:30Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.70
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2020-03-31T18:40:26Z"
    generateName: calico-typha-65bfd5544b-
    labels:
      k8s-app: calico-typha
      pod-template-hash: 65bfd5544b
    name: calico-typha-65bfd5544b-bhvr6
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: calico-typha-65bfd5544b
      uid: c9954f80-3d3e-11ea-96d3-42010a80017a
    resourceVersion: "33571164"
    selfLink: /api/v1/namespaces/kube-system/pods/calico-typha-65bfd5544b-bhvr6
    uid: 16830ee7-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: TYPHA_LOGFILEPATH
        value: none
      - name: TYPHA_LOGSEVERITYSYS
        value: none
      - name: TYPHA_LOGSEVERITYSCREEN
        value: warning
      - name: TYPHA_PROMETHEUSMETRICSENABLED
        value: "true"
      - name: TYPHA_CONNECTIONREBALANCINGMODE
        value: kubernetes
      - name: TYPHA_REPORTINGINTERVALSECS
        value: "0"
      - name: TYPHA_PROMETHEUSMETRICSPORT
        value: "9093"
      - name: TYPHA_DATASTORETYPE
        value: kubernetes
      - name: TYPHA_MAXCONNECTIONSLOWERLIMIT
        value: "1"
      - name: TYPHA_HEALTHENABLED
        value: "true"
      image: gcr.io/projectcalico-org/typha:v3.2.7
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /liveness
          port: 9098
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 1
      name: calico-typha
      ports:
      - containerPort: 5473
        hostPort: 5473
        name: calico-typha
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: localhost
          path: /readiness
          port: 9098
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 200m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/calico
        name: etc-calico
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-sa-token-wrppd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-sa
    serviceAccountName: calico-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /etc/calico
        type: ""
      name: etc-calico
    - name: calico-sa-token-wrppd
      secret:
        defaultMode: 420
        secretName: calico-sa-token-wrppd
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:47:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:47:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://fb37fff4f94f952424e2f3cae31c9bbdc6fa3e5d931b6128116edd088c4171b6
      image: gcr.io/projectcalico-org/typha:v3.2.7
      imageID: docker-pullable://gcr.io/projectcalico-org/typha@sha256:2a43d2cd0f27f2da9f6a1200d5466fbd5a4cce8bdacb139429c44c843f23402d
      lastState:
        terminated:
          containerID: docker://2fd680a6dfb392cd55eb7f3e37d4049cbf0e54492ca14b2e1e5c23399a5c61ae
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-04-01T13:00:40Z"
      name: calico-typha
      ready: true
      restartCount: 2
      state:
        running:
          startedAt: "2020-04-01T13:47:31Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.128.0.37
    qosClass: Burstable
    startTime: "2020-03-31T18:40:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.168/32
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2020-03-31T18:40:26Z"
    generateName: calico-typha-horizontal-autoscaler-d777c75b4-
    labels:
      k8s-app: calico-typha-autoscaler
      pod-template-hash: d777c75b4
    name: calico-typha-horizontal-autoscaler-d777c75b4-w86pc
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: calico-typha-horizontal-autoscaler-d777c75b4
      uid: 195b89f7-6f58-11ea-8ebf-42010a800207
    resourceVersion: "33302481"
    selfLink: /api/v1/namespaces/kube-system/pods/calico-typha-horizontal-autoscaler-d777c75b4-w86pc
    uid: 1690bae9-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - command:
      - /cluster-proportional-autoscaler
      - --namespace=kube-system
      - --configmap=calico-typha-horizontal-autoscaler
      - --target=deployment/calico-typha
      - --logtostderr=true
      - --v=2
      image: gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0
      imagePullPolicy: IfNotPresent
      name: autoscaler
      resources:
        limits:
          cpu: 10m
        requests:
          cpu: 10m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: typha-cpha-token-j59zj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      supplementalGroups:
      - 65534
    serviceAccount: typha-cpha
    serviceAccountName: typha-cpha
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: typha-cpha-token-j59zj
      secret:
        defaultMode: 420
        secretName: typha-cpha-token-j59zj
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9f80d0653ee4a14bb3b6ff9d68dccb474ffecd676871340607ab2fa1ef85f22b
      image: asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0
      imageID: docker-pullable://asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989
      lastState: {}
      name: autoscaler
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-03-31T18:42:04Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.168
    qosClass: Burstable
    startTime: "2020-03-31T18:40:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.85/32
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2020-03-31T18:40:26Z"
    generateName: calico-typha-vertical-autoscaler-d9b7979f8-
    labels:
      k8s-app: calico-typha-autoscaler
      pod-template-hash: d9b7979f8
    name: calico-typha-vertical-autoscaler-d9b7979f8-mrlwx
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: calico-typha-vertical-autoscaler-d9b7979f8
      uid: 19740c3a-6f58-11ea-8ebf-42010a800207
    resourceVersion: "33573318"
    selfLink: /api/v1/namespaces/kube-system/pods/calico-typha-vertical-autoscaler-d9b7979f8-mrlwx
    uid: 16a51738-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - command:
      - /cpvpa
      - --target=deployment/calico-typha
      - --namespace=kube-system
      - --logtostderr=true
      - --poll-period-seconds=30
      - --v=2
      - --config-file=/etc/config/typha-autoscaler
      image: gke.gcr.io/cpvpa-amd64:v0.7.1-gke.0
      imagePullPolicy: IfNotPresent
      name: autoscaler
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: calico-cpva-token-l8qbm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: calico-cpva
    serviceAccountName: calico-cpva
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: calico-typha-vertical-autoscaler
      name: config
    - name: calico-cpva-token-l8qbm
      secret:
        defaultMode: 420
        secretName: calico-cpva-token-l8qbm
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:56:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:56:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://eceb217293714b4baef9c8e1baf34ea26125cd56da1f17b867b426a652e7a06c
      image: asia.gcr.io/gke-release-staging/cpvpa-amd64:v0.7.1-gke.0
      imageID: docker-pullable://asia.gcr.io/gke-release-staging/cpvpa-amd64@sha256:5e56182e651b57d4c2f861214e32ec9fac063694d7222c59ee915203bdfb5181
      lastState: {}
      name: autoscaler
      ready: true
      restartCount: 6
      state:
        running:
          startedAt: "2020-04-01T13:56:04Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.85
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.107/32
    creationTimestamp: "2020-03-31T18:40:26Z"
    generateName: event-exporter-v0.2.4-5f88c66fb7-
    labels:
      k8s-app: event-exporter
      pod-template-hash: 5f88c66fb7
      version: v0.2.4
    name: event-exporter-v0.2.4-5f88c66fb7-85fwj
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: event-exporter-v0.2.4-5f88c66fb7
      uid: bf007eef-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "33571766"
    selfLink: /api/v1/namespaces/kube-system/pods/event-exporter-v0.2.4-5f88c66fb7-85fwj
    uid: 16c36188-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - command:
      - /event-exporter
      - -sink-opts=-stackdriver-resource-model=old
      image: k8s.gcr.io/event-exporter:v0.2.4
      imagePullPolicy: IfNotPresent
      name: event-exporter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: event-exporter-sa-token-bqsj8
        readOnly: true
    - command:
      - /monitor
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --source=event_exporter:http://localhost:80?whitelisted=stackdriver_sink_received_entry_count,stackdriver_sink_request_count,stackdriver_sink_successfully_sent_entry_count
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd-exporter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: event-exporter-sa-token-bqsj8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: event-exporter-sa
    serviceAccountName: event-exporter-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: ""
      name: ssl-certs
    - name: event-exporter-sa-token-bqsj8
      secret:
        defaultMode: 420
        secretName: event-exporter-sa-token-bqsj8
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://feffabc17641a19e6c245c4f5c419a438de9ac4b8869744e4fa3f5a8eb7e6146
      image: k8s.gcr.io/event-exporter:v0.2.4
      imageID: docker-pullable://k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc
      lastState: {}
      name: event-exporter
      ready: true
      restartCount: 2
      state:
        running:
          startedAt: "2020-04-01T13:49:51Z"
    - containerID: docker://a5cda3e908ccd3f5ddb95ff283d3246af48a18b7041f5e7752238baf1fff7c16
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e
      lastState:
        terminated:
          containerID: docker://e2e481e89496c7352a36b1ab96011287f35d1a627026a2cbab39a9480b24a12c
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-31T18:41:54Z"
      name: prometheus-to-sd-exporter
      ready: true
      restartCount: 2
      state:
        running:
          startedAt: "2020-04-01T13:50:04Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.107
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:27Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.114/32
    creationTimestamp: "2020-03-31T18:40:27Z"
    generateName: fluentd-gcp-scaler-6965bb45c9-
    labels:
      k8s-app: fluentd-gcp-scaler
      pod-template-hash: 6965bb45c9
    name: fluentd-gcp-scaler-6965bb45c9-hzbxf
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: fluentd-gcp-scaler-6965bb45c9
      uid: 1b58e436-6f58-11ea-8ebf-42010a800207
    resourceVersion: "33571781"
    selfLink: /api/v1/namespaces/kube-system/pods/fluentd-gcp-scaler-6965bb45c9-hzbxf
    uid: 16e543b9-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - command:
      - /scaler.sh
      - --ds-name=fluentd-gcp-v3.2.0
      - --scaling-policy=fluentd-gcp-scaling-policy
      env:
      - name: CPU_REQUEST
        value: 100m
      - name: MEMORY_REQUEST
        value: 200Mi
      - name: CPU_LIMIT
        value: "1"
      - name: MEMORY_LIMIT
        value: 500Mi
      image: k8s.gcr.io/fluentd-gcp-scaler:0.5.2
      imagePullPolicy: IfNotPresent
      name: fluentd-gcp-scaler
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-scaler-token-bbnrq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: fluentd-gcp-scaler
    serviceAccountName: fluentd-gcp-scaler
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: fluentd-gcp-scaler-token-bbnrq
      secret:
        defaultMode: 420
        secretName: fluentd-gcp-scaler-token-bbnrq
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:09Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:09Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://6d9223fdd0b7e4ff2960f364536a1bfcdb703cc088a6baba102fd41ea4c7c715
      image: k8s.gcr.io/fluentd-gcp-scaler:0.5.2
      imageID: docker-pullable://k8s.gcr.io/fluentd-gcp-scaler@sha256:4f28f10fb89506768910b858f7a18ffb996824a16d70d5ac895e49687df9ff58
      lastState:
        terminated:
          containerID: docker://a14c985de8be3482a76a65df9911bccf51813e3363c7852135c9fc18f7cce02f
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-31T18:41:17Z"
      name: fluentd-gcp-scaler
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:59Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.114
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2020-03-27T23:11:50Z"
    generateName: fluentd-gcp-v3.2.0-
    labels:
      controller-revision-hash: 6cdb68795b
      k8s-app: fluentd-gcp
      kubernetes.io/cluster-service: "true"
      pod-template-generation: "4"
      version: v3.2.0
    name: fluentd-gcp-v3.2.0-fhp9g
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: fluentd-gcp-v3.2.0
      uid: bf270dd4-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "33301767"
    selfLink: /api/v1/namespaces/kube-system/pods/fluentd-gcp-v3.2.0-fhp9g
    uid: 56d13f86-7080-11ea-8ebf-42010a800207
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-kubevious-samples-pool-2-d17eaa99-w6zl
    containers:
    - env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: STACKDRIVER_METADATA_AGENT_URL
        value: http://$(NODE_NAME):8799
      image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.17
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - |
            LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${STUCK_THRESHOLD_SECONDS:-900}; if [ ! -e /var/log/fluentd-buffers ]; then
              exit 1;
            fi; touch -d "${STUCK_THRESHOLD_SECONDS} seconds ago" /tmp/marker-stuck; if [ -z "$(find /var/log/fluentd-buffers -type d -newer /tmp/marker-stuck -print -quit)" ]; then
              rm -rf /var/log/fluentd-buffers;
              exit 1;
            fi; touch -d "${LIVENESS_THRESHOLD_SECONDS} seconds ago" /tmp/marker-liveness; if [ -z "$(find /var/log/fluentd-buffers -type d -newer /tmp/marker-liveness -print -quit)" ]; then
              exit 1;
            fi;
        failureThreshold: 3
        initialDelaySeconds: 600
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 1
      name: fluentd-gcp
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/docker/containers
        name: varlibdockercontainers
        readOnly: true
      - mountPath: /etc/google-fluentd/config.d
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-token-85sk7
        readOnly: true
    - command:
      - /monitor
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --source=fluentd:http://localhost:24231?whitelisted=stackdriver_successful_requests_count,stackdriver_failed_requests_count,stackdriver_ingested_entries_count,stackdriver_dropped_entries_count
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd-exporter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-token-85sk7
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    hostNetwork: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: fluentd-gcp
    serviceAccountName: fluentd-gcp
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: varlibdockercontainers
    - configMap:
        defaultMode: 420
        name: fluentd-gcp-config-old-v1.2.5
      name: config-volume
    - name: fluentd-gcp-token-85sk7
      secret:
        defaultMode: 420
        secretName: fluentd-gcp-token-85sk7
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-27T23:11:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T03:39:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T03:39:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-27T23:11:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://04e8f185af06c88d04c3dff0023738097ff94190c517d0c11693ec30c58984ba
      image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.17
      imageID: docker-pullable://gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:810aadac042cee96db99d94cec2436482b886774681b44bb141254edda5e3cdf
      lastState:
        terminated:
          containerID: docker://399b043f28801e3ee2530481f79bd3d3f2e382a02b9d5929139ec91d362d9e1a
          exitCode: 255
          finishedAt: "2020-03-31T03:38:52Z"
          reason: Error
          startedAt: "2020-03-30T20:00:21Z"
      name: fluentd-gcp
      ready: true
      restartCount: 4
      state:
        running:
          startedAt: "2020-03-31T03:39:38Z"
    - containerID: docker://148c272e3a8e9dae6f8417bbe1f70640bd999c5e95cf8c1652a2d3bd8b517ebe
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e
      lastState:
        terminated:
          containerID: docker://0c89e579ce770da81e51434b526e31dc66786457e8a154be133422885e7f1b22
          exitCode: 255
          finishedAt: "2020-03-31T03:38:52Z"
          reason: Error
          startedAt: "2020-03-30T20:00:22Z"
      name: prometheus-to-sd-exporter
      ready: true
      restartCount: 4
      state:
        running:
          startedAt: "2020-03-31T03:39:40Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.128.0.35
    qosClass: Burstable
    startTime: "2020-03-27T23:11:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2020-03-30T19:16:30Z"
    generateName: fluentd-gcp-v3.2.0-
    labels:
      controller-revision-hash: 6cdb68795b
      k8s-app: fluentd-gcp
      kubernetes.io/cluster-service: "true"
      pod-template-generation: "4"
      version: v3.2.0
    name: fluentd-gcp-v3.2.0-vdqxs
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: fluentd-gcp-v3.2.0
      uid: bf270dd4-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "33571169"
    selfLink: /api/v1/namespaces/kube-system/pods/fluentd-gcp-v3.2.0-vdqxs
    uid: f5bb005b-72ba-11ea-8ebf-42010a800207
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-kubevious-samples-pool-2-d17eaa99-95cn
    containers:
    - env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: STACKDRIVER_METADATA_AGENT_URL
        value: http://$(NODE_NAME):8799
      image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.17
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - |
            LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${STUCK_THRESHOLD_SECONDS:-900}; if [ ! -e /var/log/fluentd-buffers ]; then
              exit 1;
            fi; touch -d "${STUCK_THRESHOLD_SECONDS} seconds ago" /tmp/marker-stuck; if [ -z "$(find /var/log/fluentd-buffers -type d -newer /tmp/marker-stuck -print -quit)" ]; then
              rm -rf /var/log/fluentd-buffers;
              exit 1;
            fi; touch -d "${LIVENESS_THRESHOLD_SECONDS} seconds ago" /tmp/marker-liveness; if [ -z "$(find /var/log/fluentd-buffers -type d -newer /tmp/marker-liveness -print -quit)" ]; then
              exit 1;
            fi;
        failureThreshold: 3
        initialDelaySeconds: 600
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 1
      name: fluentd-gcp
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/docker/containers
        name: varlibdockercontainers
        readOnly: true
      - mountPath: /etc/google-fluentd/config.d
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-token-85sk7
        readOnly: true
    - command:
      - /monitor
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --source=fluentd:http://localhost:24231?whitelisted=stackdriver_successful_requests_count,stackdriver_failed_requests_count,stackdriver_ingested_entries_count,stackdriver_dropped_entries_count
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd-exporter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-token-85sk7
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    hostNetwork: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: fluentd-gcp
    serviceAccountName: fluentd-gcp
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: varlibdockercontainers
    - configMap:
        defaultMode: 420
        name: fluentd-gcp-config-old-v1.2.5
      name: config-volume
    - name: fluentd-gcp-token-85sk7
      secret:
        defaultMode: 420
        secretName: fluentd-gcp-token-85sk7
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:47:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:47:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://65cc3921f717a5e3317a9ec3b4e575a68d9c46d75cd8d73c37e89156a94227b7
      image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.17
      imageID: docker-pullable://gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:810aadac042cee96db99d94cec2436482b886774681b44bb141254edda5e3cdf
      lastState:
        terminated:
          containerID: docker://cdce98be926d89e4d164f552c45286636a57ce42b9fdadf65a61168f7cb44ff6
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:16:49Z"
      name: fluentd-gcp
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:47:23Z"
    - containerID: docker://a154cada79ca95caf2f126e71607dba62913ff8cdb999221e84617127d6ae275
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e
      lastState:
        terminated:
          containerID: docker://363ef0f32dafa5d4415095f1d193814fbecb34b4bc8c578708ce2134d6f35a10
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:16:50Z"
      name: prometheus-to-sd-exporter
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:47:23Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.128.0.37
    qosClass: Burstable
    startTime: "2020-03-30T19:16:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.83/32
      scheduler.alpha.kubernetes.io/critical-pod: ""
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2020-03-31T18:40:27Z"
    generateName: heapster-gke-566bdc98db-
    labels:
      k8s-app: heapster
      pod-template-hash: 566bdc98db
      version: v1.7.2
    name: heapster-gke-566bdc98db-92f4r
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: heapster-gke-566bdc98db
      uid: 5af6681d-6f58-11ea-8ebf-42010a800207
    resourceVersion: "37190283"
    selfLink: /api/v1/namespaces/kube-system/pods/heapster-gke-566bdc98db-92f4r
    uid: 17045181-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - command:
      - /heapster
      - --source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id
      - --sink=stackdriver:?cluster_name=kubevious-samples&use_old_resources=true&use_new_resources=false&min_interval_sec=100&batch_export_timeout_sec=110&cluster_location=us-central1-a
      image: gke.gcr.io/heapster:v1.7.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8082
          scheme: HTTP
        initialDelaySeconds: 180
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: heapster
      resources:
        limits:
          cpu: 13m
          memory: 120Mi
        requests:
          cpu: 13m
          memory: 120Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: heapster-token-ph6nd
        readOnly: true
    - command:
      - /monitor
      - --source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imagePullPolicy: IfNotPresent
      name: prom-to-sd
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: heapster-token-ph6nd
        readOnly: true
    - command:
      - /pod_nanny
      - --config-dir=/etc/config
      - --cpu=10m
      - --extra-cpu=0.5m
      - --memory=100Mi
      - --extra-memory=4Mi
      - --threshold=5
      - --deployment=heapster-gke
      - --container=heapster
      - --poll-period=300000
      - --estimator=exponential
      - --minClusterSize=5
      env:
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: MY_POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/addon-resizer:1.8.3
      imagePullPolicy: IfNotPresent
      name: heapster-nanny
      resources:
        limits:
          cpu: 50m
          memory: 92760Ki
        requests:
          cpu: 50m
          memory: 92760Ki
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: heapster-config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: heapster-token-ph6nd
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      supplementalGroups:
      - 65534
    serviceAccount: heapster
    serviceAccountName: heapster
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: heapster-config
      name: heapster-config-volume
    - name: heapster-token-ph6nd
      secret:
        defaultMode: 420
        secretName: heapster-token-ph6nd
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:41:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:41:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://2fe1b86e73dd77459e8c2d8e6b9f40d929917719ec40bb693dc624cc1cbb0c45
      image: gke.gcr.io/heapster:v1.7.2
      imageID: docker-pullable://gke.gcr.io/heapster@sha256:26891d2f8c22407a772b911aed912993e1c530128ebb2d3561b49fc16a52d765
      lastState: {}
      name: heapster
      ready: true
      restartCount: 104
      state:
        running:
          startedAt: "2020-04-10T23:00:43Z"
    - containerID: docker://563ecf52d3bff50e72f1795cb06c9ab9ba7532d77c0ce5644e2d35712f12352e
      image: k8s.gcr.io/addon-resizer:1.8.3
      imageID: docker-pullable://k8s.gcr.io/addon-resizer@sha256:07353f7b26327f0d933515a22b1de587b040d3d85c464ea299c1b9f242529326
      lastState:
        terminated:
          containerID: docker://129b8964c946e4475e179600de4e1f0ed0310661aeacc77a34f6996695589060
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-31T18:47:50Z"
      name: heapster-nanny
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:42Z"
    - containerID: docker://1afe736e32a5702180383fe192b78938d5ec999035f8587fad59b3b567faf574
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e
      lastState: {}
      name: prom-to-sd
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:36Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.83
    qosClass: Burstable
    startTime: "2020-03-31T18:41:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2020-03-27T23:11:50Z"
    generateName: ip-masq-agent-
    labels:
      controller-revision-hash: 5cf75cf696
      k8s-app: ip-masq-agent
      pod-template-generation: "2"
    name: ip-masq-agent-4blmr
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: ip-masq-agent
      uid: c99105c9-3d3e-11ea-96d3-42010a80017a
    resourceVersion: "33301861"
    selfLink: /api/v1/namespaces/kube-system/pods/ip-masq-agent-4blmr
    uid: 56ded166-7080-11ea-8ebf-42010a800207
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-kubevious-samples-pool-2-d17eaa99-w6zl
    containers:
    - args:
      - --masq-chain=IP-MASQ
      - --nomasq-all-reserved-ranges
      image: k8s.gcr.io/ip-masq-agent-amd64:v2.4.1
      imagePullPolicy: IfNotPresent
      name: ip-masq-agent
      resources:
        requests:
          cpu: 10m
          memory: 16Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: ip-masq-agent-token-rqq5j
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    nodeSelector:
      beta.kubernetes.io/masq-agent-ds-ready: "true"
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: ip-masq-agent
    serviceAccountName: ip-masq-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: config
          path: ip-masq-agent
        name: ip-masq-agent
        optional: true
      name: config
    - name: ip-masq-agent-token-rqq5j
      secret:
        defaultMode: 420
        secretName: ip-masq-agent-token-rqq5j
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-27T23:11:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T03:39:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T03:39:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-27T23:11:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://52da18391d18f9ec8944e2e6512a766f72da4a17bcc6f2c63fc8889f22e05fd2
      image: k8s.gcr.io/ip-masq-agent-amd64:v2.4.1
      imageID: docker-pullable://k8s.gcr.io/ip-masq-agent-amd64@sha256:eb43b4cdc43a50260b82fdd63208e0b2d401ec830b486d0c7a570a6d73854451
      lastState:
        terminated:
          containerID: docker://a4845875329974c0d1ad6fea47afb65eca16b6322d2f56e76d7af7417890d367
          exitCode: 255
          finishedAt: "2020-03-31T03:38:52Z"
          reason: Error
          startedAt: "2020-03-30T20:00:20Z"
      name: ip-masq-agent
      ready: true
      restartCount: 4
      state:
        running:
          startedAt: "2020-03-31T03:39:25Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.128.0.35
    qosClass: Burstable
    startTime: "2020-03-27T23:11:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2020-03-30T19:16:30Z"
    generateName: ip-masq-agent-
    labels:
      controller-revision-hash: 5cf75cf696
      k8s-app: ip-masq-agent
      pod-template-generation: "2"
    name: ip-masq-agent-lmxr5
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: ip-masq-agent
      uid: c99105c9-3d3e-11ea-96d3-42010a80017a
    resourceVersion: "33571179"
    selfLink: /api/v1/namespaces/kube-system/pods/ip-masq-agent-lmxr5
    uid: f5bb34ec-72ba-11ea-8ebf-42010a800207
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-kubevious-samples-pool-2-d17eaa99-95cn
    containers:
    - args:
      - --masq-chain=IP-MASQ
      - --nomasq-all-reserved-ranges
      image: k8s.gcr.io/ip-masq-agent-amd64:v2.4.1
      imagePullPolicy: IfNotPresent
      name: ip-masq-agent
      resources:
        requests:
          cpu: 10m
          memory: 16Mi
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: ip-masq-agent-token-rqq5j
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/masq-agent-ds-ready: "true"
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: ip-masq-agent
    serviceAccountName: ip-masq-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: config
          path: ip-masq-agent
        name: ip-masq-agent
        optional: true
      name: config
    - name: ip-masq-agent-token-rqq5j
      secret:
        defaultMode: 420
        secretName: ip-masq-agent-token-rqq5j
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:47:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:47:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://01e22136027818553c341e17551389e53bc56e38ec0e3b86545a3328799008fd
      image: k8s.gcr.io/ip-masq-agent-amd64:v2.4.1
      imageID: docker-pullable://k8s.gcr.io/ip-masq-agent-amd64@sha256:eb43b4cdc43a50260b82fdd63208e0b2d401ec830b486d0c7a570a6d73854451
      lastState:
        terminated:
          containerID: docker://23dc8f83e18e920c885ac391c16c875bf9ba8b426f486e9849a31d6e8ad8019b
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:16:37Z"
      name: ip-masq-agent
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:47:26Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.128.0.37
    qosClass: Burstable
    startTime: "2020-03-30T19:16:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.78/32
      scheduler.alpha.kubernetes.io/critical-pod: ""
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2020-03-31T18:40:27Z"
    generateName: kube-dns-79868f54c5-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 79868f54c5
    name: kube-dns-79868f54c5-87l98
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-dns-79868f54c5
      uid: bec3208d-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "33571692"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-dns-79868f54c5-87l98
    uid: 170b358e-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - args:
      - --domain=cluster.local.
      - --dns-port=10053
      - --config-dir=/kube-dns-config
      - --v=2
      env:
      - name: PROMETHEUS_PORT
        value: "10055"
      image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthcheck/kubedns
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kubedns
      ports:
      - containerPort: 10053
        name: dns-local
        protocol: UDP
      - containerPort: 10053
        name: dns-tcp-local
        protocol: TCP
      - containerPort: 10055
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /kube-dns-config
        name: kube-dns-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-r2bjs
        readOnly: true
    - args:
      - -v=2
      - -logtostderr
      - -configDir=/etc/k8s/dns/dnsmasq-nanny
      - -restartDnsmasq=true
      - --
      - -k
      - --cache-size=1000
      - --no-negcache
      - --log-facility=-
      - --server=/cluster.local/127.0.0.1#10053
      - --server=/in-addr.arpa/127.0.0.1#10053
      - --server=/ip6.arpa/127.0.0.1#10053
      image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthcheck/dnsmasq
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: dnsmasq
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      resources:
        requests:
          cpu: 150m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/k8s/dns/dnsmasq-nanny
        name: kube-dns-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-r2bjs
        readOnly: true
    - args:
      - --v=2
      - --logtostderr
      - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV
      - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV
      image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /metrics
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: sidecar
      ports:
      - containerPort: 10054
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-r2bjs
        readOnly: true
    - command:
      - /monitor
      - --source=kubedns:http://localhost:10054?whitelisted=probe_kubedns_latency_ms,probe_kubedns_errors,dnsmasq_misses,dnsmasq_hits
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      - --v=2
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.4.2
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-r2bjs
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-dns
    serviceAccountName: kube-dns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-dns
        optional: true
      name: kube-dns-config
    - name: kube-dns-token-r2bjs
      secret:
        defaultMode: 420
        secretName: kube-dns-token-r2bjs
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:41:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:41:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://2225eb4edd0741e958035a8c21fea6d4b209a5ebd3f610caabad9f841771a2a4
      image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4
      imageID: docker-pullable://k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:0a70a8a9ae8cfe752021de84f13b3ecd109d9b5fbe3f1541c52fcd1d4c2c0b45
      lastState: {}
      name: dnsmasq
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:35Z"
    - containerID: docker://87686b8c6d3ad8fda6f45b7bdf3ee18a38ce2e9102cde7d1db0594c78d176bbd
      image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4
      imageID: docker-pullable://k8s.gcr.io/k8s-dns-kube-dns-amd64@sha256:a13c60e2a9d49f965095a1e003388926f3f2a6189ed4aecb1541f114c955f8ec
      lastState:
        terminated:
          containerID: docker://94be2f1e2fd6504d636856055d8697bf9c1ed004531244a422501d12459cfd1c
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-31T18:41:22Z"
      name: kubedns
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:48:48Z"
    - containerID: docker://b9a5d8352d9d92e8cb206df92f0e5b7bc571b0662a2f791fe50eadfb97ac2810
      image: k8s.gcr.io/prometheus-to-sd:v0.4.2
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2
      lastState:
        terminated:
          containerID: docker://2904155fe475f1ca555e3f73e5f99be174d8b011eb75942047b5422fc965d1e9
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-31T18:41:52Z"
      name: prometheus-to-sd
      ready: true
      restartCount: 2
      state:
        running:
          startedAt: "2020-04-01T13:49:47Z"
    - containerID: docker://2411fb670ed24c27fd0488f86b95c7653207b9ca12ea23ec4baad82e0f1f2e34
      image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4
      imageID: docker-pullable://k8s.gcr.io/k8s-dns-sidecar-amd64@sha256:e55cbd5361a86bf0a01bfeaca2e958e15571f1e741356eab83bb444a13020d4c
      lastState:
        terminated:
          containerID: docker://afb997c58ac8c94b1e31252df5f37f579fe7a8377e124820e5bc0f661f8459ce
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-31T18:41:31Z"
      name: sidecar
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:42Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.78
    qosClass: Burstable
    startTime: "2020-03-31T18:41:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.81/32
      scheduler.alpha.kubernetes.io/critical-pod: ""
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2020-03-30T19:16:37Z"
    generateName: kube-dns-79868f54c5-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 79868f54c5
    name: kube-dns-79868f54c5-p854d
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-dns-79868f54c5
      uid: bec3208d-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "33571710"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-dns-79868f54c5-p854d
    uid: fa0b502f-72ba-11ea-8ebf-42010a800207
  spec:
    containers:
    - args:
      - --domain=cluster.local.
      - --dns-port=10053
      - --config-dir=/kube-dns-config
      - --v=2
      env:
      - name: PROMETHEUS_PORT
        value: "10055"
      image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthcheck/kubedns
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kubedns
      ports:
      - containerPort: 10053
        name: dns-local
        protocol: UDP
      - containerPort: 10053
        name: dns-tcp-local
        protocol: TCP
      - containerPort: 10055
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /kube-dns-config
        name: kube-dns-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-r2bjs
        readOnly: true
    - args:
      - -v=2
      - -logtostderr
      - -configDir=/etc/k8s/dns/dnsmasq-nanny
      - -restartDnsmasq=true
      - --
      - -k
      - --cache-size=1000
      - --no-negcache
      - --log-facility=-
      - --server=/cluster.local/127.0.0.1#10053
      - --server=/in-addr.arpa/127.0.0.1#10053
      - --server=/ip6.arpa/127.0.0.1#10053
      image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthcheck/dnsmasq
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: dnsmasq
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      resources:
        requests:
          cpu: 150m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/k8s/dns/dnsmasq-nanny
        name: kube-dns-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-r2bjs
        readOnly: true
    - args:
      - --v=2
      - --logtostderr
      - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV
      - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV
      image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /metrics
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: sidecar
      ports:
      - containerPort: 10054
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-r2bjs
        readOnly: true
    - command:
      - /monitor
      - --source=kubedns:http://localhost:10054?whitelisted=probe_kubedns_latency_ms,probe_kubedns_errors,dnsmasq_misses,dnsmasq_hits
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      - --v=2
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.4.2
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-r2bjs
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-dns
    serviceAccountName: kube-dns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-dns
        optional: true
      name: kube-dns-config
    - name: kube-dns-token-r2bjs
      secret:
        defaultMode: 420
        secretName: kube-dns-token-r2bjs
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:17:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:00Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:00Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:17:01Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://0178a5adfa739df7935ee56d2347595da25b7b50326027c1f4245d6beb1e35db
      image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4
      imageID: docker-pullable://k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:0a70a8a9ae8cfe752021de84f13b3ecd109d9b5fbe3f1541c52fcd1d4c2c0b45
      lastState: {}
      name: dnsmasq
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:35Z"
    - containerID: docker://b1f66c3e684b26298b0bbc78a485c5291b9c70e3fda80b4dc8ea99affe052932
      image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4
      imageID: docker-pullable://k8s.gcr.io/k8s-dns-kube-dns-amd64@sha256:a13c60e2a9d49f965095a1e003388926f3f2a6189ed4aecb1541f114c955f8ec
      lastState:
        terminated:
          containerID: docker://206d5377421b4c7a5889092af3c32dd0657df11e6f6e0619842106f15398c5c3
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:17:43Z"
      name: kubedns
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:48:55Z"
    - containerID: docker://92c66a841ca05d5b7e075ec7888e511202798acdae695b67c9c734595bef7030
      image: k8s.gcr.io/prometheus-to-sd:v0.4.2
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2
      lastState:
        terminated:
          containerID: docker://564befc5b53f9fd76776ed8c713eee583b5b7b67c92348cfa454ed4d61df61b6
          exitCode: 255
          finishedAt: "2020-04-01T13:46:49Z"
          reason: Error
          startedAt: "2020-03-30T19:18:57Z"
      name: prometheus-to-sd
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:47Z"
    - containerID: docker://c50da39026c85a6b77fac2d8873ec847950027dbdd36d4c6a3809d6fed188ffa
      image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4
      imageID: docker-pullable://k8s.gcr.io/k8s-dns-sidecar-amd64@sha256:e55cbd5361a86bf0a01bfeaca2e958e15571f1e741356eab83bb444a13020d4c
      lastState:
        terminated:
          containerID: docker://f1708a2f5d2e2d904d0de2f210746e2c311ec5b946ae432fac2ebe43bda06c72
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:18:53Z"
      name: sidecar
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:42Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.81
    qosClass: Burstable
    startTime: "2020-03-30T19:17:01Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.170/32
      scheduler.alpha.kubernetes.io/critical-pod: ""
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2020-03-31T18:40:27Z"
    generateName: kube-dns-autoscaler-8687c64fc-
    labels:
      k8s-app: kube-dns-autoscaler
      pod-template-hash: 8687c64fc
    name: kube-dns-autoscaler-8687c64fc-f95vf
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-dns-autoscaler-8687c64fc
      uid: 198d8621-6f58-11ea-8ebf-42010a800207
    resourceVersion: "33302450"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-dns-autoscaler-8687c64fc-f95vf
    uid: 172232fd-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - command:
      - /cluster-proportional-autoscaler
      - --namespace=kube-system
      - --configmap=kube-dns-autoscaler
      - --target=Deployment/kube-dns
      - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"preventSinglePointFailure":true}}
      - --logtostderr=true
      - --v=2
      image: gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0
      imagePullPolicy: IfNotPresent
      name: autoscaler
      resources:
        requests:
          cpu: 20m
          memory: 10Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-autoscaler-token-g2fjv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      supplementalGroups:
      - 65534
    serviceAccount: kube-dns-autoscaler
    serviceAccountName: kube-dns-autoscaler
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-dns-autoscaler-token-g2fjv
      secret:
        defaultMode: 420
        secretName: kube-dns-autoscaler-token-g2fjv
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:47Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://0a39e2f7f2611d5def82d7f66b0ad8d10cffffd259ea0dcb7dd4b4a4fa1b53dc
      image: asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64:1.7.1-gke.0
      imageID: docker-pullable://asia.gcr.io/gke-release-staging/cluster-proportional-autoscaler-amd64@sha256:e3f48b3d1e49cfa3e7f002020769c9cd01cd0e77bbc99dc133c7ab0f8097e989
      lastState: {}
      name: autoscaler
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-03-31T18:42:02Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.170
    qosClass: Burstable
    startTime: "2020-03-31T18:40:47Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 06e489ce57756b775eac55be6fbb8c6e
      kubernetes.io/config.mirror: 06e489ce57756b775eac55be6fbb8c6e
      kubernetes.io/config.seen: "2020-03-30T19:16:29.610108208Z"
      kubernetes.io/config.source: file
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2020-03-30T19:18:01Z"
    labels:
      component: kube-proxy
      tier: node
    name: kube-proxy-gke-kubevious-samples-pool-2-d17eaa99-95cn
    namespace: kube-system
    resourceVersion: "33571010"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-proxy-gke-kubevious-samples-pool-2-d17eaa99-95cn
    uid: 2c5e5195-72bb-11ea-8ebf-42010a800207
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - exec kube-proxy --master=https://104.154.22.158 --kubeconfig=/var/lib/kube-proxy/kubeconfig
        --cluster-cidr=10.8.0.0/14 --resource-container="" --oom-score-adj=-998 --v=2
        --feature-gates=DynamicKubeletConfig=false,TaintBasedEvictions=false,RotateKubeletServerCertificate=true,ExperimentalCriticalPodAnnotation=true
        --iptables-sync-period=1m --iptables-min-sync-period=10s --ipvs-sync-period=1m
        --ipvs-min-sync-period=10s 1>>/var/log/kube-proxy.log 2>&1
      image: gke.gcr.io/kube-proxy:v1.14.10-gke.17
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources:
        requests:
          cpu: 100m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-ca-certs
        readOnly: true
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/kube-proxy/kubeconfig
        name: kubeconfig
      - mountPath: /run/xtables.lock
        name: iptableslock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    volumes:
    - hostPath:
        path: /usr/share/ca-certificates
        type: ""
      name: usr-ca-certs
    - hostPath:
        path: /etc/ssl/certs
        type: ""
      name: etc-ssl-certs
    - hostPath:
        path: /var/lib/kube-proxy/kubeconfig
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: iptableslock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T23:03:14Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://da7be3a9b86bb1889efbfaa9dccb6f6ad0ced439ee8dab0cc7964bfee52ae71f
      image: gke.gcr.io/kube-proxy:v1.14.10-gke.17
      imageID: docker://sha256:3257c6160a80d816d9c4e72da93fa0d57928f52df59d775c47b49063ec1cfebf
      lastState:
        terminated:
          containerID: docker://6aeb44f43ca2717191cd54f8ed5ff62da265f4877e6082e790afb387f59f781f
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:16:31Z"
      name: kube-proxy
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:47:11Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.128.0.37
    qosClass: Burstable
    startTime: "2020-03-30T19:16:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 06e489ce57756b775eac55be6fbb8c6e
      kubernetes.io/config.mirror: 06e489ce57756b775eac55be6fbb8c6e
      kubernetes.io/config.seen: "2020-03-31T18:40:29.7326862Z"
      kubernetes.io/config.source: file
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2020-03-31T18:41:21Z"
    labels:
      component: kube-proxy
      tier: node
    name: kube-proxy-gke-kubevious-samples-pool-2-d17eaa99-w6zl
    namespace: kube-system
    resourceVersion: "33302150"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-proxy-gke-kubevious-samples-pool-2-d17eaa99-w6zl
    uid: 374be1b7-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - exec kube-proxy --master=https://104.154.22.158 --kubeconfig=/var/lib/kube-proxy/kubeconfig
        --cluster-cidr=10.8.0.0/14 --resource-container="" --oom-score-adj=-998 --v=2
        --feature-gates=DynamicKubeletConfig=false,TaintBasedEvictions=false,RotateKubeletServerCertificate=true,ExperimentalCriticalPodAnnotation=true
        --iptables-sync-period=1m --iptables-min-sync-period=10s --ipvs-sync-period=1m
        --ipvs-min-sync-period=10s 1>>/var/log/kube-proxy.log 2>&1
      image: gke.gcr.io/kube-proxy:v1.14.10-gke.17
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources:
        requests:
          cpu: 100m
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-ca-certs
        readOnly: true
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/kube-proxy/kubeconfig
        name: kubeconfig
      - mountPath: /run/xtables.lock
        name: iptableslock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    volumes:
    - hostPath:
        path: /usr/share/ca-certificates
        type: ""
      name: usr-ca-certs
    - hostPath:
        path: /etc/ssl/certs
        type: ""
      name: etc-ssl-certs
    - hostPath:
        path: /var/lib/kube-proxy/kubeconfig
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: iptableslock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-28T22:33:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T03:39:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T03:39:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-28T22:33:49Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://4a53097307877f96af997d22ddc9f791c93ff924cb494c7c28ed6108761f6218
      image: gke.gcr.io/kube-proxy:v1.14.10-gke.17
      imageID: docker://sha256:3257c6160a80d816d9c4e72da93fa0d57928f52df59d775c47b49063ec1cfebf
      lastState:
        terminated:
          containerID: docker://9ffc7b9f366bf679af16f28f7f433fa5c48c2ec13dc7848af2e99ffdc2ba54af
          exitCode: 255
          finishedAt: "2020-03-31T03:38:52Z"
          reason: Error
          startedAt: "2020-03-30T20:00:35Z"
      name: kube-proxy
      ready: true
      restartCount: 4
      state:
        running:
          startedAt: "2020-03-31T03:39:38Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.128.0.35
    qosClass: Burstable
    startTime: "2020-03-28T22:33:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.94/32
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2020-03-31T18:40:27Z"
    generateName: l7-default-backend-fd59995cd-
    labels:
      k8s-app: glbc
      name: glbc
      pod-template-hash: fd59995cd
    name: l7-default-backend-fd59995cd-4rsq4
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: l7-default-backend-fd59995cd
      uid: be96f3e7-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "37199471"
    selfLink: /api/v1/namespaces/kube-system/pods/l7-default-backend-fd59995cd-4rsq4
    uid: 176c5682-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - image: k8s.gcr.io/defaultbackend-amd64:1.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: default-http-backend
      ports:
      - containerPort: 8080
        protocol: TCP
      resources:
        limits:
          cpu: 10m
          memory: 20Mi
        requests:
          cpu: 10m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-nv2z8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-nv2z8
      secret:
        defaultMode: 420
        secretName: default-token-nv2z8
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:41:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:41:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://020f0ed29813c0bad74e0047cb0e4ddea47a88028a1ea05eac874c849ca36f66
      image: k8s.gcr.io/defaultbackend-amd64:1.5
      imageID: docker-pullable://k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7
      lastState:
        terminated:
          containerID: docker://871ebca53c943a69b064c4a281872dba01fffcfc38c5c5eecbf1645c42c32f6d
          exitCode: 0
          finishedAt: "2020-04-10T23:40:25Z"
          reason: Completed
          startedAt: "2020-04-10T21:56:50Z"
      name: default-http-backend
      ready: true
      restartCount: 16
      state:
        running:
          startedAt: "2020-04-10T23:40:59Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.94
    qosClass: Guaranteed
    startTime: "2020-03-31T18:41:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.87/32
      scheduler.alpha.kubernetes.io/critical-pod: ""
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2020-03-31T18:40:28Z"
    generateName: metrics-server-v0.3.1-5c6fbf777-
    labels:
      k8s-app: metrics-server
      pod-template-hash: 5c6fbf777
      version: v0.3.1
    name: metrics-server-v0.3.1-5c6fbf777-wz946
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metrics-server-v0.3.1-5c6fbf777
      uid: 19e46983-6f58-11ea-8ebf-42010a800207
    resourceVersion: "33571970"
    selfLink: /api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-5c6fbf777-wz946
    uid: 178b2678-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - command:
      - /metrics-server
      - --metric-resolution=30s
      - --kubelet-port=10255
      - --deprecated-kubelet-completely-insecure=true
      - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
      image: k8s.gcr.io/metrics-server-amd64:v0.3.1
      imagePullPolicy: IfNotPresent
      name: metrics-server
      ports:
      - containerPort: 443
        name: https
        protocol: TCP
      resources:
        limits:
          cpu: 43m
          memory: 55Mi
        requests:
          cpu: 43m
          memory: 55Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: metrics-server-token-2s5rt
        readOnly: true
    - command:
      - /pod_nanny
      - --config-dir=/etc/config
      - --cpu=40m
      - --extra-cpu=0.5m
      - --memory=35Mi
      - --extra-memory=4Mi
      - --threshold=5
      - --deployment=metrics-server-v0.3.1
      - --container=metrics-server
      - --poll-period=300000
      - --estimator=exponential
      - --minClusterSize=5
      env:
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: MY_POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: gke.gcr.io/addon-resizer:1.8.4-gke.0
      imagePullPolicy: IfNotPresent
      name: metrics-server-nanny
      resources:
        limits:
          cpu: 100m
          memory: 300Mi
        requests:
          cpu: 5m
          memory: 50Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: metrics-server-config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: metrics-server-token-2s5rt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: metrics-server
    serviceAccountName: metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: metrics-server-config
      name: metrics-server-config-volume
    - name: metrics-server-token-2s5rt
      secret:
        defaultMode: 420
        secretName: metrics-server-token-2s5rt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:41:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:41:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://158260781ddc6de879aee3458d870c77a5640f96e01f92f201d014be2080a7dc
      image: k8s.gcr.io/metrics-server-amd64:v0.3.1
      imageID: docker-pullable://k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b
      lastState:
        terminated:
          containerID: docker://717e048d7ba8daa205031a0ad149bda41f9f2179e90610885692a3456ea46bf0
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-31T18:41:27Z"
      name: metrics-server
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:06Z"
    - containerID: docker://2e01aac59f3f40f8adf10ebb38a4e6458d1e81c849ebaef03e29f3be1acbc009
      image: asia.gcr.io/gke-release-staging/addon-resizer:1.8.4-gke.0
      imageID: docker-pullable://asia.gcr.io/gke-release-staging/addon-resizer@sha256:94e7e68175dfd18a9e3c31bc1f4a6ab19444efef47192f13b0e5af3b03dc04c6
      lastState: {}
      name: metrics-server-nanny
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:37Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.87
    qosClass: Burstable
    startTime: "2020-03-31T18:41:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2020-03-27T23:11:50Z"
    generateName: prometheus-to-sd-
    labels:
      controller-revision-hash: 6cd9748dd
      k8s-app: prometheus-to-sd
      pod-template-generation: "1"
    name: prometheus-to-sd-8vjjp
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-to-sd
      uid: bf82a533-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "33301924"
    selfLink: /api/v1/namespaces/kube-system/pods/prometheus-to-sd-8vjjp
    uid: 56bf87a5-7080-11ea-8ebf-42010a800207
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-kubevious-samples-pool-2-d17eaa99-w6zl
    containers:
    - command:
      - /monitor
      - --source=kube-proxy:http://localhost:10249?whitelisted=sync_proxy_rules_latency_microseconds&metricsPrefix=container.googleapis.com/internal/addons
      - --source=kubelet:http://localhost:10255?whitelisted=docker_operations,docker_operations_errors,runtime_operations,runtime_operations_errors,runtime_operations_latency_microseconds,pleg_relist_latency_microseconds,pod_start_latency_microseconds,rest_client_requests_total,storage_operation_duration_seconds,storage_operation_errors_total,run_podsandbox_duration_seconds,run_podsandbox_errors_total,storage_operation_status_count
      - --source=kubelet:http://localhost:10255/metrics/probes?whitelisted=prober_probe_total&podIdLabel=pod&namespaceIdLabel=namespace&containerNameLabel=container
      - --stackdriver-prefix=container.googleapis.com/internal/nodes
      - --api-override=https://monitoring.googleapis.com/
      - --export-interval=120s
      image: k8s.gcr.io/prometheus-to-sd:v0.8.2
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: prometheus-to-sd-token-4c4np
        readOnly: true
    - command:
      - /monitor
      - --source=kube-proxy:http://localhost:10249?whitelisted=sync_proxy_rules_latency_microseconds&metricsPrefix=kubernetes.io/internal/addons
      - --source=kubelet:http://localhost:10255?whitelisted=docker_operations,docker_operations_errors,runtime_operations,runtime_operations_errors,runtime_operations_latency_microseconds,pleg_relist_latency_microseconds,pod_start_latency_microseconds,rest_client_requests_total,storage_operation_duration_seconds,storage_operation_errors_total,run_podsandbox_duration_seconds,run_podsandbox_errors_total,storage_operation_status_count
      - --source=kubelet:http://localhost:10255/metrics/probes?whitelisted=prober_probe_total&podIdLabel=pod&namespaceIdLabel=namespace&containerNameLabel=container
      - --stackdriver-prefix=kubernetes.io/internal/nodes
      - --api-override=https://monitoring.googleapis.com/
      - --monitored-resource-type-prefix=k8s_
      - --monitored-resource-labels=location=us-central1-a
      - --export-interval=120s
      image: k8s.gcr.io/prometheus-to-sd:v0.8.2
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd-new-model
      resources:
        limits:
          cpu: 3m
          memory: 20Mi
        requests:
          cpu: 1m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: prometheus-to-sd-token-4c4np
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: prometheus-to-sd
    serviceAccountName: prometheus-to-sd
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: prometheus-to-sd-token-4c4np
      secret:
        defaultMode: 420
        secretName: prometheus-to-sd-token-4c4np
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-27T23:11:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T03:39:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T03:39:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-27T23:11:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://f298fa3f41ceb6b2ba845fd165d3e0e4aa9af995b16249464cbfaecaffd72a7b
      image: k8s.gcr.io/prometheus-to-sd:v0.8.2
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:3216dd0e94d6911f6dc04f4258c0bf5cb1fff088ee2d3ce742ada490cbd5ca5c
      lastState:
        terminated:
          containerID: docker://45175ff7e4101951c8b5e9d28247dfe87ecffb9dbe455eb494c070a61e0b4645
          exitCode: 255
          finishedAt: "2020-03-31T03:38:52Z"
          reason: Error
          startedAt: "2020-03-30T20:00:24Z"
      name: prometheus-to-sd
      ready: true
      restartCount: 4
      state:
        running:
          startedAt: "2020-03-31T03:39:27Z"
    - containerID: docker://55eaa631badcfbc9d90dc5f33c9ae5d0d20dc8e09f352faab9cbc084068f8cb0
      image: k8s.gcr.io/prometheus-to-sd:v0.8.2
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:3216dd0e94d6911f6dc04f4258c0bf5cb1fff088ee2d3ce742ada490cbd5ca5c
      lastState:
        terminated:
          containerID: docker://494eec1055dd08afd8795956a078b82edeae7542cc0ce150431eab0a2be9d64a
          exitCode: 255
          finishedAt: "2020-03-31T03:38:52Z"
          reason: Error
          startedAt: "2020-03-30T20:00:25Z"
      name: prometheus-to-sd-new-model
      ready: true
      restartCount: 4
      state:
        running:
          startedAt: "2020-03-31T03:39:28Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.128.0.35
    qosClass: Burstable
    startTime: "2020-03-27T23:11:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2020-03-30T19:16:29Z"
    generateName: prometheus-to-sd-
    labels:
      controller-revision-hash: 6cd9748dd
      k8s-app: prometheus-to-sd
      pod-template-generation: "1"
    name: prometheus-to-sd-fhpgv
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-to-sd
      uid: bf82a533-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "33571176"
    selfLink: /api/v1/namespaces/kube-system/pods/prometheus-to-sd-fhpgv
    uid: f5ab8069-72ba-11ea-8ebf-42010a800207
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-kubevious-samples-pool-2-d17eaa99-95cn
    containers:
    - command:
      - /monitor
      - --source=kube-proxy:http://localhost:10249?whitelisted=sync_proxy_rules_latency_microseconds&metricsPrefix=container.googleapis.com/internal/addons
      - --source=kubelet:http://localhost:10255?whitelisted=docker_operations,docker_operations_errors,runtime_operations,runtime_operations_errors,runtime_operations_latency_microseconds,pleg_relist_latency_microseconds,pod_start_latency_microseconds,rest_client_requests_total,storage_operation_duration_seconds,storage_operation_errors_total,run_podsandbox_duration_seconds,run_podsandbox_errors_total,storage_operation_status_count
      - --source=kubelet:http://localhost:10255/metrics/probes?whitelisted=prober_probe_total&podIdLabel=pod&namespaceIdLabel=namespace&containerNameLabel=container
      - --stackdriver-prefix=container.googleapis.com/internal/nodes
      - --api-override=https://monitoring.googleapis.com/
      - --export-interval=120s
      image: k8s.gcr.io/prometheus-to-sd:v0.8.2
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: prometheus-to-sd-token-4c4np
        readOnly: true
    - command:
      - /monitor
      - --source=kube-proxy:http://localhost:10249?whitelisted=sync_proxy_rules_latency_microseconds&metricsPrefix=kubernetes.io/internal/addons
      - --source=kubelet:http://localhost:10255?whitelisted=docker_operations,docker_operations_errors,runtime_operations,runtime_operations_errors,runtime_operations_latency_microseconds,pleg_relist_latency_microseconds,pod_start_latency_microseconds,rest_client_requests_total,storage_operation_duration_seconds,storage_operation_errors_total,run_podsandbox_duration_seconds,run_podsandbox_errors_total,storage_operation_status_count
      - --source=kubelet:http://localhost:10255/metrics/probes?whitelisted=prober_probe_total&podIdLabel=pod&namespaceIdLabel=namespace&containerNameLabel=container
      - --stackdriver-prefix=kubernetes.io/internal/nodes
      - --api-override=https://monitoring.googleapis.com/
      - --monitored-resource-type-prefix=k8s_
      - --monitored-resource-labels=location=us-central1-a
      - --export-interval=120s
      image: k8s.gcr.io/prometheus-to-sd:v0.8.2
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd-new-model
      resources:
        limits:
          cpu: 3m
          memory: 20Mi
        requests:
          cpu: 1m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: prometheus-to-sd-token-4c4np
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: prometheus-to-sd
    serviceAccountName: prometheus-to-sd
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: prometheus-to-sd-token-4c4np
      secret:
        defaultMode: 420
        secretName: prometheus-to-sd-token-4c4np
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:17:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:47:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:47:36Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:17:01Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://569e3b74609d373e253ca5eac83a330e9fe6129afbf04d7f21e463a7e59b3222
      image: k8s.gcr.io/prometheus-to-sd:v0.8.2
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:3216dd0e94d6911f6dc04f4258c0bf5cb1fff088ee2d3ce742ada490cbd5ca5c
      lastState:
        terminated:
          containerID: docker://7f6204f509f704cae3abf1273b25ded2c43eba1ed0e07cae40414de87eb1e5c9
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:17:41Z"
      name: prometheus-to-sd
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:47:32Z"
    - containerID: docker://11b02b0c7ad25bbee412c38fb6bef6a4a475361d941b146343645222259f94c7
      image: k8s.gcr.io/prometheus-to-sd:v0.8.2
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:3216dd0e94d6911f6dc04f4258c0bf5cb1fff088ee2d3ce742ada490cbd5ca5c
      lastState:
        terminated:
          containerID: docker://a57d5d40f302a4338a181d05f27ae3728de08fc3b67bf750e3fab8e2949c7c2e
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:17:41Z"
      name: prometheus-to-sd-new-model
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:47:33Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.128.0.37
    qosClass: Burstable
    startTime: "2020-03-30T19:17:01Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.221/32
    creationTimestamp: "2020-04-08T07:32:59Z"
    generateName: kubevious-865cf6dfcf-
    labels:
      k8s-app: kubevious
      pod-template-hash: 865cf6dfcf
    name: kubevious-865cf6dfcf-s9jjd
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kubevious-865cf6dfcf
      uid: 2c352d7e-796b-11ea-bb1c-42010a800057
    resourceVersion: "36841061"
    selfLink: /api/v1/namespaces/kubevious/pods/kubevious-865cf6dfcf-s9jjd
    uid: 2c382faa-796b-11ea-bb1c-42010a800057
  spec:
    containers:
    - env:
      - name: NODE_ENV
        value: production
      envFrom:
      - configMapRef:
          name: kubevious-mysql-client
      image: kubevious/kubevious:0.5.2
      imagePullPolicy: IfNotPresent
      name: kubevious
      ports:
      - containerPort: 4000
        name: http
        protocol: TCP
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kubevious-token-b8ckz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kubevious
    serviceAccountName: kubevious
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kubevious-token-b8ckz
      secret:
        defaultMode: 420
        secretName: kubevious-token-b8ckz
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-04-08T07:32:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:10:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-10T00:10:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-04-08T07:32:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://3350d58819fa66566c1bd3ba566e691e1f14b49a47e1f311d21389083cf928bd
      image: kubevious/kubevious:0.5.2
      imageID: docker-pullable://kubevious/kubevious@sha256:054a5156a11246cfb7f4db7f16c40c79a61d4918c4a841eecb91821ee678668c
      lastState:
        terminated:
          containerID: docker://959005e752174d5339133c40d6c494b5bb1e7dc59c483c99987ea9f87edef391
          exitCode: 139
          finishedAt: "2020-04-10T00:10:20Z"
          reason: Error
          startedAt: "2020-04-08T07:33:39Z"
      name: kubevious
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-10T00:10:27Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.221
    qosClass: Burstable
    startTime: "2020-04-08T07:32:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.100/32
    creationTimestamp: "2020-03-30T19:15:37Z"
    generateName: kubevious-mysql-
    labels:
      controller-revision-hash: kubevious-mysql-597f894869
      k8s-app: kubevious-mysql
      statefulset.kubernetes.io/pod-name: kubevious-mysql-0
    name: kubevious-mysql-0
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: kubevious-mysql
      uid: 8f9f0338-707f-11ea-8ebf-42010a800207
    resourceVersion: "33572067"
    selfLink: /api/v1/namespaces/kubevious/pods/kubevious-mysql-0
    uid: d68d2a6e-72ba-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: MYSQL_DATABASE
        value: kubevious
      - name: MYSQL_ALLOW_EMPTY_PASSWORD
        value: "1"
      image: mysql:8.0.19
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - mysqladmin
          - ping
        failureThreshold: 3
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: mysql
      ports:
      - containerPort: 3306
        name: mysql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - mysql
          - -h
          - 127.0.0.1
          - -e
          - SELECT 1
        failureThreshold: 3
        initialDelaySeconds: 5
        periodSeconds: 2
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 50m
          memory: 500Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: data
        subPath: mysql
      - mountPath: /etc/mysql/conf.d
        name: conf
      - mountPath: /docker-entrypoint-initdb.d
        name: init-script
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-tj4fr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: kubevious-mysql-0
    initContainers:
    - command:
      - bash
      - -c
      - |
        set -ex
        echo "[mysqld]" > /mnt/conf.d/server-id.cnf
        echo "server-id=1" >> /mnt/conf.d/server-id.cnf
        cp /mnt/config-map/master.cnf /mnt/conf.d/
      image: mysql:8.0.19
      imagePullPolicy: IfNotPresent
      name: init-mysql
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/conf.d
        name: conf
      - mountPath: /mnt/config-map
        name: config-map
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-tj4fr
        readOnly: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    subdomain: kubevious-mysql-svc
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-kubevious-mysql-0
    - emptyDir: {}
      name: conf
    - configMap:
        defaultMode: 420
        name: kubevious-mysql-conf
      name: config-map
    - configMap:
        defaultMode: 420
        name: kubevious-mysql-init-script
      name: init-script
    - name: default-token-tj4fr
      secret:
        defaultMode: 420
        secretName: default-token-tj4fr
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:17:18Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://b5771b0161b3daf547f28bde1d58c16d63b020680985ec5422180147c76cde9c
      image: mysql:8.0.19
      imageID: docker-pullable://mysql@sha256:4a30434ce03d2fa396d0414f075ad9ca9b0b578f14ea5685e24dcbf789450a2c
      lastState:
        terminated:
          containerID: docker://c04cb5e1111eacdc900c2d2f7f74350882270dc6ab2ad4067df65cb57594aa6a
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:17:19Z"
      name: mysql
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:50:26Z"
    hostIP: 10.128.0.37
    initContainerStatuses:
    - containerID: docker://5de012ad4a5c7766833507445c463dd09c2e59c577603ee570ec146d84637323
      image: mysql:8.0.19
      imageID: docker-pullable://mysql@sha256:4a30434ce03d2fa396d0414f075ad9ca9b0b578f14ea5685e24dcbf789450a2c
      lastState: {}
      name: init-mysql
      ready: true
      restartCount: 1
      state:
        terminated:
          containerID: docker://5de012ad4a5c7766833507445c463dd09c2e59c577603ee570ec146d84637323
          exitCode: 0
          finishedAt: "2020-04-01T13:49:21Z"
          reason: Completed
          startedAt: "2020-04-01T13:49:21Z"
    phase: Running
    podIP: 10.8.1.100
    qosClass: Burstable
    startTime: "2020-03-30T19:16:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.220/32
    creationTimestamp: "2020-04-08T07:33:00Z"
    generateName: kubevious-parser-85d9bf6d8c-
    labels:
      k8s-app: kubevious-parser
      pod-template-hash: 85d9bf6d8c
    name: kubevious-parser-85d9bf6d8c-dqqsg
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kubevious-parser-85d9bf6d8c
      uid: 2c4fdf29-796b-11ea-bb1c-42010a800057
    resourceVersion: "36202559"
    selfLink: /api/v1/namespaces/kubevious/pods/kubevious-parser-85d9bf6d8c-dqqsg
    uid: 2c5204d8-796b-11ea-bb1c-42010a800057
  spec:
    containers:
    - env:
      - name: NODE_ENV
        value: production
      - name: KUBEVIOUS_COLLECTOR
        value: http://kubevious-svc.kubevious.svc.cluster.local:4000/api/v1/collect
      image: kubevious/parser:0.5.6
      imagePullPolicy: IfNotPresent
      name: kubevious-parser
      ports:
      - containerPort: 4500
        name: http
        protocol: TCP
      resources:
        requests:
          cpu: 100m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kubevious-parser-token-lj9vh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kubevious-parser
    serviceAccountName: kubevious-parser
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kubevious-parser-token-lj9vh
      secret:
        defaultMode: 420
        secretName: kubevious-parser-token-lj9vh
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-04-08T07:33:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-08T07:33:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-08T07:33:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-04-08T07:33:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://c1035c1833e2ad308e7f56da8e17fe287a46650b211c94c79ee2f6896a858ec3
      image: kubevious/parser:0.5.6
      imageID: docker-pullable://kubevious/parser@sha256:a97d8e86b22d5899927b901dd83e3953928ff31ab4c67974e5ab1b227d956d39
      lastState: {}
      name: kubevious-parser
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-04-08T07:33:30Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.220
    qosClass: Burstable
    startTime: "2020-04-08T07:33:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.219/32
    creationTimestamp: "2020-04-08T07:33:00Z"
    generateName: kubevious-ui-686cfc86f-
    labels:
      k8s-app: kubevious-ui
      pod-template-hash: 686cfc86f
    name: kubevious-ui-686cfc86f-8g8f8
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kubevious-ui-686cfc86f
      uid: 2c68b921-796b-11ea-bb1c-42010a800057
    resourceVersion: "36202503"
    selfLink: /api/v1/namespaces/kubevious/pods/kubevious-ui-686cfc86f-8g8f8
    uid: 2c6c48eb-796b-11ea-bb1c-42010a800057
  spec:
    containers:
    - env:
      - name: NODE_ENV
        value: production
      - name: BACKEND_URL
        value: http://kubevious-svc.kubevious.svc.cluster.local:4000
      - name: FORCE_HTTPS
        value: "true"
      envFrom:
      - configMapRef:
          name: kubevious-mysql-client
      image: kubevious/ui:0.5.3
      imagePullPolicy: IfNotPresent
      name: kubevious-ui
      ports:
      - containerPort: 3000
        name: http
        protocol: TCP
      resources:
        requests:
          cpu: 25m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /app/views/partials
        name: header-config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kubevious-ui-token-wnvh6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kubevious-ui
    serviceAccountName: kubevious-ui
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: kubevious-ui-header
      name: header-config-volume
    - name: kubevious-ui-token-wnvh6
      secret:
        defaultMode: 420
        secretName: kubevious-ui-token-wnvh6
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-04-08T07:33:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-08T07:33:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-08T07:33:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-04-08T07:33:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://4db573767adcae7c662dfd64e8b6b9a0a2e189b01263a3dd415315ea05619453
      image: kubevious/ui:0.5.3
      imageID: docker-pullable://kubevious/ui@sha256:1c962d137878baf11563a5a36e8e401195685ea091f3410004bf556698ce3f57
      lastState: {}
      name: kubevious-ui
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-04-08T07:33:20Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.219
    qosClass: Burstable
    startTime: "2020-04-08T07:33:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/alertmanager-config: fc705a1674460ecc032254f930b2d2877349afe3e22607ff974a68360b57ab3e
      cni.projectcalico.org/podIP: 10.8.0.155/32
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2020-03-31T18:40:29Z"
    generateName: alertmanager-8487d7f7bb-
    labels:
      app: alertmanager
      pod-template-hash: 8487d7f7bb
    name: alertmanager-8487d7f7bb-zc26t
    namespace: openfaas
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: alertmanager-8487d7f7bb
      uid: 4870a1f1-3986-11ea-b115-42010a8001d6
    resourceVersion: "33302680"
    selfLink: /api/v1/namespaces/openfaas/pods/alertmanager-8487d7f7bb-zc26t
    uid: 184929e2-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - command:
      - alertmanager
      - --config.file=/alertmanager.yml
      - --storage.path=/alertmanager
      - --cluster.listen-address=
      image: prom/alertmanager:v0.18.0
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: 9093
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: alertmanager
      ports:
      - containerPort: 9093
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: 9093
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      resources:
        limits:
          memory: 50Mi
        requests:
          memory: 25Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /alertmanager.yml
        name: alertmanager-config
        subPath: alertmanager.yml
      - mountPath: /var/secrets
        name: auth
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-dtcgt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    nodeSelector:
      beta.kubernetes.io/arch: amd64
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: alertmanager.yml
          mode: 420
          path: alertmanager.yml
        name: alertmanager-config
      name: alertmanager-config
    - name: auth
      secret:
        defaultMode: 420
        secretName: basic-auth
    - name: default-token-dtcgt
      secret:
        defaultMode: 420
        secretName: default-token-dtcgt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://15a860ae0d684623fa5a93bde6694e4e0ec1ffdda05782f069be1c77c0fa2afd
      image: prom/alertmanager:v0.18.0
      imageID: docker-pullable://prom/alertmanager@sha256:3d52ab7ef3a2e23e26a73cfd0ce13189e3796fb276b9af3e79664e4f743d8a1d
      lastState: {}
      name: alertmanager
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-03-31T18:42:01Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.155
    qosClass: Burstable
    startTime: "2020-03-31T18:40:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.91/32
      prometheus.io.scrape: "false"
    creationTimestamp: "2020-03-30T19:01:38Z"
    generateName: basic-auth-plugin-6b4c47c965-
    labels:
      app: basic-auth-plugin
      pod-template-hash: 6b4c47c965
    name: basic-auth-plugin-6b4c47c965-8j4x9
    namespace: openfaas
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: basic-auth-plugin-6b4c47c965
      uid: 48705297-3986-11ea-b115-42010a8001d6
    resourceVersion: "33571820"
    selfLink: /api/v1/namespaces/openfaas/pods/basic-auth-plugin-6b4c47c965-8j4x9
    uid: e29e8bc2-72b8-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: secret_mount_path
        value: /var/secrets
      - name: basic_auth
        value: "true"
      image: openfaas/basic-auth-plugin:0.17.0
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: basic-auth-plugin
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 20m
          memory: 50Mi
      securityContext:
        readOnlyRootFilesystem: true
        runAsUser: 10001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/secrets
        name: auth
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-dtcgt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/arch: amd64
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: auth
      secret:
        defaultMode: 420
        secretName: basic-auth
    - name: default-token-dtcgt
      secret:
        defaultMode: 420
        secretName: default-token-dtcgt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://6abb2211cf8f0e24f59d1bb2cc00fd6223b89588950daa7e6daf3fc6edb53162
      image: openfaas/basic-auth-plugin:0.17.0
      imageID: docker-pullable://openfaas/basic-auth-plugin@sha256:94a59da8aa57cfe42a9b49c50ff2ede65f3ea0c330822e9399e8a97ab8c5012b
      lastState:
        terminated:
          containerID: docker://aed5830e62d176449ae5b24d376cb527019ecfaff7421f629ac1229584b5c8f1
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:17:21Z"
      name: basic-auth-plugin
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:31Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.91
    qosClass: Burstable
    startTime: "2020-03-30T19:16:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.167/32
      prometheus.io.scrape: "false"
    creationTimestamp: "2020-03-31T18:40:29Z"
    generateName: faas-idler-66ff47fdf5-
    labels:
      app: faas-idler
      pod-template-hash: 66ff47fdf5
    name: faas-idler-66ff47fdf5-7cqgd
    namespace: openfaas
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: faas-idler-66ff47fdf5
      uid: 48700c42-3986-11ea-b115-42010a8001d6
    resourceVersion: "33302394"
    selfLink: /api/v1/namespaces/openfaas/pods/faas-idler-66ff47fdf5-7cqgd
    uid: 187fbdae-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - command:
      - /home/app/faas-idler
      - -dry-run=true
      env:
      - name: gateway_url
        value: http://gateway.openfaas:8080/
      - name: prometheus_host
        value: prometheus.openfaas
      - name: prometheus_port
        value: "9090"
      - name: inactivity_duration
        value: 30m
      - name: reconcile_interval
        value: 2m
      image: openfaas/faas-idler:0.2.1
      imagePullPolicy: Always
      name: faas-idler
      resources:
        requests:
          memory: 64Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/secrets/
        name: auth
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-dtcgt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    nodeSelector:
      beta.kubernetes.io/arch: amd64
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: auth
      secret:
        defaultMode: 420
        secretName: basic-auth
    - name: default-token-dtcgt
      secret:
        defaultMode: 420
        secretName: default-token-dtcgt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://7ca84fedda599fe6623b8796b7a42fb8843b15ea973e42d77b9266837e52e637
      image: openfaas/faas-idler:0.2.1
      imageID: docker-pullable://openfaas/faas-idler@sha256:d47c6bc0462c825ed456c61f81af0ffab8198af3100d701654cc153121765d55
      lastState: {}
      name: faas-idler
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-03-31T18:42:03Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.167
    qosClass: Burstable
    startTime: "2020-03-31T18:40:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.76/32
      prometheus.io.port: "8082"
      prometheus.io.scrape: "true"
    creationTimestamp: "2020-03-30T19:01:39Z"
    generateName: gateway-d56c44b6d-
    labels:
      app: gateway
      pod-template-hash: d56c44b6d
    name: gateway-d56c44b6d-7kprp
    namespace: openfaas
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gateway-d56c44b6d
      uid: 9954d670-6bcf-11ea-923a-42010a800150
    resourceVersion: "33572232"
    selfLink: /api/v1/namespaces/openfaas/pods/gateway-d56c44b6d-7kprp
    uid: e2d3e276-72b8-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: read_timeout
        value: 65s
      - name: write_timeout
        value: 65s
      - name: upstream_timeout
        value: 60s
      - name: functions_provider_url
        value: http://127.0.0.1:8081/
      - name: direct_functions
        value: "true"
      - name: direct_functions_suffix
        value: openfaas-fn.svc.cluster.local
      - name: function_namespace
        value: openfaas-fn
      - name: faas_nats_address
        value: nats.openfaas.svc.cluster.local
      - name: faas_nats_port
        value: "4222"
      - name: faas_nats_channel
        value: faas-request
      - name: basic_auth
        value: "true"
      - name: secret_mount_path
        value: /var/secrets
      - name: auth_proxy_url
        value: http://basic-auth-plugin.openfaas:8080/validate
      - name: auth_pass_body
        value: "false"
      - name: scale_from_zero
        value: "true"
      - name: max_idle_conns
        value: "1024"
      - name: max_idle_conns_per_host
        value: "1024"
      image: openfaas/gateway:0.18.7
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: gateway
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        requests:
          cpu: 25m
          memory: 120Mi
      securityContext:
        readOnlyRootFilesystem: true
        runAsUser: 10001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/secrets
        name: auth
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: openfaas-controller-token-rxrff
        readOnly: true
    - env:
      - name: port
        value: "8081"
      - name: function_namespace
        value: openfaas-fn
      - name: read_timeout
        value: 60s
      - name: write_timeout
        value: 60s
      - name: image_pull_policy
        value: Always
      - name: http_probe
        value: "true"
      - name: set_nonroot_user
        value: "false"
      - name: readiness_probe_initial_delay_seconds
        value: "2"
      - name: readiness_probe_timeout_seconds
        value: "1"
      - name: readiness_probe_period_seconds
        value: "2"
      - name: liveness_probe_initial_delay_seconds
        value: "2"
      - name: liveness_probe_timeout_seconds
        value: "1"
      - name: liveness_probe_period_seconds
        value: "2"
      image: openfaas/faas-netes:0.9.15
      imagePullPolicy: IfNotPresent
      name: faas-netes
      ports:
      - containerPort: 8081
        protocol: TCP
      resources:
        requests:
          cpu: 25m
          memory: 120Mi
      securityContext:
        readOnlyRootFilesystem: true
        runAsUser: 10001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: faas-netes-temp-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: openfaas-controller-token-rxrff
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/arch: amd64
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: openfaas-controller
    serviceAccountName: openfaas-controller
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: faas-netes-temp-volume
    - name: auth
      secret:
        defaultMode: 420
        secretName: basic-auth
    - name: openfaas-controller-token-rxrff
      secret:
        defaultMode: 420
        secretName: openfaas-controller-token-rxrff
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:51:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:51:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9ecf5ec84016e51018cb2ad419e69e2cf364251bf9d520b426b1e103a22a9c5e
      image: openfaas/faas-netes:0.9.15
      imageID: docker-pullable://openfaas/faas-netes@sha256:0e0c51ec33307991cd48d226ca725c1db1f011d14bf5c69861b73f79b326f6ff
      lastState:
        terminated:
          containerID: docker://d8d8f9eb1d103ccc4623f4238f576196cb8aefeed119ebe0307a89d9a25befbf
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:17:45Z"
      name: faas-netes
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:30Z"
    - containerID: docker://e4c9dd094931cf80deccb35a80c15fa72737c2c4cb4b0e2fc88b1242500d257e
      image: openfaas/gateway:0.18.7
      imageID: docker-pullable://openfaas/gateway@sha256:2136bb0eceb7acb33edcdb0002e3a0d0fa7b9fe012d32525f36a1c322baf00c5
      lastState:
        terminated:
          containerID: docker://a58a44925845f52dc83f916cd26d779f6267461e52ceade58d659bcb7f970834
          exitCode: 1
          finishedAt: "2020-04-01T13:49:47Z"
          reason: Error
          startedAt: "2020-04-01T13:48:37Z"
      name: gateway
      ready: true
      restartCount: 2
      state:
        running:
          startedAt: "2020-04-01T13:50:25Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.76
    qosClass: Burstable
    startTime: "2020-03-30T19:16:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.0.156/32
      prometheus.io.scrape: "false"
      sidecar.istio.io/inject: "false"
    creationTimestamp: "2020-03-31T18:40:30Z"
    generateName: nats-7666fb76bd-
    labels:
      app: nats
      pod-template-hash: 7666fb76bd
    name: nats-7666fb76bd-8pfw6
    namespace: openfaas
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: nats-7666fb76bd
      uid: 486f7690-3986-11ea-b115-42010a8001d6
    resourceVersion: "33302508"
    selfLink: /api/v1/namespaces/openfaas/pods/nats-7666fb76bd-8pfw6
    uid: 18b80ed3-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - args:
      - --store
      - memory
      - --cluster_id
      - faas-cluster
      command:
      - /nats-streaming-server
      image: nats-streaming:0.11.2
      imagePullPolicy: Always
      name: nats
      ports:
      - containerPort: 4222
        protocol: TCP
      resources:
        requests:
          memory: 120Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-dtcgt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    nodeSelector:
      beta.kubernetes.io/arch: amd64
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-dtcgt
      secret:
        defaultMode: 420
        secretName: default-token-dtcgt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:11Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:11Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://71e490dbec41a4028269f523bc1f17cd6a32cbe91469c10533a59e55a7fd58ad
      image: nats-streaming:0.11.2
      imageID: docker-pullable://nats-streaming@sha256:b25585f075e6578613bd73779efb880e9a47c487ba101c0313272a7ebee38045
      lastState: {}
      name: nats
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-03-31T18:42:00Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.156
    qosClass: Burstable
    startTime: "2020-03-31T18:40:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/prometheus-config: c3bbdb127a0bfc5afa2014b462e13989f96cb8931d7e3650f50503b9586ed0e2
      cni.projectcalico.org/podIP: 10.8.1.95/32
      sidecar.istio.io/inject: "true"
    creationTimestamp: "2020-03-31T18:40:30Z"
    generateName: prometheus-6d4c6646b9-
    labels:
      app: prometheus
      pod-template-hash: 6d4c6646b9
    name: prometheus-6d4c6646b9-llzc9
    namespace: openfaas
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-6d4c6646b9
      uid: 487de90f-3986-11ea-b115-42010a8001d6
    resourceVersion: "33571929"
    selfLink: /api/v1/namespaces/openfaas/pods/prometheus-6d4c6646b9-llzc9
    uid: 18cc0293-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - command:
      - prometheus
      - --config.file=/etc/prometheus/prometheus.yml
      image: prom/prometheus:v2.11.0
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/healthy
          port: 9090
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: prometheus
      ports:
      - containerPort: 9090
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/healthy
          port: 9090
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      resources:
        requests:
          memory: 512Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/prometheus/prometheus.yml
        name: prometheus-config
        subPath: prometheus.yml
      - mountPath: /etc/prometheus/alert.rules.yml
        name: prometheus-config
        subPath: alert.rules.yml
      - mountPath: /prometheus/data
        name: prom-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: openfaas-prometheus-token-qbf4n
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/arch: amd64
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: openfaas-prometheus
    serviceAccountName: openfaas-prometheus
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: prometheus.yml
          mode: 420
          path: prometheus.yml
        - key: alert.rules.yml
          mode: 420
          path: alert.rules.yml
        name: prometheus-config
      name: prometheus-config
    - emptyDir: {}
      name: prom-data
    - name: openfaas-prometheus-token-qbf4n
      secret:
        defaultMode: 420
        secretName: openfaas-prometheus-token-qbf4n
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:41:05Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:41:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9f9f806f6f30debe7ee5ba8dc8240a1df8909849a9a28595c4c9713b1161f185
      image: prom/prometheus:v2.11.0
      imageID: docker-pullable://prom/prometheus@sha256:b6275644820cf7a256cd9502eafb35a0b1862b59a3c49afb8267f75ffe79efcc
      lastState: {}
      name: prometheus
      ready: true
      restartCount: 10
      state:
        running:
          startedAt: "2020-04-01T13:49:33Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.95
    qosClass: Burstable
    startTime: "2020-03-31T18:41:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.103/32
      prometheus.io.scrape: "false"
    creationTimestamp: "2020-03-30T19:01:39Z"
    generateName: queue-worker-75658c877d-
    labels:
      app: queue-worker
      pod-template-hash: 75658c877d
    name: queue-worker-75658c877d-fc6fs
    namespace: openfaas
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: queue-worker-75658c877d
      uid: 487e5b5d-3986-11ea-b115-42010a8001d6
    resourceVersion: "33572225"
    selfLink: /api/v1/namespaces/openfaas/pods/queue-worker-75658c877d-fc6fs
    uid: e31802ed-72b8-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: faas_nats_address
        value: nats.openfaas.svc.cluster.local
      - name: faas_nats_durable_queue_subscription
        value: "false"
      - name: faas_nats_channel
        value: faas-request
      - name: faas_nats_queue_group
        value: faas
      - name: faas_gateway_address
        value: gateway.openfaas.svc.cluster.local
      - name: gateway_invoke
        value: "true"
      - name: faas_function_suffix
        value: .openfaas-fn.svc.cluster.local
      - name: ack_wait
        value: 60s
      - name: secret_mount_path
        value: /var/secrets
      - name: basic_auth
        value: "true"
      image: openfaas/queue-worker:0.9.0
      imagePullPolicy: Always
      name: queue-worker
      resources:
        requests:
          cpu: 50m
          memory: 120Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/secrets
        name: auth
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-dtcgt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/arch: amd64
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: auth
      secret:
        defaultMode: 420
        secretName: basic-auth
    - name: default-token-dtcgt
      secret:
        defaultMode: 420
        secretName: default-token-dtcgt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:51:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:51:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://ac9c1f676cafbc91a2ac977fbd7e73665a67ad224cb82eab58e210dfb257e2a7
      image: openfaas/queue-worker:0.9.0
      imageID: docker-pullable://openfaas/queue-worker@sha256:1f03b2388f56ebe81e465fd2a98dc13dc8a9fda4842349f82bb91ee3a6cc3fa1
      lastState:
        terminated:
          containerID: docker://0dddf4273beb9dc3c2a4a09b3b6b6a86419e20211a60078235a22515b56a9af6
          exitCode: 2
          finishedAt: "2020-04-01T13:49:30Z"
          reason: Error
          startedAt: "2020-04-01T13:49:30Z"
      name: queue-worker
      ready: true
      restartCount: 2
      state:
        running:
          startedAt: "2020-04-01T13:50:19Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.103
    qosClass: Burstable
    startTime: "2020-03-30T19:16:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 8aa5a565fba7a2db98d46752087de8c1dcc83b70cd762c5829d5ba01270d54a2
      cni.projectcalico.org/podIP: 10.8.0.166/32
    creationTimestamp: "2020-03-31T18:40:30Z"
    generateName: polaris-dashboard-8554786c49-
    labels:
      app: polaris
      component: dashboard
      pod-template-hash: 8554786c49
    name: polaris-dashboard-8554786c49-xq69z
    namespace: polaris
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: polaris-dashboard-8554786c49
      uid: b00f0760-6bcf-11ea-923a-42010a800150
    resourceVersion: "33302978"
    selfLink: /api/v1/namespaces/polaris/pods/polaris-dashboard-8554786c49-xq69z
    uid: 18d477e5-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - command:
      - polaris
      - --dashboard
      - --config
      - /opt/app/config.yaml
      image: quay.io/fairwinds/polaris:0.6
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 20
        successThreshold: 1
        timeoutSeconds: 1
      name: dashboard
      ports:
      - containerPort: 8080
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 20
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 50m
          memory: 128Mi
        requests:
          cpu: 10m
          memory: 128Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        privileged: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /opt/app/config.yaml
        name: config
        readOnly: true
        subPath: config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: polaris-dashboard-token-phrlm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-w6zl
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: polaris-dashboard
    serviceAccountName: polaris-dashboard
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: polaris
      name: config
    - name: polaris-dashboard-token-phrlm
      secret:
        defaultMode: 420
        secretName: polaris-dashboard-token-phrlm
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:42:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://62e0f26abbc28b70ae8c51a0159bf093300fa8c2dcff86637623610c2b85ba73
      image: quay.io/fairwinds/polaris:0.6
      imageID: docker-pullable://quay.io/fairwinds/polaris@sha256:d48993260cae9d1282ca4b20179efa91123599445e1b6cdabf2db6e0ccc62070
      lastState: {}
      name: dashboard
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-03-31T18:42:03Z"
    hostIP: 10.128.0.35
    phase: Running
    podIP: 10.8.0.166
    qosClass: Burstable
    startTime: "2020-03-31T18:40:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.111/32
    creationTimestamp: "2020-03-31T18:40:30Z"
    generateName: carts-6bfcf84f4-
    labels:
      name: carts
      pod-template-hash: 6bfcf84f4
    name: carts-6bfcf84f4-hsqsd
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: carts-6bfcf84f4
      uid: cc4356c4-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571837"
    selfLink: /api/v1/namespaces/sock-shop/pods/carts-6bfcf84f4-hsqsd
    uid: 1910e589-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: ZIPKIN
        value: zipkin.jaeger.svc.cluster.local
      - name: JAVA_OPTS
        value: -Xms64m -Xmx128m -XX:PermSize=32m -XX:MaxPermSize=64m -XX:+UseG1GC
          -Djava.security.egd=file:/dev/urandom
      image: weaveworksdemos/carts:0.4.8
      imagePullPolicy: IfNotPresent
      name: carts
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 10001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-ckr79
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: tmp-volume
    - name: default-token-ckr79
      secret:
        defaultMode: 420
        secretName: default-token-ckr79
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://f313bd23cd2b2ac4b0be8c850d89da088e301530ee8376d2108bf50dc96272c4
      image: weaveworksdemos/carts:0.4.8
      imageID: docker-pullable://weaveworksdemos/carts@sha256:434d2f5a6e0e8beef1f253fe96f45b8437a703125fc003434c5282ecf8969a4f
      lastState: {}
      name: carts
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:59Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.111
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.86/32
    creationTimestamp: "2020-03-31T18:40:30Z"
    generateName: carts-db-6bfc588c5f-
    labels:
      name: carts-db
      pod-template-hash: 6bfc588c5f
    name: carts-db-6bfc588c5f-zrvzj
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: carts-db-6bfc588c5f
      uid: cbf93c93-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571745"
    selfLink: /api/v1/namespaces/sock-shop/pods/carts-db-6bfc588c5f-zrvzj
    uid: 19273582-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - image: mongo
      imagePullPolicy: Always
      name: carts-db
      ports:
      - containerPort: 27017
        name: mongo
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - CHOWN
          - SETGID
          - SETUID
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-ckr79
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: tmp-volume
    - name: default-token-ckr79
      secret:
        defaultMode: 420
        secretName: default-token-ckr79
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:04Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:04Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://33cca708efd36b9bbf215ff0f8b4b83d5e709c3ed232d4a55e70280bfbe76331
      image: mongo:latest
      imageID: docker-pullable://mongo@sha256:e02955d5aad655bff95edfd8860add767a75a760d7952966cbd3a01202684e95
      lastState: {}
      name: carts-db
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:48Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.86
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.79/32
    creationTimestamp: "2020-03-31T18:40:30Z"
    generateName: catalogue-77d4f66dbf-
    labels:
      name: catalogue
      pod-template-hash: 77d4f66dbf
    name: catalogue-77d4f66dbf-pj7jv
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: catalogue-77d4f66dbf
      uid: ccd19f94-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571675"
    selfLink: /api/v1/namespaces/sock-shop/pods/catalogue-77d4f66dbf-pj7jv
    uid: 19361672-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - image: weaveworksdemos/catalogue:0.3.5
      imagePullPolicy: IfNotPresent
      name: catalogue
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 10001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-ckr79
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-ckr79
      secret:
        defaultMode: 420
        secretName: default-token-ckr79
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://1ff88deef02b69c8580e3648a2762cdb4c76474da05b9102e7f4e9282b511046
      image: weaveworksdemos/catalogue:0.3.5
      imageID: docker-pullable://weaveworksdemos/catalogue@sha256:0147a65b7116569439eefb1a6dbed455fe022464ef70e0c3cab75bc4a226b39b
      lastState: {}
      name: catalogue
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:38Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.79
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.105/32
    creationTimestamp: "2020-03-31T18:40:31Z"
    generateName: catalogue-db-99cbcbb88-
    labels:
      name: catalogue-db
      pod-template-hash: 99cbcbb88
    name: catalogue-db-99cbcbb88-26kfx
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: catalogue-db-99cbcbb88
      uid: cc905ce0-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571908"
    selfLink: /api/v1/namespaces/sock-shop/pods/catalogue-db-99cbcbb88-26kfx
    uid: 195c2679-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: MYSQL_ROOT_PASSWORD
        value: fake_password
      - name: MYSQL_DATABASE
        value: socksdb
      image: weaveworksdemos/catalogue-db:0.3.0
      imagePullPolicy: IfNotPresent
      name: catalogue-db
      ports:
      - containerPort: 3306
        name: mysql
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-ckr79
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-ckr79
      secret:
        defaultMode: 420
        secretName: default-token-ckr79
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9865e7c856cf541a5fefdff7023cd275f6774ec04aa5d67e08078956b3dcfed5
      image: weaveworksdemos/catalogue-db:0.3.0
      imageID: docker-pullable://weaveworksdemos/catalogue-db@sha256:7ba74ec9adf88f6625b8d85d3323d1ee5232b39877e1590021ea485cf9457251
      lastState: {}
      name: catalogue-db
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:57Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.105
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.98/32
    creationTimestamp: "2020-03-30T19:01:40Z"
    generateName: front-end-b5f568888-
    labels:
      name: front-end
      pod-template-hash: b5f568888
    name: front-end-b5f568888-8gg8p
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: front-end-b5f568888
      uid: cd16e418-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571871"
    selfLink: /api/v1/namespaces/sock-shop/pods/front-end-b5f568888-8gg8p
    uid: e3a83a3d-72b8-11ea-8ebf-42010a800207
  spec:
    containers:
    - image: weaveworksdemos/front-end:0.3.12
      imagePullPolicy: IfNotPresent
      name: front-end
      ports:
      - containerPort: 8079
        protocol: TCP
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        capabilities:
          drop:
          - all
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 10001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-ckr79
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-ckr79
      secret:
        defaultMode: 420
        secretName: default-token-ckr79
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-30T19:16:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://c03c32138744cd339cdda48e137d070e2e427f2eee1bde4bc87520ec2851e5a7
      image: weaveworksdemos/front-end:0.3.12
      imageID: docker-pullable://weaveworksdemos/front-end@sha256:26a2d9b6b291dee2dca32fca3f5bff6c2fa07bb5954359afcbc8001cc70eac71
      lastState:
        terminated:
          containerID: docker://f1a958280233120bcc82893134ec5ef12cc5a5300b2918cc5a453729df61e7e6
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-30T19:17:31Z"
      name: front-end
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:30Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.98
    qosClass: Burstable
    startTime: "2020-03-30T19:16:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.106/32
    creationTimestamp: "2020-03-31T18:40:31Z"
    generateName: orders-54d7666f75-
    labels:
      name: orders
      pod-template-hash: 54d7666f75
    name: orders-54d7666f75-dmj9p
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: orders-54d7666f75
      uid: cd9b96c2-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571761"
    selfLink: /api/v1/namespaces/sock-shop/pods/orders-54d7666f75-dmj9p
    uid: 19829734-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: ZIPKIN
        value: zipkin.jaeger.svc.cluster.local
      - name: JAVA_OPTS
        value: -Xms64m -Xmx128m -XX:PermSize=32m -XX:MaxPermSize=64m -XX:+UseG1GC
          -Djava.security.egd=file:/dev/urandom
      image: weaveworksdemos/orders:0.4.7
      imagePullPolicy: IfNotPresent
      name: orders
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 10001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-ckr79
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: tmp-volume
    - name: default-token-ckr79
      secret:
        defaultMode: 420
        secretName: default-token-ckr79
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://538b83d68d0f66c6806c8875f38c0e732be883ab3057285bc3a5a55c76b3dc68
      image: weaveworksdemos/orders:0.4.7
      imageID: docker-pullable://weaveworksdemos/orders@sha256:b622e40e83433baf6374f15e076b53893f79958640fc6667dff597622eff03b9
      lastState: {}
      name: orders
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:53Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.106
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.68/32
    creationTimestamp: "2020-03-31T18:40:31Z"
    generateName: orders-db-7888765df9-
    labels:
      name: orders-db
      pod-template-hash: 7888765df9
    name: orders-db-7888765df9-dqsp6
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: orders-db-7888765df9
      uid: cd591f13-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571772"
    selfLink: /api/v1/namespaces/sock-shop/pods/orders-db-7888765df9-dqsp6
    uid: 1999274f-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - image: mongo
      imagePullPolicy: Always
      name: orders-db
      ports:
      - containerPort: 27017
        name: mongo
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - CHOWN
          - SETGID
          - SETUID
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-ckr79
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: tmp-volume
    - name: default-token-ckr79
      secret:
        defaultMode: 420
        secretName: default-token-ckr79
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://58297a9e99edd7de1c5272002c2c1722d6c1b652ae9135e16e5f61123d0fb38b
      image: mongo:latest
      imageID: docker-pullable://mongo@sha256:e02955d5aad655bff95edfd8860add767a75a760d7952966cbd3a01202684e95
      lastState: {}
      name: orders-db
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:31Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.68
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.110/32
    creationTimestamp: "2020-03-31T18:40:31Z"
    generateName: payment-7d8497bcd7-
    labels:
      name: payment
      pod-template-hash: 7d8497bcd7
    name: payment-7d8497bcd7-z8ksd
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: payment-7d8497bcd7
      uid: cdde9d8b-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571915"
    selfLink: /api/v1/namespaces/sock-shop/pods/payment-7d8497bcd7-z8ksd
    uid: 19b7a6af-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - image: weaveworksdemos/payment:0.4.3
      imagePullPolicy: IfNotPresent
      name: payment
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 10001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-ckr79
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-ckr79
      secret:
        defaultMode: 420
        secretName: default-token-ckr79
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9e444744d1f1bd45ad256cd523f0d3405082ff0edff9fc7f5a42dbf0fddcf4c3
      image: weaveworksdemos/payment:0.4.3
      imageID: docker-pullable://weaveworksdemos/payment@sha256:5ab1c9877480a018d4dda10d6dfa382776e6bca9fc1c60bacbb80903fde8cfe0
      lastState: {}
      name: payment
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:59Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.110
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.112/32
    creationTimestamp: "2020-03-31T18:40:31Z"
    generateName: queue-master-6bb75d8867-
    labels:
      name: queue-master
      pod-template-hash: 6bb75d8867
    name: queue-master-6bb75d8867-9dh6g
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: queue-master-6bb75d8867
      uid: ce2359f4-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571889"
    selfLink: /api/v1/namespaces/sock-shop/pods/queue-master-6bb75d8867-9dh6g
    uid: 19ce95f6-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - image: weaveworksdemos/queue-master:0.3.1
      imagePullPolicy: IfNotPresent
      name: queue-master
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-ckr79
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-ckr79
      secret:
        defaultMode: 420
        secretName: default-token-ckr79
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://0dd628f381b4d57610f5d30e8a2d2bb8b751c81e681303eceef2899512c37ea8
      image: weaveworksdemos/queue-master:0.3.1
      imageID: docker-pullable://weaveworksdemos/queue-master@sha256:6292d3095f4c7aeed8d863527f8ef6d7a75d3128f20fc61e57f398c100142712
      lastState:
        terminated:
          containerID: docker://b2c9686000f881591068e9a7225d7a0dfd89b1463edca5f25ed8820be750e5a6
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-31T18:42:49Z"
      name: queue-master
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:59Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.112
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.80/32
    creationTimestamp: "2020-03-31T18:40:32Z"
    generateName: rabbitmq-5786759fc9-
    labels:
      name: rabbitmq
      pod-template-hash: 5786759fc9
    name: rabbitmq-5786759fc9-w6t8r
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rabbitmq-5786759fc9
      uid: ce77cf2d-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571642"
    selfLink: /api/v1/namespaces/sock-shop/pods/rabbitmq-5786759fc9-w6t8r
    uid: 19edc499-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - image: rabbitmq:3.6.8
      imagePullPolicy: IfNotPresent
      name: rabbitmq
      ports:
      - containerPort: 5672
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - CHOWN
          - SETGID
          - SETUID
          - DAC_OVERRIDE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-ckr79
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-ckr79
      secret:
        defaultMode: 420
        secretName: default-token-ckr79
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:49:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://269ab9f89c0f8c784282e31004a62a232be9bdc9a33032512b7bb3c84d841b5d
      image: rabbitmq:3.6.8
      imageID: docker-pullable://rabbitmq@sha256:a9f4923559bbcd00b93b02e61615aef5eb6f1d1c98db51053bab0fa6b680db26
      lastState: {}
      name: rabbitmq
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:41Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.80
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.115/32
    creationTimestamp: "2020-03-31T18:40:32Z"
    generateName: shipping-58bc954d85-
    labels:
      name: shipping
      pod-template-hash: 58bc954d85
    name: shipping-58bc954d85-jv9zg
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: shipping-58bc954d85
      uid: cebe06a1-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571884"
    selfLink: /api/v1/namespaces/sock-shop/pods/shipping-58bc954d85-jv9zg
    uid: 1a0c4457-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: ZIPKIN
        value: zipkin.jaeger.svc.cluster.local
      - name: JAVA_OPTS
        value: -Xms64m -Xmx128m -XX:PermSize=32m -XX:MaxPermSize=64m -XX:+UseG1GC
          -Djava.security.egd=file:/dev/urandom
      image: weaveworksdemos/shipping:0.4.8
      imagePullPolicy: IfNotPresent
      name: shipping
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 10001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-ckr79
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: tmp-volume
    - name: default-token-ckr79
      secret:
        defaultMode: 420
        secretName: default-token-ckr79
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://12721f63538eba76ca0d16440dffa96a8cf1fa4bfd9e3537bddfbd1a5fedcdc2
      image: weaveworksdemos/shipping:0.4.8
      imageID: docker-pullable://weaveworksdemos/shipping@sha256:983305c948fded487f4a4acdeab5f898e89d577b4bc1ca3de7750076469ccad4
      lastState:
        terminated:
          containerID: docker://6015f98d8812038bf6764a0f791f59a9290917c27fc42ffe1d8a2d1cc4bf5b1f
          exitCode: 255
          finishedAt: "2020-04-01T13:46:48Z"
          reason: Error
          startedAt: "2020-03-31T18:45:20Z"
      name: shipping
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:57Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.115
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.97/32
    creationTimestamp: "2020-03-31T18:40:32Z"
    generateName: user-6b5f6896c4-
    labels:
      name: user
      pod-template-hash: 6b5f6896c4
    name: user-6b5f6896c4-7xjgm
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: user-6b5f6896c4
      uid: cf44f035-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571848"
    selfLink: /api/v1/namespaces/sock-shop/pods/user-6b5f6896c4-7xjgm
    uid: 1a2285e9-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - env:
      - name: MONGO_HOST
        value: user-db:27017
      image: weaveworksdemos/user:0.4.7
      imagePullPolicy: IfNotPresent
      name: user
      ports:
      - containerPort: 80
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 10001
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-ckr79
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-ckr79
      secret:
        defaultMode: 420
        secretName: default-token-ckr79
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://c3599b818477e82bd09413ebd0e2374a15c6a42241a7f558f401ea185bee6b3b
      image: weaveworksdemos/user:0.4.7
      imageID: docker-pullable://weaveworksdemos/user@sha256:2ffccc332963c89e035fea52201012208bf62df43a55fe461ad6598a5c757ab7
      lastState: {}
      name: user
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:48Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.97
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cni.projectcalico.org/podIP: 10.8.1.109/32
    creationTimestamp: "2020-03-31T18:40:32Z"
    generateName: user-db-6d5c9f6d84-
    labels:
      name: user-db
      pod-template-hash: 6d5c9f6d84
    name: user-db-6d5c9f6d84-4b9wj
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: user-db-6d5c9f6d84
      uid: ceff8b17-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571786"
    selfLink: /api/v1/namespaces/sock-shop/pods/user-db-6d5c9f6d84-4b9wj
    uid: 1a415272-737f-11ea-8ebf-42010a800207
  spec:
    containers:
    - image: weaveworksdemos/user-db:0.4.0
      imagePullPolicy: IfNotPresent
      name: user-db
      ports:
      - containerPort: 27017
        name: mongo
        protocol: TCP
      resources: {}
      securityContext:
        capabilities:
          add:
          - CHOWN
          - SETGID
          - SETUID
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-ckr79
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-kubevious-samples-pool-2-d17eaa99-95cn
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: tmp-volume
    - name: default-token-ckr79
      secret:
        defaultMode: 420
        secretName: default-token-ckr79
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-04-01T13:50:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-03-31T18:40:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://efbb0b154d07cde0a2a0e8a997bf3ce639817f36129d77450095957f0ca20fd1
      image: weaveworksdemos/user-db:0.4.0
      imageID: docker-pullable://weaveworksdemos/user-db@sha256:b43f0f8a76e0c908805fcec74d1ad7f4af4d93c4612632bd6dc20a87508e0b68
      lastState: {}
      name: user-db
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2020-04-01T13:49:59Z"
    hostIP: 10.128.0.37
    phase: Running
    podIP: 10.8.1.109
    qosClass: BestEffort
    startTime: "2020-03-31T18:40:36Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"name":"book-app-svc"},"name":"book-app-svc","namespace":"book"},"spec":{"ports":[{"port":80,"protocol":"TCP","targetPort":4000}],"selector":{"name":"book-app"},"type":"NodePort"}}
    creationTimestamp: "2020-01-11T21:42:30Z"
    labels:
      name: book-app-svc
    name: book-app-svc
    namespace: book
    resourceVersion: "197797"
    selfLink: /api/v1/namespaces/book/services/book-app-svc
    uid: 44b059ac-34bb-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.6.161
    externalTrafficPolicy: Cluster
    ports:
    - nodePort: 32017
      port: 80
      protocol: TCP
      targetPort: 4000
    selector:
      name: book-app
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"name":"book-web-svc"},"name":"book-web-svc","namespace":"book"},"spec":{"ports":[{"port":80,"protocol":"TCP","targetPort":4000}],"selector":{"app":"book-web"},"type":"NodePort"}}
    creationTimestamp: "2020-01-11T21:42:30Z"
    labels:
      name: book-web-svc
    name: book-web-svc
    namespace: book
    resourceVersion: "29697024"
    selfLink: /api/v1/namespaces/book/services/book-web-svc
    uid: 4504882c-34bb-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.5.141
    externalTrafficPolicy: Cluster
    ports:
    - nodePort: 32348
      port: 80
      protocol: TCP
      targetPort: 3500
    selector:
      app: book-web
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"name":"book-web-svc-2"},"name":"book-web-svc-2","namespace":"book"},"spec":{"ports":[{"port":80,"protocol":"TCP","targetPort":3500}],"selector":{"app":"book-web"},"type":"ClusterIP"}}
    creationTimestamp: "2020-01-11T22:30:26Z"
    labels:
      name: book-web-svc-2
    name: book-web-svc-2
    namespace: book
    resourceVersion: "22026622"
    selfLink: /api/v1/namespaces/book/services/book-web-svc-2
    uid: f70267ff-34c1-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.1.215
    ports:
    - port: 80
      protocol: TCP
      targetPort: 4000
    selector:
      app: book-web
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-11T06:00:34Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "38"
    selfLink: /api/v1/namespaces/default/services/kubernetes
    uid: aed9de20-3437-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.0.1
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: cert-manager
      app.kubernetes.io/instance: gitlab
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      helm.sh/chart: certmanager-v0.10.1
    name: gitlab-cert-manager
    namespace: gitlab
    resourceVersion: "2016269"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-cert-manager
    uid: 9942047d-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.11.104
    ports:
    - port: 9402
      protocol: TCP
      targetPort: 9402
    selector:
      app.kubernetes.io/instance: gitlab
      app.kubernetes.io/name: cert-manager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: webhook
      app.kubernetes.io/instance: gitlab
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      helm.sh/chart: certmanager-v0.10.1
    name: gitlab-cert-manager-webhook
    namespace: gitlab
    resourceVersion: "2016241"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-cert-manager-webhook
    uid: 992ec69a-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.13.118
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    selector:
      app: webhook
      app.kubernetes.io/instance: gitlab
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9236"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: gitaly
      chart: gitaly-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-gitaly
    namespace: gitlab
    resourceVersion: "2016233"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-gitaly
    uid: 99293180-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: None
    ports:
    - name: gitaly
      port: 8075
      protocol: TCP
      targetPort: 8075
    - name: gitaly-metrics
      port: 9236
      protocol: TCP
      targetPort: 9236
    selector:
      app: gitaly
      release: gitlab
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: gitlab-exporter
      chart: gitlab-exporter-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-gitlab-exporter
    namespace: gitlab
    resourceVersion: "2016278"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-gitlab-exporter
    uid: 99454717-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.11.0
    ports:
    - name: gitlab-exporter
      port: 9168
      protocol: TCP
      targetPort: 9168
    selector:
      app: gitlab-exporter
      release: gitlab
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: gitlab-shell
      chart: gitlab-shell-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-gitlab-shell
    namespace: gitlab
    resourceVersion: "2016282"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-gitlab-shell
    uid: 994749a3-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.6.248
    ports:
    - name: ssh
      port: 22
      protocol: TCP
      targetPort: 2222
    selector:
      app: gitlab-shell
      release: gitlab
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: minio
      chart: minio-0.4.3
      heritage: Helm
      release: gitlab
    name: gitlab-minio-svc
    namespace: gitlab
    resourceVersion: "2016268"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-minio-svc
    uid: 9940666f-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.13.104
    ports:
    - name: service
      port: 9000
      protocol: TCP
      targetPort: 9000
    selector:
      app: minio
      component: app
      release: gitlab
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.30.0-1
      component: controller
      heritage: Helm
      release: gitlab
    name: gitlab-nginx-ingress-controller
    namespace: gitlab
    resourceVersion: "2016943"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-nginx-ingress-controller
    uid: 9949937c-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.13.94
    externalTrafficPolicy: Local
    healthCheckNodePort: 31635
    ports:
    - name: http
      nodePort: 31297
      port: 80
      protocol: TCP
      targetPort: http
    - name: https
      nodePort: 31866
      port: 443
      protocol: TCP
      targetPort: https
    - name: gitlab-shell
      nodePort: 30052
      port: 22
      protocol: TCP
      targetPort: gitlab-shell
    selector:
      app: nginx-ingress
      component: controller
      release: gitlab
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 35.192.33.224
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "10254"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.30.0-1
      component: controller
      heritage: Helm
      release: gitlab
    name: gitlab-nginx-ingress-controller-metrics
    namespace: gitlab
    resourceVersion: "2016234"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-nginx-ingress-controller-metrics
    uid: 99299cbe-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.14.58
    ports:
    - name: metrics
      port: 9913
      protocol: TCP
      targetPort: metrics
    selector:
      app: nginx-ingress
      component: controller
      release: gitlab
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.30.0-1
      component: controller
      heritage: Helm
      release: gitlab
    name: gitlab-nginx-ingress-controller-stats
    namespace: gitlab
    resourceVersion: "2016245"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-nginx-ingress-controller-stats
    uid: 9930e8bf-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.11.120
    ports:
    - name: stats
      port: 18080
      protocol: TCP
      targetPort: stats
    selector:
      app: nginx-ingress
      component: controller
      release: gitlab
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.30.0-1
      component: default-backend
      heritage: Helm
      release: gitlab
    name: gitlab-nginx-ingress-default-backend
    namespace: gitlab
    resourceVersion: "2016262"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-nginx-ingress-default-backend
    uid: 993cb5fd-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.0.28
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
    selector:
      app: nginx-ingress
      component: default-backend
      release: gitlab
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9187"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: postgresql
      chart: postgresql-0.12.0
      heritage: Helm
      release: gitlab
    name: gitlab-postgresql
    namespace: gitlab
    resourceVersion: "2016254"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-postgresql
    uid: 99353b81-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.8.76
    ports:
    - name: postgresql
      port: 5432
      protocol: TCP
      targetPort: postgresql
    selector:
      app: postgresql
      release: gitlab
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: prometheus
      chart: prometheus-9.0.0
      component: server
      heritage: Helm
      release: gitlab
    name: gitlab-prometheus-server
    namespace: gitlab
    resourceVersion: "2016256"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-prometheus-server
    uid: 99390947-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.4.12
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9090
    selector:
      app: prometheus
      component: server
      release: gitlab
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9121"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: redis
      chart: redis-ha-0.1.0
      heritage: Helm
      release: gitlab
    name: gitlab-redis
    namespace: gitlab
    resourceVersion: "2016291"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-redis
    uid: 994bad3a-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.1.124
    ports:
    - name: redis
      port: 6379
      protocol: TCP
      targetPort: 6379
    - name: metrics
      port: 9121
      protocol: TCP
      targetPort: metrics
    selector:
      app: redis
      redis-node: "true"
      redis-role: master
      release: gitlab
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: redis
      chart: redis-ha-0.1.0
      heritage: Helm
      name: redis
      release: gitlab
      role: service
    name: gitlab-redis-sentinel
    namespace: gitlab
    resourceVersion: "2016294"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-redis-sentinel
    uid: 994e4b74-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.2.106
    ports:
    - port: 26379
      protocol: TCP
      targetPort: 26379
    selector:
      app: redis
      chart: redis-ha-0.1.0
      heritage: Helm
      redis-role: sentinel
      release: gitlab
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9121"
      prometheus.io/scrape: "true"
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: redis
      chart: redis-ha-0.1.0
      heritage: Helm
      release: gitlab
    name: gitlab-redis-slave-svc
    namespace: gitlab
    resourceVersion: "2016247"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-redis-slave-svc
    uid: 9932d1d5-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.2.58
    ports:
    - name: redis
      port: 6379
      protocol: TCP
      targetPort: 6379
    - name: metrics
      port: 9121
      protocol: TCP
      targetPort: metrics
    selector:
      app: redis
      redis-node: "true"
      redis-role: slave
      release: gitlab
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: registry
      chart: registry-0.3.0
      heritage: Helm
      release: gitlab
    name: gitlab-registry
    namespace: gitlab
    resourceVersion: "2016237"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-registry
    uid: 992bbeef-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.4.54
    ports:
    - name: registry
      port: 5000
      protocol: TCP
      targetPort: 5000
    selector:
      app: registry
      release: gitlab
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: unicorn
      chart: unicorn-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-unicorn
    namespace: gitlab
    resourceVersion: "2016286"
    selfLink: /api/v1/namespaces/gitlab/services/gitlab-unicorn
    uid: 99493854-398b-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.7.220
    ports:
    - name: unicorn
      port: 8080
      protocol: TCP
      targetPort: 8080
    - name: workhorse
      port: 8181
      protocol: TCP
      targetPort: 8181
    selector:
      app: unicorn
      release: gitlab
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"adservice","namespace":"hipster"},"spec":{"ports":[{"name":"grpc","port":9555,"targetPort":9555}],"selector":{"app":"adservice"},"type":"ClusterIP"}}
    creationTimestamp: "2020-01-11T07:11:52Z"
    name: adservice
    namespace: hipster
    resourceVersion: "16672"
    selfLink: /api/v1/namespaces/hipster/services/adservice
    uid: a44d8ac9-3441-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.1.194
    ports:
    - name: grpc
      port: 9555
      protocol: TCP
      targetPort: 9555
    selector:
      app: adservice
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"cartservice","namespace":"hipster"},"spec":{"ports":[{"name":"grpc","port":7070,"targetPort":7070}],"selector":{"app":"cartservice"},"type":"ClusterIP"}}
    creationTimestamp: "2020-01-11T07:11:49Z"
    name: cartservice
    namespace: hipster
    resourceVersion: "16592"
    selfLink: /api/v1/namespaces/hipster/services/cartservice
    uid: a2bdc174-3441-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.1.95
    ports:
    - name: grpc
      port: 7070
      protocol: TCP
      targetPort: 7070
    selector:
      app: cartservice
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"checkoutservice","namespace":"hipster"},"spec":{"ports":[{"name":"grpc","port":5050,"targetPort":5050}],"selector":{"app":"checkoutservice"},"type":"ClusterIP"}}
    creationTimestamp: "2020-01-11T07:11:46Z"
    name: checkoutservice
    namespace: hipster
    resourceVersion: "16493"
    selfLink: /api/v1/namespaces/hipster/services/checkoutservice
    uid: a0e854af-3441-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.1.108
    ports:
    - name: grpc
      port: 5050
      protocol: TCP
      targetPort: 5050
    selector:
      app: checkoutservice
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"currencyservice","namespace":"hipster"},"spec":{"ports":[{"name":"grpc","port":7000,"targetPort":7000}],"selector":{"app":"currencyservice"},"type":"ClusterIP"}}
    creationTimestamp: "2020-01-11T07:11:50Z"
    name: currencyservice
    namespace: hipster
    resourceVersion: "16618"
    selfLink: /api/v1/namespaces/hipster/services/currencyservice
    uid: a33e37ea-3441-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.3.196
    ports:
    - name: grpc
      port: 7000
      protocol: TCP
      targetPort: 7000
    selector:
      app: currencyservice
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"emailservice","namespace":"hipster"},"spec":{"ports":[{"name":"grpc","port":5000,"targetPort":8080}],"selector":{"app":"emailservice"},"type":"ClusterIP"}}
    creationTimestamp: "2020-01-11T07:11:45Z"
    name: emailservice
    namespace: hipster
    resourceVersion: "16478"
    selfLink: /api/v1/namespaces/hipster/services/emailservice
    uid: a09038cc-3441-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.15.140
    ports:
    - name: grpc
      port: 5000
      protocol: TCP
      targetPort: 8080
    selector:
      app: emailservice
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"frontend","namespace":"hipster"},"spec":{"ports":[{"name":"http","port":80,"targetPort":8080}],"selector":{"app":"frontend"},"type":"ClusterIP"}}
    creationTimestamp: "2020-01-11T07:11:47Z"
    name: frontend
    namespace: hipster
    resourceVersion: "16531"
    selfLink: /api/v1/namespaces/hipster/services/frontend
    uid: a190a01c-3441-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.12.165
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8080
    selector:
      app: frontend
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"frontend-external","namespace":"hipster"},"spec":{"ports":[{"name":"http","port":80,"targetPort":8080}],"selector":{"app":"frontend"},"type":"LoadBalancer"}}
    creationTimestamp: "2020-01-11T07:11:47Z"
    name: frontend-external
    namespace: hipster
    resourceVersion: "16895"
    selfLink: /api/v1/namespaces/hipster/services/frontend-external
    uid: a1b44970-3441-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.4.56
    externalTrafficPolicy: Cluster
    ports:
    - name: http
      nodePort: 31752
      port: 80
      protocol: TCP
      targetPort: 8080
    selector:
      app: frontend
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 34.67.184.11
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"paymentservice","namespace":"hipster"},"spec":{"ports":[{"name":"grpc","port":50051,"targetPort":50051}],"selector":{"app":"paymentservice"},"type":"ClusterIP"}}
    creationTimestamp: "2020-01-11T07:11:48Z"
    name: paymentservice
    namespace: hipster
    resourceVersion: "16551"
    selfLink: /api/v1/namespaces/hipster/services/paymentservice
    uid: a215b42a-3441-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.4.67
    ports:
    - name: grpc
      port: 50051
      protocol: TCP
      targetPort: 50051
    selector:
      app: paymentservice
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"productcatalogservice","namespace":"hipster"},"spec":{"ports":[{"name":"grpc","port":3550,"targetPort":3550}],"selector":{"app":"productcatalogservice"},"type":"ClusterIP"}}
    creationTimestamp: "2020-01-11T07:11:48Z"
    name: productcatalogservice
    namespace: hipster
    resourceVersion: "16570"
    selfLink: /api/v1/namespaces/hipster/services/productcatalogservice
    uid: a25cfd06-3441-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.7.101
    ports:
    - name: grpc
      port: 3550
      protocol: TCP
      targetPort: 3550
    selector:
      app: productcatalogservice
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"recommendationservice","namespace":"hipster"},"spec":{"ports":[{"name":"grpc","port":8080,"targetPort":8080}],"selector":{"app":"recommendationservice"},"type":"ClusterIP"}}
    creationTimestamp: "2020-01-11T07:11:47Z"
    name: recommendationservice
    namespace: hipster
    resourceVersion: "16512"
    selfLink: /api/v1/namespaces/hipster/services/recommendationservice
    uid: a1380ad6-3441-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.12.213
    ports:
    - name: grpc
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app: recommendationservice
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"redis-cart","namespace":"hipster"},"spec":{"ports":[{"name":"redis","port":6379,"targetPort":6379}],"selector":{"app":"redis-cart"},"type":"ClusterIP"}}
    creationTimestamp: "2020-01-11T07:11:51Z"
    name: redis-cart
    namespace: hipster
    resourceVersion: "16655"
    selfLink: /api/v1/namespaces/hipster/services/redis-cart
    uid: a402c654-3441-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.5.24
    ports:
    - name: redis
      port: 6379
      protocol: TCP
      targetPort: 6379
    selector:
      app: redis-cart
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"shippingservice","namespace":"hipster"},"spec":{"ports":[{"name":"grpc","port":50051,"targetPort":50051}],"selector":{"app":"shippingservice"},"type":"ClusterIP"}}
    creationTimestamp: "2020-01-11T07:11:51Z"
    name: shippingservice
    namespace: hipster
    resourceVersion: "16636"
    selfLink: /api/v1/namespaces/hipster/services/shippingservice
    uid: a3a38279-3441-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.14.107
    ports:
    - name: grpc
      port: 50051
      protocol: TCP
      targetPort: 50051
    selector:
      app: shippingservice
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"calico-typha","kubernetes.io/cluster-service":"true"},"name":"calico-typha","namespace":"kube-system"},"spec":{"ports":[{"name":"calico-typha","port":5473,"protocol":"TCP","targetPort":"calico-typha"}],"selector":{"k8s-app":"calico-typha"}}}
    creationTimestamp: "2020-01-22T17:44:03Z"
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: calico-typha
      kubernetes.io/cluster-service: "true"
    name: calico-typha
    namespace: kube-system
    resourceVersion: "4122836"
    selfLink: /api/v1/namespaces/kube-system/services/calico-typha
    uid: c7a029f2-3d3e-11ea-96d3-42010a80017a
  spec:
    clusterIP: 10.75.14.181
    ports:
    - name: calico-typha
      port: 5473
      protocol: TCP
      targetPort: calico-typha
    selector:
      k8s-app: calico-typha
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"glbc","kubernetes.io/cluster-service":"true","kubernetes.io/name":"GLBCDefaultBackend"},"name":"default-http-backend","namespace":"kube-system"},"spec":{"ports":[{"name":"http","port":80,"protocol":"TCP","targetPort":8080}],"selector":{"k8s-app":"glbc"},"type":"NodePort"}}
    creationTimestamp: "2020-01-11T06:01:01Z"
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: glbc
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: GLBCDefaultBackend
    name: default-http-backend
    namespace: kube-system
    resourceVersion: "308"
    selfLink: /api/v1/namespaces/kube-system/services/default-http-backend
    uid: bea70705-3437-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.9.178
    externalTrafficPolicy: Cluster
    ports:
    - name: http
      nodePort: 31689
      port: 80
      protocol: TCP
      targetPort: 8080
    selector:
      k8s-app: glbc
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","kubernetes.io/cluster-service":"true","kubernetes.io/name":"Heapster"},"name":"heapster","namespace":"kube-system"},"spec":{"ports":[{"port":80,"targetPort":8082}],"selector":{"k8s-app":"heapster"}}}
    creationTimestamp: "2020-01-11T06:01:02Z"
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: Heapster
    name: heapster
    namespace: kube-system
    resourceVersion: "379"
    selfLink: /api/v1/namespaces/kube-system/services/heapster
    uid: bf5afd8c-3437-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.4.80
    ports:
    - port: 80
      protocol: TCP
      targetPort: 8082
    selector:
      k8s-app: heapster
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kube-dns","kubernetes.io/cluster-service":"true","kubernetes.io/name":"KubeDNS"},"name":"kube-dns","namespace":"kube-system"},"spec":{"clusterIP":"10.75.0.10","ports":[{"name":"dns","port":53,"protocol":"UDP"},{"name":"dns-tcp","port":53,"protocol":"TCP"}],"selector":{"k8s-app":"kube-dns"}}}
    creationTimestamp: "2020-01-11T06:01:01Z"
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: KubeDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "316"
    selfLink: /api/v1/namespaces/kube-system/services/kube-dns
    uid: beb7055a-3437-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.0.10
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","kubernetes.io/cluster-service":"true","kubernetes.io/name":"Metrics-server"},"name":"metrics-server","namespace":"kube-system"},"spec":{"ports":[{"port":443,"protocol":"TCP","targetPort":"https"}],"selector":{"k8s-app":"metrics-server"}}}
    creationTimestamp: "2020-01-11T06:01:04Z"
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: Metrics-server
    name: metrics-server
    namespace: kube-system
    resourceVersion: "419"
    selfLink: /api/v1/namespaces/kube-system/services/metrics-server
    uid: c0625784-3437-11ea-9cdc-42010a8001cf
  spec:
    clusterIP: 10.75.11.117
    ports:
    - port: 443
      protocol: TCP
      targetPort: https
    selector:
      k8s-app: metrics-server
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-03-27T23:06:15Z"
    labels:
      k8s-app: kubevious-mysql-svc
    name: kubevious-mysql-svc
    namespace: kubevious
    resourceVersion: "31987757"
    selfLink: /api/v1/namespaces/kubevious/services/kubevious-mysql-svc
    uid: 8f85e7c8-707f-11ea-8ebf-42010a800207
  spec:
    clusterIP: None
    ports:
    - port: 3306
      protocol: TCP
      targetPort: 3306
    selector:
      k8s-app: kubevious-mysql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-03-27T23:06:15Z"
    labels:
      k8s-app: kubevious-svc
    name: kubevious-svc
    namespace: kubevious
    resourceVersion: "31987752"
    selfLink: /api/v1/namespaces/kubevious/services/kubevious-svc
    uid: 8f84ce2f-707f-11ea-8ebf-42010a800207
  spec:
    clusterIP: 10.75.11.4
    externalTrafficPolicy: Cluster
    ports:
    - nodePort: 32059
      port: 4000
      protocol: TCP
      targetPort: 4000
    selector:
      k8s-app: kubevious
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-03-27T23:06:15Z"
    labels:
      k8s-app: kubevious-ui-svc
    name: kubevious-ui-svc
    namespace: kubevious
    resourceVersion: "31987760"
    selfLink: /api/v1/namespaces/kubevious/services/kubevious-ui-svc
    uid: 8f88528b-707f-11ea-8ebf-42010a800207
  spec:
    clusterIP: 10.75.4.109
    externalTrafficPolicy: Cluster
    ports:
    - nodePort: 31248
      port: 3000
      protocol: TCP
      targetPort: 3000
    selector:
      k8s-app: kubevious-ui
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:05:48Z"
    labels:
      app: openfaas
      chart: openfaas-5.4.1
      component: alertmanager
      heritage: Helm
      release: openfaas
    name: alertmanager
    namespace: openfaas
    resourceVersion: "2007039"
    selfLink: /api/v1/namespaces/openfaas/services/alertmanager
    uid: 4851f214-3986-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.2.178
    ports:
    - port: 9093
      protocol: TCP
      targetPort: 9093
    selector:
      app: alertmanager
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:05:49Z"
    labels:
      app: openfaas
      chart: openfaas-5.4.1
      component: basic-auth-plugin
      heritage: Helm
      release: openfaas
    name: basic-auth-plugin
    namespace: openfaas
    resourceVersion: "2007047"
    selfLink: /api/v1/namespaces/openfaas/services/basic-auth-plugin
    uid: 4854d8bc-3986-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.11.119
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
    selector:
      app: basic-auth-plugin
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:05:48Z"
    labels:
      app: openfaas
      chart: openfaas-5.4.1
      component: gateway
      heritage: Helm
      release: openfaas
    name: gateway
    namespace: openfaas
    resourceVersion: "2007033"
    selfLink: /api/v1/namespaces/openfaas/services/gateway
    uid: 484ed27a-3986-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.13.219
    ports:
    - name: http
      port: 8080
      protocol: TCP
      targetPort: http
    selector:
      app: gateway
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:05:48Z"
    labels:
      app: openfaas
      chart: openfaas-5.4.1
      component: gateway
      heritage: Helm
      release: openfaas
    name: gateway-external
    namespace: openfaas
    resourceVersion: "2007040"
    selfLink: /api/v1/namespaces/openfaas/services/gateway-external
    uid: 4851856c-3986-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.3.135
    externalTrafficPolicy: Cluster
    ports:
    - name: http
      nodePort: 31112
      port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app: gateway
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:05:48Z"
    labels:
      app: openfaas
      chart: openfaas-5.4.1
      component: nats
      heritage: Helm
      release: openfaas
    name: nats
    namespace: openfaas
    resourceVersion: "2007029"
    selfLink: /api/v1/namespaces/openfaas/services/nats
    uid: 484dcc8f-3986-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.9.94
    ports:
    - name: clients
      port: 4222
      protocol: TCP
      targetPort: 4222
    selector:
      app: nats
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-01-18T00:05:48Z"
    labels:
      app: openfaas
      chart: openfaas-5.4.1
      component: prometheus
      heritage: Helm
      release: openfaas
    name: prometheus
    namespace: openfaas
    resourceVersion: "2007036"
    selfLink: /api/v1/namespaces/openfaas/services/prometheus
    uid: 484f41f2-3986-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.13.224
    ports:
    - port: 9090
      protocol: TCP
      targetPort: 9090
    selector:
      app: prometheus
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"polaris"},"name":"polaris-dashboard","namespace":"polaris"},"spec":{"ports":[{"name":"dashboard","port":80,"protocol":"TCP","targetPort":8080}],"selector":{"app":"polaris","component":"dashboard"},"type":"ClusterIP"}}
    creationTimestamp: "2020-02-27T04:07:09Z"
    labels:
      app: polaris
    name: polaris-dashboard
    namespace: polaris
    resourceVersion: "19359249"
    selfLink: /api/v1/namespaces/polaris/services/polaris-dashboard
    uid: 9fd3173c-5916-11ea-8ab4-42010a8000a7
  spec:
    clusterIP: 10.75.9.120
    ports:
    - name: dashboard
      port: 80
      protocol: TCP
      targetPort: 8080
    selector:
      app: polaris
      component: dashboard
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"name":"carts"},"name":"carts","namespace":"sock-shop"},"spec":{"ports":[{"port":80,"targetPort":80}],"selector":{"name":"carts"}}}
    creationTimestamp: "2020-01-17T21:24:52Z"
    labels:
      name: carts
    name: carts
    namespace: sock-shop
    resourceVersion: "1973589"
    selfLink: /api/v1/namespaces/sock-shop/services/carts
    uid: cc664684-396f-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.6.183
    ports:
    - port: 80
      protocol: TCP
      targetPort: 80
    selector:
      name: carts
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"name":"carts-db"},"name":"carts-db","namespace":"sock-shop"},"spec":{"ports":[{"port":27017,"targetPort":27017}],"selector":{"name":"carts-db"}}}
    creationTimestamp: "2020-01-17T21:24:51Z"
    labels:
      name: carts-db
    name: carts-db
    namespace: sock-shop
    resourceVersion: "1973573"
    selfLink: /api/v1/namespaces/sock-shop/services/carts-db
    uid: cc2167f8-396f-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.11.47
    ports:
    - port: 27017
      protocol: TCP
      targetPort: 27017
    selector:
      name: carts-db
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"name":"catalogue"},"name":"catalogue","namespace":"sock-shop"},"spec":{"ports":[{"port":80,"targetPort":80}],"selector":{"name":"catalogue"}}}
    creationTimestamp: "2020-01-17T21:24:53Z"
    labels:
      name: catalogue
    name: catalogue
    namespace: sock-shop
    resourceVersion: "1973622"
    selfLink: /api/v1/namespaces/sock-shop/services/catalogue
    uid: ccf28b30-396f-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.7.177
    ports:
    - port: 80
      protocol: TCP
      targetPort: 80
    selector:
      name: catalogue
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"name":"catalogue-db"},"name":"catalogue-db","namespace":"sock-shop"},"spec":{"ports":[{"port":3306,"targetPort":3306}],"selector":{"name":"catalogue-db"}}}
    creationTimestamp: "2020-01-17T21:24:52Z"
    labels:
      name: catalogue-db
    name: catalogue-db
    namespace: sock-shop
    resourceVersion: "1973605"
    selfLink: /api/v1/namespaces/sock-shop/services/catalogue-db
    uid: ccaffcff-396f-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.0.179
    ports:
    - port: 3306
      protocol: TCP
      targetPort: 3306
    selector:
      name: catalogue-db
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"name":"front-end"},"name":"front-end","namespace":"sock-shop"},"spec":{"ports":[{"nodePort":30001,"port":80,"targetPort":8079}],"selector":{"name":"front-end"},"type":"NodePort"}}
    creationTimestamp: "2020-01-17T21:24:53Z"
    labels:
      name: front-end
    name: front-end
    namespace: sock-shop
    resourceVersion: "1973639"
    selfLink: /api/v1/namespaces/sock-shop/services/front-end
    uid: cd3887dc-396f-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.0.146
    externalTrafficPolicy: Cluster
    ports:
    - nodePort: 30001
      port: 80
      protocol: TCP
      targetPort: 8079
    selector:
      name: front-end
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"name":"orders"},"name":"orders","namespace":"sock-shop"},"spec":{"ports":[{"port":80,"targetPort":80}],"selector":{"name":"orders"}}}
    creationTimestamp: "2020-01-17T21:24:54Z"
    labels:
      name: orders
    name: orders
    namespace: sock-shop
    resourceVersion: "1973671"
    selfLink: /api/v1/namespaces/sock-shop/services/orders
    uid: cdbb8cbb-396f-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.5.132
    ports:
    - port: 80
      protocol: TCP
      targetPort: 80
    selector:
      name: orders
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"name":"orders-db"},"name":"orders-db","namespace":"sock-shop"},"spec":{"ports":[{"port":27017,"targetPort":27017}],"selector":{"name":"orders-db"}}}
    creationTimestamp: "2020-01-17T21:24:53Z"
    labels:
      name: orders-db
    name: orders-db
    namespace: sock-shop
    resourceVersion: "1973655"
    selfLink: /api/v1/namespaces/sock-shop/services/orders-db
    uid: cd783455-396f-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.1.134
    ports:
    - port: 27017
      protocol: TCP
      targetPort: 27017
    selector:
      name: orders-db
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"name":"payment"},"name":"payment","namespace":"sock-shop"},"spec":{"ports":[{"port":80,"targetPort":80}],"selector":{"name":"payment"}}}
    creationTimestamp: "2020-01-17T21:24:54Z"
    labels:
      name: payment
    name: payment
    namespace: sock-shop
    resourceVersion: "1973688"
    selfLink: /api/v1/namespaces/sock-shop/services/payment
    uid: cdfec943-396f-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.3.33
    ports:
    - port: 80
      protocol: TCP
      targetPort: 80
    selector:
      name: payment
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{"prometheus.io/path":"/prometheus"},"labels":{"name":"queue-master"},"name":"queue-master","namespace":"sock-shop"},"spec":{"ports":[{"port":80,"targetPort":80}],"selector":{"name":"queue-master"}}}
      prometheus.io/path: /prometheus
    creationTimestamp: "2020-01-17T21:24:55Z"
    labels:
      name: queue-master
    name: queue-master
    namespace: sock-shop
    resourceVersion: "1973705"
    selfLink: /api/v1/namespaces/sock-shop/services/queue-master
    uid: ce4f4da9-396f-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.3.46
    ports:
    - port: 80
      protocol: TCP
      targetPort: 80
    selector:
      name: queue-master
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"name":"rabbitmq"},"name":"rabbitmq","namespace":"sock-shop"},"spec":{"ports":[{"port":5672,"targetPort":5672}],"selector":{"name":"rabbitmq"}}}
    creationTimestamp: "2020-01-17T21:24:55Z"
    labels:
      name: rabbitmq
    name: rabbitmq
    namespace: sock-shop
    resourceVersion: "1973722"
    selfLink: /api/v1/namespaces/sock-shop/services/rabbitmq
    uid: ce9baabe-396f-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.5.39
    ports:
    - port: 5672
      protocol: TCP
      targetPort: 5672
    selector:
      name: rabbitmq
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"name":"shipping"},"name":"shipping","namespace":"sock-shop"},"spec":{"ports":[{"port":80,"targetPort":80}],"selector":{"name":"shipping"}}}
    creationTimestamp: "2020-01-17T21:24:56Z"
    labels:
      name: shipping
    name: shipping
    namespace: sock-shop
    resourceVersion: "1973738"
    selfLink: /api/v1/namespaces/sock-shop/services/shipping
    uid: cede99c6-396f-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.15.25
    ports:
    - port: 80
      protocol: TCP
      targetPort: 80
    selector:
      name: shipping
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"name":"user"},"name":"user","namespace":"sock-shop"},"spec":{"ports":[{"port":80,"targetPort":80}],"selector":{"name":"user"}}}
    creationTimestamp: "2020-01-17T21:24:57Z"
    labels:
      name: user
    name: user
    namespace: sock-shop
    resourceVersion: "1973772"
    selfLink: /api/v1/namespaces/sock-shop/services/user
    uid: cf67b31f-396f-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.13.100
    ports:
    - port: 80
      protocol: TCP
      targetPort: 80
    selector:
      name: user
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"name":"user-db"},"name":"user-db","namespace":"sock-shop"},"spec":{"ports":[{"port":27017,"targetPort":27017}],"selector":{"name":"user-db"}}}
    creationTimestamp: "2020-01-17T21:24:56Z"
    labels:
      name: user-db
    name: user-db
    namespace: sock-shop
    resourceVersion: "1973753"
    selfLink: /api/v1/namespaces/sock-shop/services/user-db
    uid: cf20a203-396f-11ea-b115-42010a8001d6
  spec:
    clusterIP: 10.75.6.244
    ports:
    - port: 27017
      protocol: TCP
      targetPort: 27017
    selector:
      name: user-db
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "4"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"calico-node","kubernetes.io/cluster-service":"true"},"name":"calico-node","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"calico-node"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"calico-node"}},"spec":{"containers":[{"env":[{"name":"CALICO_DISABLE_FILE_LOGGING","value":"true"},{"name":"CALICO_NETWORKING_BACKEND","value":"none"},{"name":"DATASTORE_TYPE","value":"kubernetes"},{"name":"FELIX_DEFAULTENDPOINTTOHOSTACTION","value":"ACCEPT"},{"name":"FELIX_HEALTHENABLED","value":"true"},{"name":"FELIX_IGNORELOOSERPF","value":"true"},{"name":"FELIX_IPTABLESMANGLEALLOWACTION","value":"RETURN"},{"name":"FELIX_IPV6SUPPORT","value":"false"},{"name":"FELIX_LOGSEVERITYSYS","value":"none"},{"name":"FELIX_LOGSEVERITYSCREEN","value":"warning"},{"name":"FELIX_PROMETHEUSMETRICSENABLED","value":"true"},{"name":"FELIX_REPORTINGINTERVALSECS","value":"0"},{"name":"FELIX_TYPHAK8SSERVICENAME","value":"calico-typha"},{"name":"IP","value":""},{"name":"NO_DEFAULT_POOLS","value":"true"},{"name":"NODENAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"WAIT_FOR_DATASTORE","value":"true"}],"image":"gcr.io/projectcalico-org/node:v3.2.7","livenessProbe":{"failureThreshold":6,"httpGet":{"host":"localhost","path":"/liveness","port":9099},"initialDelaySeconds":10,"periodSeconds":10},"name":"calico-node","readinessProbe":{"httpGet":{"host":"localhost","path":"/readiness","port":9099},"periodSeconds":10},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true},{"mountPath":"/etc/calico","name":"etc-calico","readOnly":true},{"mountPath":"/var/run/calico","name":"var-run-calico","readOnly":false},{"mountPath":"/var/lib/calico","name":"var-lib-calico","readOnly":false}]},{"command":["/install-cni.sh"],"env":[{"name":"CNI_CONF_NAME","value":"10-calico.conflist"},{"name":"CNI_NETWORK_CONFIG","value":"{\n  \"name\": \"k8s-pod-network\",\n  \"cniVersion\": \"0.3.0\",\n  \"plugins\": [\n    {\n      \"type\": \"calico\",\n      \"mtu\": 1460,\n      \"log_level\": \"warning\",\n      \"datastore_type\": \"kubernetes\",\n      \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n      \"ipam\": {\n        \"type\": \"host-local\",\n        \"subnet\": \"usePodCidr\"\n      },\n      \"policy\": {\n        \"type\": \"k8s\",\n        \"k8s_auth_token\": \"__SERVICEACCOUNT_TOKEN__\"\n      },\n      \"kubernetes\": {\n        \"k8s_api_root\": \"https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__\",\n        \"kubeconfig\": \"__KUBECONFIG_FILEPATH__\"\n      }\n    },\n    {\n      \"type\": \"portmap\",\n      \"capabilities\": {\"portMappings\": true},\n      \"snat\": true\n    }\n  ]\n}"},{"name":"KUBERNETES_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}}],"image":"gcr.io/projectcalico-org/cni:v3.2.7","name":"install-cni","volumeMounts":[{"mountPath":"/host/opt/cni/bin","name":"cni-bin-dir"},{"mountPath":"/host/etc/cni/net.d","name":"cni-net-dir"}]}],"hostNetwork":true,"nodeSelector":{"beta.kubernetes.io/os":"linux","projectcalico.org/ds-ready":"true"},"priorityClassName":"system-node-critical","serviceAccountName":"calico-sa","terminationGracePeriodSeconds":0,"tolerations":[{"effect":"NoSchedule","operator":"Exists"},{"effect":"NoExecute","operator":"Exists"},{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/lib/modules"},"name":"lib-modules"},{"hostPath":{"path":"/etc/calico"},"name":"etc-calico"},{"hostPath":{"path":"/home/kubernetes/bin"},"name":"cni-bin-dir"},{"hostPath":{"path":"/etc/cni/net.d"},"name":"cni-net-dir"},{"hostPath":{"path":"/var/run/calico"},"name":"var-run-calico"},{"hostPath":{"path":"/var/lib/calico"},"name":"var-lib-calico"}]}},"updateStrategy":{"type":"RollingUpdate"}}}
    creationTimestamp: "2020-01-22T17:44:02Z"
    generation: 4
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: calico-node
      kubernetes.io/cluster-service: "true"
    name: calico-node
    namespace: kube-system
    resourceVersion: "33571192"
    selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/calico-node
    uid: c72c3f7d-3d3e-11ea-96d3-42010a80017a
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: calico-node
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-node
      spec:
        containers:
        - env:
          - name: CALICO_DISABLE_FILE_LOGGING
            value: "true"
          - name: CALICO_NETWORKING_BACKEND
            value: none
          - name: DATASTORE_TYPE
            value: kubernetes
          - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
            value: ACCEPT
          - name: FELIX_HEALTHENABLED
            value: "true"
          - name: FELIX_IGNORELOOSERPF
            value: "true"
          - name: FELIX_IPTABLESMANGLEALLOWACTION
            value: RETURN
          - name: FELIX_IPV6SUPPORT
            value: "false"
          - name: FELIX_LOGSEVERITYSYS
            value: none
          - name: FELIX_LOGSEVERITYSCREEN
            value: warning
          - name: FELIX_PROMETHEUSMETRICSENABLED
            value: "true"
          - name: FELIX_REPORTINGINTERVALSECS
            value: "0"
          - name: FELIX_TYPHAK8SSERVICENAME
            value: calico-typha
          - name: IP
          - name: NO_DEFAULT_POOLS
            value: "true"
          - name: NODENAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: WAIT_FOR_DATASTORE
            value: "true"
          image: gcr.io/projectcalico-org/node:v3.2.7
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              host: localhost
              path: /liveness
              port: 9099
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: calico-node
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: localhost
              path: /readiness
              port: 9099
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /etc/calico
            name: etc-calico
            readOnly: true
          - mountPath: /var/run/calico
            name: var-run-calico
          - mountPath: /var/lib/calico
            name: var-lib-calico
        - command:
          - /install-cni.sh
          env:
          - name: CNI_CONF_NAME
            value: 10-calico.conflist
          - name: CNI_NETWORK_CONFIG
            value: |-
              {
                "name": "k8s-pod-network",
                "cniVersion": "0.3.0",
                "plugins": [
                  {
                    "type": "calico",
                    "mtu": 1460,
                    "log_level": "warning",
                    "datastore_type": "kubernetes",
                    "nodename": "__KUBERNETES_NODE_NAME__",
                    "ipam": {
                      "type": "host-local",
                      "subnet": "usePodCidr"
                    },
                    "policy": {
                      "type": "k8s",
                      "k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"
                    },
                    "kubernetes": {
                      "k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",
                      "kubeconfig": "__KUBECONFIG_FILEPATH__"
                    }
                  },
                  {
                    "type": "portmap",
                    "capabilities": {"portMappings": true},
                    "snat": true
                  }
                ]
              }
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: gcr.io/projectcalico-org/cni:v3.2.7
          imagePullPolicy: IfNotPresent
          name: install-cni
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /host/opt/cni/bin
            name: cni-bin-dir
          - mountPath: /host/etc/cni/net.d
            name: cni-net-dir
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          beta.kubernetes.io/os: linux
          projectcalico.org/ds-ready: "true"
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-sa
        serviceAccountName: calico-sa
        terminationGracePeriodSeconds: 0
        tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /etc/calico
            type: ""
          name: etc-calico
        - hostPath:
            path: /home/kubernetes/bin
            type: ""
          name: cni-bin-dir
        - hostPath:
            path: /etc/cni/net.d
            type: ""
          name: cni-net-dir
        - hostPath:
            path: /var/run/calico
            type: ""
          name: var-run-calico
        - hostPath:
            path: /var/lib/calico
            type: ""
          name: var-lib-calico
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 4
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "4"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd-gcp","kubernetes.io/cluster-service":"true","version":"v3.2.0"},"name":"fluentd-gcp-v3.2.0","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"fluentd-gcp","version":"v3.2.0"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"fluentd-gcp","kubernetes.io/cluster-service":"true","version":"v3.2.0"}},"spec":{"containers":[{"env":[{"name":"NODE_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"spec.nodeName"}}},{"name":"STACKDRIVER_METADATA_AGENT_URL","value":"http://$(NODE_NAME):8799"}],"image":"gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.17","livenessProbe":{"exec":{"command":["/bin/sh","-c","LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${STUCK_THRESHOLD_SECONDS:-900}; if [ ! -e /var/log/fluentd-buffers ]; then\n  exit 1;\nfi; touch -d \"${STUCK_THRESHOLD_SECONDS} seconds ago\" /tmp/marker-stuck; if [ -z \"$(find /var/log/fluentd-buffers -type d -newer /tmp/marker-stuck -print -quit)\" ]; then\n  rm -rf /var/log/fluentd-buffers;\n  exit 1;\nfi; touch -d \"${LIVENESS_THRESHOLD_SECONDS} seconds ago\" /tmp/marker-liveness; if [ -z \"$(find /var/log/fluentd-buffers -type d -newer /tmp/marker-liveness -print -quit)\" ]; then\n  exit 1;\nfi;\n"]},"initialDelaySeconds":600,"periodSeconds":60},"name":"fluentd-gcp","volumeMounts":[{"mountPath":"/var/log","name":"varlog"},{"mountPath":"/var/lib/docker/containers","name":"varlibdockercontainers","readOnly":true},{"mountPath":"/etc/google-fluentd/config.d","name":"config-volume"}]},{"command":["/monitor","--stackdriver-prefix=container.googleapis.com/internal/addons","--api-override=https://monitoring.googleapis.com/","--source=fluentd:http://localhost:24231?whitelisted=stackdriver_successful_requests_count,stackdriver_failed_requests_count,stackdriver_ingested_entries_count,stackdriver_dropped_entries_count","--pod-id=$(POD_NAME)","--namespace-id=$(POD_NAMESPACE)"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/prometheus-to-sd:v0.5.0","name":"prometheus-to-sd-exporter"}],"dnsPolicy":"Default","hostNetwork":true,"nodeSelector":{"beta.kubernetes.io/os":"linux"},"priorityClassName":"system-node-critical","serviceAccountName":"fluentd-gcp","terminationGracePeriodSeconds":60,"tolerations":[{"effect":"NoExecute","operator":"Exists"},{"effect":"NoSchedule","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/var/log"},"name":"varlog"},{"hostPath":{"path":"/var/lib/docker/containers"},"name":"varlibdockercontainers"},{"configMap":{"name":"fluentd-gcp-config-old-v1.2.5"},"name":"config-volume"}]}},"updateStrategy":{"type":"RollingUpdate"}}}
    creationTimestamp: "2020-01-11T06:01:02Z"
    generation: 4
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: fluentd-gcp
      kubernetes.io/cluster-service: "true"
      version: v3.2.0
    name: fluentd-gcp-v3.2.0
    namespace: kube-system
    resourceVersion: "33571170"
    selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/fluentd-gcp-v3.2.0
    uid: bf270dd4-3437-11ea-9cdc-42010a8001cf
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: fluentd-gcp
        kubernetes.io/cluster-service: "true"
        version: v3.2.0
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: fluentd-gcp
          kubernetes.io/cluster-service: "true"
          version: v3.2.0
      spec:
        containers:
        - env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: STACKDRIVER_METADATA_AGENT_URL
            value: http://$(NODE_NAME):8799
          image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.17
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - |
                LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${STUCK_THRESHOLD_SECONDS:-900}; if [ ! -e /var/log/fluentd-buffers ]; then
                  exit 1;
                fi; touch -d "${STUCK_THRESHOLD_SECONDS} seconds ago" /tmp/marker-stuck; if [ -z "$(find /var/log/fluentd-buffers -type d -newer /tmp/marker-stuck -print -quit)" ]; then
                  rm -rf /var/log/fluentd-buffers;
                  exit 1;
                fi; touch -d "${LIVENESS_THRESHOLD_SECONDS} seconds ago" /tmp/marker-liveness; if [ -z "$(find /var/log/fluentd-buffers -type d -newer /tmp/marker-liveness -print -quit)" ]; then
                  exit 1;
                fi;
            failureThreshold: 3
            initialDelaySeconds: 600
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 1
          name: fluentd-gcp
          resources:
            limits:
              cpu: "1"
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/log
            name: varlog
          - mountPath: /var/lib/docker/containers
            name: varlibdockercontainers
            readOnly: true
          - mountPath: /etc/google-fluentd/config.d
            name: config-volume
        - command:
          - /monitor
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --source=fluentd:http://localhost:24231?whitelisted=stackdriver_successful_requests_count,stackdriver_failed_requests_count,stackdriver_ingested_entries_count,stackdriver_dropped_entries_count
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd-exporter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: Default
        hostNetwork: true
        nodeSelector:
          beta.kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: fluentd-gcp
        serviceAccountName: fluentd-gcp
        terminationGracePeriodSeconds: 60
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /var/log
            type: ""
          name: varlog
        - hostPath:
            path: /var/lib/docker/containers
            type: ""
          name: varlibdockercontainers
        - configMap:
            defaultMode: 420
            name: fluentd-gcp-config-old-v1.2.5
          name: config-volume
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 4
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile"},"name":"ip-masq-agent","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"ip-masq-agent"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"ip-masq-agent"}},"spec":{"containers":[{"args":["--masq-chain=IP-MASQ","--nomasq-all-reserved-ranges"],"image":"k8s.gcr.io/ip-masq-agent-amd64:v2.4.1","name":"ip-masq-agent","resources":{"requests":{"cpu":"10m","memory":"16Mi"}},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/etc/config","name":"config"}]}],"hostNetwork":true,"nodeSelector":{"beta.kubernetes.io/masq-agent-ds-ready":"true"},"priorityClassName":"system-node-critical","serviceAccountName":"ip-masq-agent","tolerations":[{"effect":"NoSchedule","operator":"Exists"},{"effect":"NoExecute","operator":"Exists"},{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"configMap":{"items":[{"key":"config","path":"ip-masq-agent"}],"name":"ip-masq-agent","optional":true},"name":"config"}]}}}}
    creationTimestamp: "2020-01-22T17:44:06Z"
    generation: 2
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
    name: ip-masq-agent
    namespace: kube-system
    resourceVersion: "33571180"
    selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/ip-masq-agent
    uid: c99105c9-3d3e-11ea-96d3-42010a80017a
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: ip-masq-agent
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: ip-masq-agent
      spec:
        containers:
        - args:
          - --masq-chain=IP-MASQ
          - --nomasq-all-reserved-ranges
          image: k8s.gcr.io/ip-masq-agent-amd64:v2.4.1
          imagePullPolicy: IfNotPresent
          name: ip-masq-agent
          resources:
            requests:
              cpu: 10m
              memory: 16Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          beta.kubernetes.io/masq-agent-ds-ready: "true"
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: ip-masq-agent
        serviceAccountName: ip-masq-agent
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: config
              path: ip-masq-agent
            name: ip-masq-agent
            optional: true
          name: config
    updateStrategy:
      type: OnDelete
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 2
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"metadata-proxy","kubernetes.io/cluster-service":"true","version":"v0.1"},"name":"metadata-proxy-v0.1","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"metadata-proxy","version":"v0.1"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"metadata-proxy","kubernetes.io/cluster-service":"true","version":"v0.1"}},"spec":{"containers":[{"image":"k8s.gcr.io/metadata-proxy:v0.1.11","name":"metadata-proxy","resources":{"limits":{"cpu":"30m","memory":"25Mi"},"requests":{"cpu":"30m","memory":"25Mi"}},"securityContext":{"privileged":true}},{"command":["/monitor","--stackdriver-prefix=container.googleapis.com/internal/addons","--api-override=https://monitoring.googleapis.com/","--source=metadata_proxy:http://127.0.0.1:989?whitelisted=request_count","--pod-id=$(POD_NAME)","--namespace-id=$(POD_NAMESPACE)"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/prometheus-to-sd:v0.5.0","name":"prometheus-to-sd-exporter","resources":{"limits":{"cpu":"2m","memory":"20Mi"},"requests":{"cpu":"2m","memory":"20Mi"}}}],"dnsPolicy":"Default","hostNetwork":true,"nodeSelector":{"beta.kubernetes.io/metadata-proxy-ready":"true","beta.kubernetes.io/os":"linux"},"priorityClassName":"system-node-critical","serviceAccountName":"metadata-proxy","terminationGracePeriodSeconds":30,"tolerations":[{"effect":"NoExecute","operator":"Exists"},{"effect":"NoSchedule","operator":"Exists"}]}},"updateStrategy":{"type":"RollingUpdate"}}}
    creationTimestamp: "2020-01-11T06:01:02Z"
    generation: 2
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: metadata-proxy
      kubernetes.io/cluster-service: "true"
      version: v0.1
    name: metadata-proxy-v0.1
    namespace: kube-system
    resourceVersion: "31420505"
    selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/metadata-proxy-v0.1
    uid: bf8bd124-3437-11ea-9cdc-42010a8001cf
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: metadata-proxy
        kubernetes.io/cluster-service: "true"
        version: v0.1
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: metadata-proxy
          kubernetes.io/cluster-service: "true"
          version: v0.1
      spec:
        containers:
        - image: k8s.gcr.io/metadata-proxy:v0.1.11
          imagePullPolicy: IfNotPresent
          name: metadata-proxy
          resources:
            limits:
              cpu: 30m
              memory: 25Mi
            requests:
              cpu: 30m
              memory: 25Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --source=metadata_proxy:http://127.0.0.1:989?whitelisted=request_count
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd-exporter
          resources:
            limits:
              cpu: 2m
              memory: 20Mi
            requests:
              cpu: 2m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: Default
        hostNetwork: true
        nodeSelector:
          beta.kubernetes.io/metadata-proxy-ready: "true"
          beta.kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metadata-proxy
        serviceAccountName: metadata-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 0
    desiredNumberScheduled: 0
    numberMisscheduled: 0
    numberReady: 0
    observedGeneration: 2
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"nvidia-gpu-device-plugin"},"name":"nvidia-gpu-device-plugin","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"nvidia-gpu-device-plugin"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"nvidia-gpu-device-plugin"}},"spec":{"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"cloud.google.com/gke-accelerator","operator":"Exists"}]}]}}},"containers":[{"command":["/usr/bin/nvidia-gpu-device-plugin","-logtostderr"],"image":"k8s.gcr.io/nvidia-gpu-device-plugin@sha256:4b036e8844920336fa48f36edeb7d4398f426d6a934ba022848deed2edbf09aa","name":"nvidia-gpu-device-plugin","resources":{"limits":{"cpu":"50m","memory":"10Mi"},"requests":{"cpu":"50m","memory":"10Mi"}},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/device-plugin","name":"device-plugin"},{"mountPath":"/dev","name":"dev"}]}],"priorityClassName":"system-node-critical","tolerations":[{"effect":"NoExecute","operator":"Exists"},{"effect":"NoSchedule","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/var/lib/kubelet/device-plugins"},"name":"device-plugin"},{"hostPath":{"path":"/dev"},"name":"dev"}]}},"updateStrategy":{"type":"RollingUpdate"}}}
    creationTimestamp: "2020-01-11T06:01:13Z"
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: nvidia-gpu-device-plugin
    name: nvidia-gpu-device-plugin
    namespace: kube-system
    resourceVersion: "467"
    selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/nvidia-gpu-device-plugin
    uid: c5fd4ba3-3437-11ea-9cdc-42010a8001cf
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: nvidia-gpu-device-plugin
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: nvidia-gpu-device-plugin
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: cloud.google.com/gke-accelerator
                  operator: Exists
        containers:
        - command:
          - /usr/bin/nvidia-gpu-device-plugin
          - -logtostderr
          image: k8s.gcr.io/nvidia-gpu-device-plugin@sha256:4b036e8844920336fa48f36edeb7d4398f426d6a934ba022848deed2edbf09aa
          imagePullPolicy: IfNotPresent
          name: nvidia-gpu-device-plugin
          resources:
            limits:
              cpu: 50m
              memory: 10Mi
            requests:
              cpu: 50m
              memory: 10Mi
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /device-plugin
            name: device-plugin
          - mountPath: /dev
            name: dev
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /var/lib/kubelet/device-plugins
            type: ""
          name: device-plugin
        - hostPath:
            path: /dev
            type: ""
          name: dev
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 0
    desiredNumberScheduled: 0
    numberMisscheduled: 0
    numberReady: 0
    observedGeneration: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","kubernetes.io/cluster-service":"true"},"name":"prometheus-to-sd","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"prometheus-to-sd"}},"template":{"metadata":{"labels":{"k8s-app":"prometheus-to-sd"}},"spec":{"containers":[{"command":["/monitor","--source=kube-proxy:http://localhost:10249?whitelisted=sync_proxy_rules_latency_microseconds\u0026metricsPrefix=container.googleapis.com/internal/addons","--source=kubelet:http://localhost:10255?whitelisted=docker_operations,docker_operations_errors,runtime_operations,runtime_operations_errors,runtime_operations_latency_microseconds,pleg_relist_latency_microseconds,pod_start_latency_microseconds,rest_client_requests_total,storage_operation_duration_seconds,storage_operation_errors_total,run_podsandbox_duration_seconds,run_podsandbox_errors_total,storage_operation_status_count","--source=kubelet:http://localhost:10255/metrics/probes?whitelisted=prober_probe_total\u0026podIdLabel=pod\u0026namespaceIdLabel=namespace\u0026containerNameLabel=container","--stackdriver-prefix=container.googleapis.com/internal/nodes","--api-override=https://monitoring.googleapis.com/","--export-interval=120s"],"image":"k8s.gcr.io/prometheus-to-sd:v0.8.2","imagePullPolicy":"IfNotPresent","name":"prometheus-to-sd"},{"command":["/monitor","--source=kube-proxy:http://localhost:10249?whitelisted=sync_proxy_rules_latency_microseconds\u0026metricsPrefix=kubernetes.io/internal/addons","--source=kubelet:http://localhost:10255?whitelisted=docker_operations,docker_operations_errors,runtime_operations,runtime_operations_errors,runtime_operations_latency_microseconds,pleg_relist_latency_microseconds,pod_start_latency_microseconds,rest_client_requests_total,storage_operation_duration_seconds,storage_operation_errors_total,run_podsandbox_duration_seconds,run_podsandbox_errors_total,storage_operation_status_count","--source=kubelet:http://localhost:10255/metrics/probes?whitelisted=prober_probe_total\u0026podIdLabel=pod\u0026namespaceIdLabel=namespace\u0026containerNameLabel=container","--stackdriver-prefix=kubernetes.io/internal/nodes","--api-override=https://monitoring.googleapis.com/","--monitored-resource-type-prefix=k8s_","--monitored-resource-labels=location=us-central1-a","--export-interval=120s"],"image":"k8s.gcr.io/prometheus-to-sd:v0.8.2","imagePullPolicy":"IfNotPresent","name":"prometheus-to-sd-new-model","resources":{"limits":{"cpu":"3m","memory":"20Mi"},"requests":{"cpu":"1m","memory":"20Mi"}}}],"hostNetwork":true,"nodeSelector":{"beta.kubernetes.io/os":"linux"},"priorityClassName":"system-node-critical","serviceAccountName":"prometheus-to-sd","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}]}}}}
    creationTimestamp: "2020-01-11T06:01:02Z"
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      kubernetes.io/cluster-service: "true"
    name: prometheus-to-sd
    namespace: kube-system
    resourceVersion: "33571177"
    selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/prometheus-to-sd
    uid: bf82a533-3437-11ea-9cdc-42010a8001cf
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: prometheus-to-sd
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: prometheus-to-sd
      spec:
        containers:
        - command:
          - /monitor
          - --source=kube-proxy:http://localhost:10249?whitelisted=sync_proxy_rules_latency_microseconds&metricsPrefix=container.googleapis.com/internal/addons
          - --source=kubelet:http://localhost:10255?whitelisted=docker_operations,docker_operations_errors,runtime_operations,runtime_operations_errors,runtime_operations_latency_microseconds,pleg_relist_latency_microseconds,pod_start_latency_microseconds,rest_client_requests_total,storage_operation_duration_seconds,storage_operation_errors_total,run_podsandbox_duration_seconds,run_podsandbox_errors_total,storage_operation_status_count
          - --source=kubelet:http://localhost:10255/metrics/probes?whitelisted=prober_probe_total&podIdLabel=pod&namespaceIdLabel=namespace&containerNameLabel=container
          - --stackdriver-prefix=container.googleapis.com/internal/nodes
          - --api-override=https://monitoring.googleapis.com/
          - --export-interval=120s
          image: k8s.gcr.io/prometheus-to-sd:v0.8.2
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=kube-proxy:http://localhost:10249?whitelisted=sync_proxy_rules_latency_microseconds&metricsPrefix=kubernetes.io/internal/addons
          - --source=kubelet:http://localhost:10255?whitelisted=docker_operations,docker_operations_errors,runtime_operations,runtime_operations_errors,runtime_operations_latency_microseconds,pleg_relist_latency_microseconds,pod_start_latency_microseconds,rest_client_requests_total,storage_operation_duration_seconds,storage_operation_errors_total,run_podsandbox_duration_seconds,run_podsandbox_errors_total,storage_operation_status_count
          - --source=kubelet:http://localhost:10255/metrics/probes?whitelisted=prober_probe_total&podIdLabel=pod&namespaceIdLabel=namespace&containerNameLabel=container
          - --stackdriver-prefix=kubernetes.io/internal/nodes
          - --api-override=https://monitoring.googleapis.com/
          - --monitored-resource-type-prefix=k8s_
          - --monitored-resource-labels=location=us-central1-a
          - --export-interval=120s
          image: k8s.gcr.io/prometheus-to-sd:v0.8.2
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd-new-model
          resources:
            limits:
              cpu: 3m
              memory: 20Mi
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          beta.kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: prometheus-to-sd
        serviceAccountName: prometheus-to-sd
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 2
    desiredNumberScheduled: 2
    numberAvailable: 2
    numberMisscheduled: 0
    numberReady: 2
    observedGeneration: 1
    updatedNumberScheduled: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"book-app"},"name":"book-app","namespace":"book"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"book-app"}},"template":{"metadata":{"labels":{"app":"book-app"}},"spec":{"containers":[{"command":["/cloud_sql_proxy","-instances=my-project:us-central1:mysql-db-name=tcp:3306","-credential_file=/var/secrets/google/service-key.json"],"image":"gcr.io/cloudsql-docker/gce-proxy:1.11","imagePullPolicy":"IfNotPresent","name":"cloudsql-proxy","resources":{},"securityContext":{"allowPrivilegeEscalation":false,"runAsUser":2},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/var/secrets/google","name":"google-cloud-key","readOnly":true}]},{"env":[{"name":"MY_ENV_VAR_2","value":"abcd1234"}],"envFrom":[{"configMapRef":{"name":"my-config-1"}}],"image":"nginx:1.16","name":"nginx","ports":[{"containerPort":3000,"name":"default","protocol":"TCP"}],"resources":{"limits":{"cpu":"100m"},"requests":{"cpu":"50m","memory":"100Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","securityContext":{},"terminationGracePeriodSeconds":30,"volumes":[{"name":"google-cloud-key","secret":{"secretName":"gprod-addr-main-app"}}]}}}}
    creationTimestamp: "2020-01-11T21:42:30Z"
    generation: 3
    labels:
      app: book-app
    name: book-app
    namespace: book
    resourceVersion: "29696837"
    selfLink: /apis/apps/v1/namespaces/book/deployments/book-app
    uid: 44d4077a-34bb-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: book-app
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: book-app
      spec:
        containers:
        - command:
          - /cloud_sql_proxy
          - -instances=my-project:us-central1:mysql-db-name=tcp:3306
          - -credential_file=/var/secrets/google/service-key.json
          image: gcr.io/cloudsql-docker/gce-proxy:1.11
          imagePullPolicy: IfNotPresent
          name: cloudsql-proxy
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            runAsUser: 2
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/secrets/google
            name: google-cloud-key
            readOnly: true
        - env:
          - name: MY_ENV_VAR_2
            value: abcd1234
          envFrom:
          - configMapRef:
              name: my-config-1
          image: nginx:1.16
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - containerPort: 3000
            name: default
            protocol: TCP
          resources:
            limits:
              cpu: 10m
            requests:
              cpu: 10m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: google-cloud-key
          secret:
            defaultMode: 420
            secretName: gprod-addr-main-app
  status:
    conditions:
    - lastTransitionTime: "2020-03-21T23:43:19Z"
      lastUpdateTime: "2020-03-21T23:43:19Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2020-03-21T23:43:19Z"
      lastUpdateTime: "2020-03-21T23:43:19Z"
      message: ReplicaSet "book-app-7676cf7c64" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 3
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "6"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"book-web"},"name":"book-web","namespace":"book"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"book-web"}},"template":{"metadata":{"labels":{"app":"book-web"}},"spec":{"containers":[{"env":[{"name":"MY_ENV_VAR_1","value":"abcd1234"}],"envFrom":[{"configMapRef":{"name":"my-config-1"}}],"image":"nginx:1.16","name":"nginx","ports":[{"containerPort":4000,"name":"default","protocol":"TCP"}],"resources":{"limits":{"cpu":"100m"},"requests":{"cpu":"50m","memory":"100Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","securityContext":{},"terminationGracePeriodSeconds":30}}}}
    creationTimestamp: "2020-01-11T21:42:29Z"
    generation: 7
    labels:
      app: book-web
    name: book-web
    namespace: book
    resourceVersion: "29699485"
    selfLink: /apis/apps/v1/namespaces/book/deployments/book-web
    uid: 4430eec8-34bb-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 0
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: book-web
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: book-web
      spec:
        containers:
        - env:
          - name: MY_ENV_VAR_1
            value: abcd1234
          envFrom:
          - configMapRef:
              name: my-config-1
          image: nginx:1.16
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - containerPort: 4000
            name: default
            protocol: TCP
          resources:
            limits:
              cpu: 15m
            requests:
              cpu: 10m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    conditions:
    - lastTransitionTime: "2020-01-11T21:42:29Z"
      lastUpdateTime: "2020-03-02T20:34:30Z"
      message: ReplicaSet "book-web-b476b75d7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-03-15T12:44:59Z"
      lastUpdateTime: "2020-03-15T12:44:59Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 7
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: cainjector
      app.kubernetes.io/instance: gitlab
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      helm.sh/chart: cainjector-v0.10.1
    name: gitlab-cainjector
    namespace: gitlab
    resourceVersion: "37202423"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-cainjector
    uid: 995eb501-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: cainjector
        app.kubernetes.io/instance: gitlab
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: cainjector
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cainjector
          app.kubernetes.io/instance: gitlab
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          helm.sh/chart: cainjector-v0.10.1
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=$(POD_NAMESPACE)
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v0.10.1
          imagePullPolicy: IfNotPresent
          name: cainjector
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: gitlab-cainjector
        serviceAccountName: gitlab-cainjector
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:43:52Z"
      lastUpdateTime: "2020-01-18T00:44:04Z"
      message: ReplicaSet "gitlab-cainjector-5d757b9fdd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-10T23:54:05Z"
      lastUpdateTime: "2020-04-10T23:54:05Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: cert-manager
      app.kubernetes.io/instance: gitlab
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      helm.sh/chart: certmanager-v0.10.1
    name: gitlab-cert-manager
    namespace: gitlab
    resourceVersion: "37198418"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-cert-manager
    uid: 99614a55-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: cert-manager
        app.kubernetes.io/instance: gitlab
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: cert-manager
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/instance: gitlab
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          helm.sh/chart: certmanager-v0.10.1
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --webhook-namespace=$(POD_NAMESPACE)
          - --webhook-ca-secret=gitlab-cert-manager-webhook-ca
          - --webhook-serving-secret=gitlab-cert-manager-webhook-tls
          - --webhook-dns-names=gitlab-cert-manager-webhook,gitlab-cert-manager-webhook.gitlab,gitlab-cert-manager-webhook.gitlab.svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v0.10.1
          imagePullPolicy: IfNotPresent
          name: certmanager
          ports:
          - containerPort: 9402
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: gitlab-cert-manager
        serviceAccountName: gitlab-cert-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:43:52Z"
      lastUpdateTime: "2020-01-18T00:44:08Z"
      message: ReplicaSet "gitlab-cert-manager-5ffcc7f99f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-10T23:36:23Z"
      lastUpdateTime: "2020-04-10T23:36:23Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: webhook
      app.kubernetes.io/instance: gitlab
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      helm.sh/chart: certmanager-v0.10.1
    name: gitlab-cert-manager-webhook
    namespace: gitlab
    resourceVersion: "33571690"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-cert-manager-webhook
    uid: 99637e48-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: webhook
        app.kubernetes.io/instance: gitlab
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: webhook
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: webhook
          app.kubernetes.io/instance: gitlab
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          helm.sh/chart: certmanager-v0.10.1
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=6443
          - --tls-cert-file=/certs/tls.crt
          - --tls-private-key-file=/certs/tls.key
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v0.10.1
          imagePullPolicy: IfNotPresent
          name: certmanager
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: certs
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: gitlab-cert-manager-webhook
        serviceAccountName: gitlab-cert-manager-webhook
        terminationGracePeriodSeconds: 30
        volumes:
        - name: certs
          secret:
            defaultMode: 420
            secretName: gitlab-cert-manager-webhook-tls
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:43:52Z"
      lastUpdateTime: "2020-01-18T00:44:21Z"
      message: ReplicaSet "gitlab-cert-manager-webhook-76d9d9cc69" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-01T13:49:56Z"
      lastUpdateTime: "2020-04-01T13:49:56Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: gitlab-exporter
      chart: gitlab-exporter-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-gitlab-exporter
    namespace: gitlab
    resourceVersion: "33303024"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-gitlab-exporter
    uid: 9963d015-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: gitlab-exporter
        release: gitlab
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 1aebe5270e7534dc7510737e5fcfb26b9b6ca0f289d95563a9cdddae80ad7e24
          prometheus.io/path: /metrics
          prometheus.io/port: "9168"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: gitlab-exporter
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: gitlab-exporter
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab-exporter/templates
          - name: CONFIG_DIRECTORY
            value: /etc/gitlab-exporter
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-exporter:5.1.0
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -f 'gitlab-exporter'
          livenessProbe:
            exec:
              command:
              - pgrep
              - -f
              - gitlab-exporter
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: gitlab-exporter
          ports:
          - containerPort: 9168
            name: gitlab-exporter
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - pgrep
              - -f
              - gitlab-exporter
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 50m
              memory: 100M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab-exporter/templates/gitlab-exporter.yml.erb
            name: gitlab-exporter-config
            subPath: gitlab-exporter.yml.erb
          - mountPath: /etc/gitlab
            name: gitlab-exporter-secrets
            readOnly: true
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: gitlab-exporter-config
            readOnly: true
          - mountPath: /init-config
            name: init-gitlab-exporter-secrets
            readOnly: true
          - mountPath: /init-secrets
            name: gitlab-exporter-secrets
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: gitlab-gitlab-exporter
          name: gitlab-exporter-config
        - name: init-gitlab-exporter-secrets
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: postgres-password
                  path: postgres/psql-password
                name: gitlab-postgresql-password
            - secret:
                items:
                - key: secret
                  path: redis/password
                name: gitlab-redis-secret
        - emptyDir:
            medium: Memory
          name: gitlab-exporter-secrets
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T01:01:19Z"
      lastUpdateTime: "2020-01-18T01:01:19Z"
      message: ReplicaSet "gitlab-gitlab-exporter-868bc56dd8" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-03-31T18:43:13Z"
      lastUpdateTime: "2020-03-31T18:43:13Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: gitlab-gitlab-runner
      chart: gitlab-runner-0.12.0
      heritage: Helm
      release: gitlab
    name: gitlab-gitlab-runner
    namespace: gitlab
    resourceVersion: "37201639"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-gitlab-runner
    uid: 9960a25d-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: gitlab-gitlab-runner
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/configmap: b6037b659634c6ed64ec4ba6a1ff81db0af76d05c0b3be594c981a6274cb551d
          checksum/secrets: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          prometheus.io/port: "9252"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: gitlab-gitlab-runner
          chart: gitlab-runner-0.12.0
          heritage: Helm
          release: gitlab
      spec:
        containers:
        - command:
          - /bin/bash
          - /scripts/entrypoint
          env:
          - name: CI_SERVER_URL
            value: https://gitlab.example.com
          - name: CLONE_URL
          - name: RUNNER_REQUEST_CONCURRENCY
            value: "1"
          - name: RUNNER_EXECUTOR
            value: kubernetes
          - name: REGISTER_LOCKED
            value: "false"
          - name: RUNNER_TAG_LIST
          - name: RUNNER_OUTPUT_LIMIT
            value: "4096"
          - name: KUBERNETES_IMAGE
            value: ubuntu:16.04
          - name: KUBERNETES_NAMESPACE
            value: gitlab
          - name: KUBERNETES_POLL_TIMEOUT
            value: "180"
          - name: KUBERNETES_CPU_LIMIT
          - name: KUBERNETES_MEMORY_LIMIT
          - name: KUBERNETES_CPU_REQUEST
          - name: KUBERNETES_MEMORY_REQUEST
          - name: KUBERNETES_SERVICE_ACCOUNT
          - name: KUBERNETES_SERVICE_CPU_LIMIT
          - name: KUBERNETES_SERVICE_MEMORY_LIMIT
          - name: KUBERNETES_SERVICE_CPU_REQUEST
          - name: KUBERNETES_SERVICE_MEMORY_REQUEST
          - name: KUBERNETES_HELPER_CPU_LIMIT
          - name: KUBERNETES_HELPER_MEMORY_LIMIT
          - name: KUBERNETES_HELPER_CPU_REQUEST
          - name: KUBERNETES_HELPER_MEMORY_REQUEST
          - name: KUBERNETES_HELPER_IMAGE
          - name: KUBERNETES_PULL_POLICY
          - name: CACHE_TYPE
            value: s3
          - name: CACHE_PATH
            value: gitlab-runner
          - name: CACHE_SHARED
            value: "true"
          - name: CACHE_S3_SERVER_ADDRESS
            value: minio.example.com
          - name: CACHE_S3_BUCKET_NAME
            value: runner-cache
          - name: CACHE_S3_BUCKET_LOCATION
            value: us-east-1
          image: gitlab/gitlab-runner:alpine-v12.6.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/bash
              - /scripts/check-live
            failureThreshold: 3
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: gitlab-gitlab-runner
          ports:
          - containerPort: 9252
            name: metrics
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /usr/bin/pgrep
              - gitlab.*runner
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /secrets
            name: runner-secrets
          - mountPath: /home/gitlab-runner/.gitlab-runner
            name: etc-gitlab-runner
          - mountPath: /scripts
            name: scripts
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - /config/configure
          env:
          - name: CI_SERVER_URL
            value: https://gitlab.example.com
          - name: CLONE_URL
          - name: RUNNER_REQUEST_CONCURRENCY
            value: "1"
          - name: RUNNER_EXECUTOR
            value: kubernetes
          - name: REGISTER_LOCKED
            value: "false"
          - name: RUNNER_TAG_LIST
          - name: RUNNER_OUTPUT_LIMIT
            value: "4096"
          - name: KUBERNETES_IMAGE
            value: ubuntu:16.04
          - name: KUBERNETES_NAMESPACE
            value: gitlab
          - name: KUBERNETES_POLL_TIMEOUT
            value: "180"
          - name: KUBERNETES_CPU_LIMIT
          - name: KUBERNETES_MEMORY_LIMIT
          - name: KUBERNETES_CPU_REQUEST
          - name: KUBERNETES_MEMORY_REQUEST
          - name: KUBERNETES_SERVICE_ACCOUNT
          - name: KUBERNETES_SERVICE_CPU_LIMIT
          - name: KUBERNETES_SERVICE_MEMORY_LIMIT
          - name: KUBERNETES_SERVICE_CPU_REQUEST
          - name: KUBERNETES_SERVICE_MEMORY_REQUEST
          - name: KUBERNETES_HELPER_CPU_LIMIT
          - name: KUBERNETES_HELPER_MEMORY_LIMIT
          - name: KUBERNETES_HELPER_CPU_REQUEST
          - name: KUBERNETES_HELPER_MEMORY_REQUEST
          - name: KUBERNETES_HELPER_IMAGE
          - name: KUBERNETES_PULL_POLICY
          - name: CACHE_TYPE
            value: s3
          - name: CACHE_PATH
            value: gitlab-runner
          - name: CACHE_SHARED
            value: "true"
          - name: CACHE_S3_SERVER_ADDRESS
            value: minio.example.com
          - name: CACHE_S3_BUCKET_NAME
            value: runner-cache
          - name: CACHE_S3_BUCKET_LOCATION
            value: us-east-1
          image: gitlab/gitlab-runner:alpine-v12.6.0
          imagePullPolicy: IfNotPresent
          name: configure
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /secrets
            name: runner-secrets
          - mountPath: /config
            name: scripts
            readOnly: true
          - mountPath: /init-secrets
            name: init-runner-secrets
            readOnly: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65533
          runAsUser: 100
        serviceAccount: gitlab-gitlab-runner
        serviceAccountName: gitlab-gitlab-runner
        terminationGracePeriodSeconds: 3600
        volumes:
        - emptyDir:
            medium: Memory
          name: runner-secrets
        - emptyDir:
            medium: Memory
          name: etc-gitlab-runner
        - name: init-runner-secrets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: gitlab-minio-secret
            - secret:
                items:
                - key: runner-registration-token
                  path: runner-registration-token
                - key: runner-token
                  path: runner-token
                name: gitlab-gitlab-runner-secret
        - configMap:
            defaultMode: 420
            name: gitlab-gitlab-runner
          name: scripts
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:43:52Z"
      lastUpdateTime: "2020-01-18T00:45:09Z"
      message: ReplicaSet "gitlab-gitlab-runner-65ff67bdd8" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-10T23:50:32Z"
      lastUpdateTime: "2020-04-10T23:50:32Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 85
    labels:
      app: gitlab-shell
      chart: gitlab-shell-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-gitlab-shell
    namespace: gitlab
    resourceVersion: "33574634"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-gitlab-shell
    uid: 99655ee5-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: gitlab-shell
        release: gitlab
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: ba8fbd9408a25a13f7c9d3682c4bed3a4e3645216c471e54e71035dd3c0d5bad
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: gitlab-shell
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: gitlab-shell
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /etc/gitlab-shell
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab-shell
          - name: KEYS_DIRECTORY
            value: /etc/gitlab-secrets/ssh
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-shell:v10.3.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /scripts/healthcheck
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: gitlab-shell
          ports:
          - containerPort: 2222
            name: ssh
            protocol: TCP
          resources:
            requests:
              cpu: "0"
              memory: 6M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/gitlab-shell
            name: shell-config
          - mountPath: /etc/gitlab-secrets
            name: shell-secrets
            readOnly: true
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: shell-config
            readOnly: true
          - mountPath: /init-config
            name: shell-init-secrets
            readOnly: true
          - mountPath: /init-secrets
            name: shell-secrets
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: gitlab-gitlab-shell
          name: shell-config
        - name: shell-init-secrets
          projected:
            defaultMode: 288
            sources:
            - secret:
                name: gitlab-gitlab-shell-host-keys
            - secret:
                items:
                - key: secret
                  path: shell/.gitlab_shell_secret
                name: gitlab-gitlab-shell-secret
            - secret:
                items:
                - key: secret
                  path: redis/password
                name: gitlab-redis-secret
        - emptyDir:
            medium: Memory
          name: shell-secrets
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:43:52Z"
      lastUpdateTime: "2020-01-18T01:01:19Z"
      message: ReplicaSet "gitlab-gitlab-shell-7b89877f4c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-01T13:51:27Z"
      lastUpdateTime: "2020-04-01T13:51:27Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 85
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: minio
      chart: minio-0.4.3
      heritage: Helm
      release: gitlab
    name: gitlab-minio
    namespace: gitlab
    resourceVersion: "33571993"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-minio
    uid: 99615b81-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: minio
        component: app
        release: gitlab
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: minio
          chart: minio-0.4.3
          component: app
          heritage: Helm
          release: gitlab
        name: gitlab-minio
      spec:
        containers:
        - args:
          - -C
          - /tmp/.minio
          - --quiet
          - server
          - /export
          image: minio/minio:RELEASE.2017-12-28T01-21-00Z
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 9000
            timeoutSeconds: 1
          name: minio
          ports:
          - containerPort: 9000
            name: service
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /export
            name: export
          - mountPath: /tmp/.minio
            name: minio-server-config
          - mountPath: /podinfo
            name: podinfo
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: IfNotPresent
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: minio-configuration
          - mountPath: /minio
            name: minio-server-config
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - downwardAPI:
            defaultMode: 420
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.labels
              path: labels
          name: podinfo
        - name: export
          persistentVolumeClaim:
            claimName: gitlab-minio
        - name: minio-configuration
          projected:
            defaultMode: 420
            sources:
            - configMap:
                name: gitlab-minio-config-cm
            - secret:
                name: gitlab-minio-secret
        - emptyDir:
            medium: Memory
          name: minio-server-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:43:52Z"
      lastUpdateTime: "2020-01-18T00:44:34Z"
      message: ReplicaSet "gitlab-minio-79db4985c4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-01T13:50:31Z"
      lastUpdateTime: "2020-04-01T13:50:31Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 3
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.30.0-1
      component: controller
      heritage: Helm
      release: gitlab
    name: gitlab-nginx-ingress-controller
    namespace: gitlab
    resourceVersion: "37062133"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-nginx-ingress-controller
    uid: 995e41f0-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 3
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: nginx-ingress
        component: controller
        release: gitlab
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: controller
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: nginx-ingress
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - /nginx-ingress-controller
          - --default-backend-service=gitlab/gitlab-nginx-ingress-default-backend
          - --publish-service=gitlab/gitlab-nginx-ingress-controller
          - --election-id=ingress-controller-leader
          - --ingress-class=gitlab-nginx
          - --configmap=gitlab/gitlab-nginx-ingress-controller
          - --tcp-services-configmap=gitlab/gitlab-nginx-ingress-tcp
          - --watch-namespace=gitlab
          - --force-namespace-isolation
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: nginx-ingress-controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          - containerPort: 18080
            name: stats
            protocol: TCP
          - containerPort: 10254
            name: metrics
            protocol: TCP
          - containerPort: 22
            name: gitlab-shell
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 15m
              memory: 15Mi
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            runAsUser: 33
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: gitlab-nginx-ingress
        serviceAccountName: gitlab-nginx-ingress
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 3
    conditions:
    - lastTransitionTime: "2020-01-18T00:43:52Z"
      lastUpdateTime: "2020-02-04T05:23:08Z"
      message: ReplicaSet "gitlab-nginx-ingress-controller-7db754f856" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-10T14:23:06Z"
      lastUpdateTime: "2020-04-10T14:23:06Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 3
    replicas: 3
    updatedReplicas: 3
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: nginx-ingress
      chart: nginx-ingress-0.30.0-1
      component: default-backend
      heritage: Helm
      release: gitlab
    name: gitlab-nginx-ingress-default-backend
    namespace: gitlab
    resourceVersion: "33571816"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-nginx-ingress-default-backend
    uid: 99612109-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: nginx-ingress
        component: default-backend
        release: gitlab
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: default-backend
          release: gitlab
      spec:
        containers:
        - image: k8s.gcr.io/defaultbackend:1.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: nginx-ingress-default-backend
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 5m
              memory: 5Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2020-01-18T00:43:52Z"
      lastUpdateTime: "2020-01-18T00:44:04Z"
      message: ReplicaSet "gitlab-nginx-ingress-default-backend-7f87d67c8" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-01T13:50:13Z"
      lastUpdateTime: "2020-04-01T13:50:13Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 3
    labels:
      app: postgresql
      chart: postgresql-0.12.0
      heritage: Helm
      release: gitlab
    name: gitlab-postgresql
    namespace: gitlab
    resourceVersion: "33571958"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-postgresql
    uid: 99622113-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        app: postgresql
        release: gitlab
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: postgresql
          release: gitlab
      spec:
        containers:
        - env:
          - name: POSTGRES_USER
            value: gitlab
          - name: PGUSER
            value: gitlab
          - name: POSTGRES_DB
            value: gitlabhq_production
          - name: POSTGRES_INITDB_ARGS
          - name: PGDATA
            value: /var/lib/postgresql/data/pgdata
          - name: POSTGRES_PASSWORD_FILE
            value: /conf/postgres-password
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: postgres:9.6.8
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - exec pg_isready --host $POD_IP
            failureThreshold: 6
            initialDelaySeconds: 120
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: gitlab-postgresql
          ports:
          - containerPort: 5432
            name: postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - sh
              - -c
              - exec pg_isready --host $POD_IP
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              cpu: 10m
              memory: 25Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/postgresql/data/pgdata
            name: data
            subPath: postgresql-db
          - mountPath: /conf
            name: password-file
            readOnly: true
        - env:
          - name: DATA_SOURCE_NAME
            value: postgresql://gitlab@127.0.0.1:5432?sslmode=disable
          image: wrouesnel/postgres_exporter:v0.1.1
          imagePullPolicy: IfNotPresent
          name: metrics
          ports:
          - containerPort: 9187
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 12m
              memory: 25Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: gitlab-postgresql
        - name: password-file
          secret:
            defaultMode: 420
            items:
            - key: postgres-password
              path: postgres-password
            secretName: gitlab-postgresql-password
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:43:54Z"
      lastUpdateTime: "2020-01-18T00:43:54Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: prometheus
      chart: prometheus-9.0.0
      component: server
      heritage: Helm
      release: gitlab
    name: gitlab-prometheus-server
    namespace: gitlab
    resourceVersion: "37200147"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-prometheus-server
    uid: 9976d02c-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        app: prometheus
        component: server
        release: gitlab
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: prometheus
          chart: prometheus-9.0.0
          component: server
          heritage: Helm
          release: gitlab
      spec:
        containers:
        - args:
          - --volume-dir=/etc/config
          - --webhook-url=http://127.0.0.1:9090/-/reload
          image: jimmidyson/configmap-reload:v0.2.2
          imagePullPolicy: IfNotPresent
          name: prometheus-server-configmap-reload
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: prom/prometheus:v2.11.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: gitlab-prometheus-server
        serviceAccountName: gitlab-prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: gitlab-prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: gitlab-prometheus-server
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:43:54Z"
      lastUpdateTime: "2020-01-18T00:43:54Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 4
    labels:
      app: redis
      chart: redis-ha-0.1.0
      heritage: Helm
      release: gitlab
    name: gitlab-redis-sentinel
    namespace: gitlab
    resourceVersion: "33571943"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-redis-sentinel
    uid: 9960bf23-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: redis
        chart: redis-ha-0.1.0
        heritage: Helm
        name: redis-sentinel
        release: gitlab
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: redis
          chart: redis-ha-0.1.0
          heritage: Helm
          name: redis-sentinel
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - redis
                  - key: release
                    operator: In
                    values:
                    - gitlab
                  - key: redis-role
                    operator: In
                    values:
                    - sentinel
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: SENTINEL
            value: "true"
          - name: REDIS_CHART_PREFIX
            value: gitlab-redis
          - name: REDIS_PASSWORD_FILE
            value: /config/password
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-redis-ha:732704f18e34ba469df34b10c3b2465e0469d484
          imagePullPolicy: Always
          name: sentinel
          ports:
          - containerPort: 26379
            protocol: TCP
          resources:
            limits:
              memory: 30Mi
            requests:
              cpu: 13m
              memory: 30Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config/
            name: gitlab-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: gitlab-redis
        serviceAccountName: gitlab-redis
        terminationGracePeriodSeconds: 30
        volumes:
        - name: gitlab-config
          projected:
            defaultMode: 420
            sources:
            - secret:
                items:
                - key: secret
                  path: password
                name: gitlab-redis-secret
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2020-01-18T00:43:52Z"
      lastUpdateTime: "2020-02-04T05:20:51Z"
      message: ReplicaSet "gitlab-redis-sentinel-69879b577" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-01T13:50:24Z"
      lastUpdateTime: "2020-04-01T13:50:24Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 4
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 68
    labels:
      app: registry
      chart: registry-0.3.0
      heritage: Helm
      release: gitlab
    name: gitlab-registry
    namespace: gitlab
    resourceVersion: "37196458"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-registry
    uid: 99636a67-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: registry
        release: gitlab
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/configmap: 286c8cfd6e68b01cac791db6c7970b093f059b17c007ea126964d5c8f5cc9288
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: registry
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: registry
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - image: registry:2.7.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /debug/health
              port: 5001
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: registry
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /debug/health
              port: 5001
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 50m
              memory: 32Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/docker/registry/
            name: registry-server-config
            readOnly: true
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: IfNotPresent
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: registry-secrets
          - mountPath: /registry
            name: registry-server-config
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: registry-server-config
        - name: registry-secrets
          projected:
            defaultMode: 420
            sources:
            - configMap:
                name: gitlab-registry
            - secret:
                items:
                - key: registry-auth.crt
                  path: certificate.crt
                name: gitlab-registry-secret
            - secret:
                items:
                - key: secret
                  path: httpSecret
                name: gitlab-registry-httpsecret
            - secret:
                name: gitlab-minio-secret
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:43:52Z"
      lastUpdateTime: "2020-01-18T00:53:48Z"
      message: ReplicaSet "gitlab-registry-74c959fc8" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-10T23:22:00Z"
      lastUpdateTime: "2020-04-10T23:22:00Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 68
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 52
    labels:
      app: sidekiq
      chart: sidekiq-2.6.5
      heritage: Helm
      queue-pod-name: all-in-1
      release: gitlab
    name: gitlab-sidekiq-all-in-1
    namespace: gitlab
    resourceVersion: "33574603"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-sidekiq-all-in-1
    uid: 9962f9f2-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: sidekiq
        release: gitlab
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/configmap: 3a9f53018b334d9fcb2222de4dabc983f5b6eed7c75910d315de61d1d88a0d91
          checksum/configmap-pod: 1c9250b5fb48aeff1c2d2554f323f88f9db01c254bdaef83cf2dc26ec784ebea
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
          prometheus.io/port: "3807"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: sidekiq
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: sidekiq
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: prometheus_multiproc_dir
            value: /metrics
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          - name: SIDEKIQ_CONCURRENCY
            value: "25"
          - name: SIDEKIQ_TIMEOUT
            value: "5"
          - name: SIDEKIQ_DAEMON_MEMORY_KILLER
            value: "0"
          - name: SIDEKIQ_MEMORY_KILLER_CHECK_INTERVAL
            value: "3"
          - name: SIDEKIQ_MEMORY_KILLER_MAX_RSS
            value: "2000000"
          - name: SIDEKIQ_MEMORY_KILLER_GRACE_TIME
            value: "900"
          - name: SIDEKIQ_MEMORY_KILLER_SHUTDOWN_WAIT
            value: "30"
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-sidekiq-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -f 'sidekiq'
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 3807
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: sidekiq
          ports:
          - containerPort: 3807
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 3807
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            requests:
              cpu: 10m
              memory: 50M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /metrics
            name: sidekiq-metrics
          - mountPath: /var/opt/gitlab/templates
            name: sidekiq-config
            readOnly: true
          - mountPath: /etc/gitlab
            name: sidekiq-secrets
            readOnly: true
          - mountPath: /srv/gitlab/config/secrets.yml
            name: sidekiq-secrets
            subPath: rails-secrets/secrets.yml
          - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
            name: sidekiq-config
            subPath: smtp_settings.rb
          - mountPath: /srv/gitlab/INSTALLATION_TYPE
            name: sidekiq-config
            subPath: installation_type
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: sidekiq-config
            readOnly: true
          - mountPath: /init-config
            name: init-sidekiq-secrets
            readOnly: true
          - mountPath: /init-secrets
            name: sidekiq-secrets
        - args:
          - /scripts/wait-for-deps
          env:
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          - name: SIDEKIQ_CONCURRENCY
            value: "25"
          - name: SIDEKIQ_TIMEOUT
            value: "5"
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-sidekiq-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          name: dependencies
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab/templates
            name: sidekiq-config
            readOnly: true
          - mountPath: /etc/gitlab
            name: sidekiq-secrets
            readOnly: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: sidekiq-metrics
        - name: sidekiq-config
          projected:
            defaultMode: 420
            sources:
            - configMap:
                name: gitlab-sidekiq
            - configMap:
                name: gitlab-sidekiq-all-in-1
        - name: init-sidekiq-secrets
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: secrets.yml
                  path: rails-secrets/secrets.yml
                name: gitlab-rails-secret
            - secret:
                items:
                - key: token
                  path: gitaly/gitaly_token
                name: gitlab-gitaly-secret
            - secret:
                items:
                - key: secret
                  path: redis/password
                name: gitlab-redis-secret
            - secret:
                items:
                - key: postgres-password
                  path: postgres/psql-password
                name: gitlab-postgresql-password
            - secret:
                items:
                - key: registry-auth.key
                  path: registry/gitlab-registry.key
                name: gitlab-registry-secret
            - secret:
                items:
                - key: accesskey
                  path: minio/accesskey
                - key: secretkey
                  path: minio/secretkey
                name: gitlab-minio-secret
        - emptyDir:
            medium: Memory
          name: sidekiq-secrets
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-02-04T14:11:00Z"
      lastUpdateTime: "2020-02-04T14:11:00Z"
      message: ReplicaSet "gitlab-sidekiq-all-in-1-649889b7c6" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-01T13:55:47Z"
      lastUpdateTime: "2020-04-01T13:55:47Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 52
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: task-runner
      chart: task-runner-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-task-runner
    namespace: gitlab
    resourceVersion: "33302860"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-task-runner
    uid: 9965278e-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: task-runner
        release: gitlab
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          checksum/config: f51dac24c133254fe6c63906a579c8d3d58b44036d8737eb8744b99cef3feb8a
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: task-runner
          release: gitlab
      spec:
        containers:
        - args:
          - /bin/bash
          - -c
          - cp -v -r -L /etc/gitlab/.s3cfg $HOME/.s3cfg && while sleep 3600; do :;
            done
          env:
          - name: ARTIFACTS_BUCKET_NAME
            value: gitlab-artifacts
          - name: REGISTRY_BUCKET_NAME
            value: registry
          - name: LFS_BUCKET_NAME
            value: git-lfs
          - name: UPLOADS_BUCKET_NAME
            value: gitlab-uploads
          - name: PACKAGES_BUCKET_NAME
            value: gitlab-packages
          - name: BACKUP_BUCKET_NAME
            value: gitlab-backups
          - name: BACKUP_BACKEND
            value: s3
          - name: TMP_BUCKET_NAME
            value: tmp
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: ENABLE_BOOTSNAP
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-task-runner-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          name: task-runner
          resources:
            requests:
              cpu: 50m
              memory: 350M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab/templates
            name: task-runner-config
          - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
            name: task-runner-config
            subPath: smtp_settings.rb
          - mountPath: /etc/gitlab
            name: task-runner-secrets
            readOnly: true
          - mountPath: /srv/gitlab/config/secrets.yml
            name: task-runner-secrets
            subPath: rails-secrets/secrets.yml
          - mountPath: /srv/gitlab/tmp
            name: task-runner-tmp
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: task-runner-config
            readOnly: true
          - mountPath: /init-config
            name: init-task-runner-secrets
            readOnly: true
          - mountPath: /init-secrets
            name: task-runner-secrets
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - name: task-runner-config
          projected:
            defaultMode: 420
            sources:
            - configMap:
                name: gitlab-task-runner
        - emptyDir: {}
          name: task-runner-tmp
        - name: init-task-runner-secrets
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: secrets.yml
                  path: rails-secrets/secrets.yml
                name: gitlab-rails-secret
            - secret:
                items:
                - key: secret
                  path: shell/.gitlab_shell_secret
                name: gitlab-gitlab-shell-secret
            - secret:
                items:
                - key: token
                  path: gitaly/gitaly_token
                name: gitlab-gitaly-secret
            - secret:
                items:
                - key: secret
                  path: redis/password
                name: gitlab-redis-secret
            - secret:
                items:
                - key: postgres-password
                  path: postgres/psql-password
                name: gitlab-postgresql-password
            - secret:
                items:
                - key: registry-auth.key
                  path: registry/gitlab-registry.key
                name: gitlab-registry-secret
            - secret:
                items:
                - key: accesskey
                  path: minio/accesskey
                - key: secretkey
                  path: minio/secretkey
                name: gitlab-minio-secret
        - emptyDir:
            medium: Memory
          name: task-runner-secrets
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:43:52Z"
      lastUpdateTime: "2020-01-18T00:52:49Z"
      message: ReplicaSet "gitlab-task-runner-9f9cf668f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-03-31T18:42:55Z"
      lastUpdateTime: "2020-03-31T18:42:55Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 45
    labels:
      app: unicorn
      chart: unicorn-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-unicorn
    namespace: gitlab
    resourceVersion: "36422532"
    selfLink: /apis/apps/v1/namespaces/gitlab/deployments/gitlab-unicorn
    uid: 9968e6a8-398b-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: unicorn
        release: gitlab
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: d021e46742b5d6dd76265a6b1dd9c78cdeb6db74842f2cdaa710f411a97fbabb
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
          prometheus.io/path: /-/metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: unicorn
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: unicorn
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: TMPDIR
            value: /tmp/gitlab
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          - name: prometheus_multiproc_dir
            value: /metrics
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -SIGQUIT -f 'unicorn master'
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/liveness
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: unicorn
          ports:
          - containerPort: 8080
            name: unicorn
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/readiness
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            requests:
              cpu: 10m
              memory: 40M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /metrics
            name: unicorn-metrics
          - mountPath: /var/opt/gitlab/templates
            name: unicorn-config
          - mountPath: /etc/gitlab
            name: unicorn-secrets
            readOnly: true
          - mountPath: /srv/gitlab/config/secrets.yml
            name: unicorn-secrets
            subPath: rails-secrets/secrets.yml
          - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
            name: unicorn-config
            subPath: smtp_settings.rb
          - mountPath: /srv/gitlab/INSTALLATION_TYPE
            name: unicorn-config
            subPath: installation_type
          - mountPath: /srv/gitlab/public/uploads/tmp
            name: shared-upload-directory
          - mountPath: /srv/gitlab/shared
            name: shared-artifact-directory
          - mountPath: /tmp
            name: shared-tmp
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        - env:
          - name: TMPDIR
            value: /tmp/gitlab
          - name: GITLAB_WORKHORSE_EXTRA_ARGS
          - name: GITLAB_WORKHORSE_LISTEN_PORT
            value: "8181"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /scripts/healthcheck
            failureThreshold: 3
            initialDelaySeconds: 20
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: gitlab-workhorse
          ports:
          - containerPort: 8181
            name: workhorse
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /scripts/healthcheck
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            requests:
              cpu: 10m
              memory: 40M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab/templates
            name: workhorse-config
          - mountPath: /etc/gitlab
            name: workhorse-secrets
            readOnly: true
          - mountPath: /srv/gitlab/public/uploads/tmp
            name: shared-upload-directory
          - mountPath: /srv/gitlab/shared
            name: shared-artifact-directory
          - mountPath: /tmp
            name: shared-tmp
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - args:
          - -c
          - sh -x /config-unicorn/configure ; sh -x /config-workhorse/configure ;
            mkdir -p -m 3770 /tmp/gitlab
          command:
          - sh
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config-unicorn
            name: unicorn-config
            readOnly: true
          - mountPath: /config-workhorse
            name: workhorse-config
            readOnly: true
          - mountPath: /init-config
            name: init-unicorn-secrets
            readOnly: true
          - mountPath: /init-secrets
            name: unicorn-secrets
          - mountPath: /init-secrets-workhorse
            name: workhorse-secrets
          - mountPath: /tmp
            name: shared-tmp
        - args:
          - /scripts/wait-for-deps
          env:
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          - name: WORKHORSE_ARCHIVE_CACHE_DISABLED
            value: "1"
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          name: dependencies
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab/templates
            name: unicorn-config
          - mountPath: /etc/gitlab
            name: unicorn-secrets
            readOnly: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: shared-tmp
        - emptyDir:
            medium: Memory
          name: unicorn-metrics
        - configMap:
            defaultMode: 420
            name: gitlab-unicorn
          name: unicorn-config
        - configMap:
            defaultMode: 420
            name: gitlab-workhorse-config
          name: workhorse-config
        - name: init-unicorn-secrets
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: secrets.yml
                  path: rails-secrets/secrets.yml
                name: gitlab-rails-secret
            - secret:
                items:
                - key: secret
                  path: shell/.gitlab_shell_secret
                name: gitlab-gitlab-shell-secret
            - secret:
                items:
                - key: token
                  path: gitaly/gitaly_token
                name: gitlab-gitaly-secret
            - secret:
                items:
                - key: secret
                  path: redis/password
                name: gitlab-redis-secret
            - secret:
                items:
                - key: postgres-password
                  path: postgres/psql-password
                name: gitlab-postgresql-password
            - secret:
                items:
                - key: registry-auth.key
                  path: registry/gitlab-registry.key
                name: gitlab-registry-secret
            - secret:
                items:
                - key: shared_secret
                  path: gitlab-workhorse/secret
                name: gitlab-gitlab-workhorse-secret
            - secret:
                items:
                - key: accesskey
                  path: minio/accesskey
                - key: secretkey
                  path: minio/secretkey
                name: gitlab-minio-secret
        - emptyDir:
            medium: Memory
          name: unicorn-secrets
        - emptyDir:
            medium: Memory
          name: workhorse-secrets
        - emptyDir: {}
          name: shared-upload-directory
        - emptyDir: {}
          name: shared-artifact-directory
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2020-02-04T14:27:49Z"
      lastUpdateTime: "2020-02-04T14:33:21Z"
      message: ReplicaSet "gitlab-unicorn-84d7dc6557" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-08T21:56:58Z"
      lastUpdateTime: "2020-04-08T21:56:58Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 45
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"adservice","namespace":"hipster"},"spec":{"selector":{"matchLabels":{"app":"adservice"}},"template":{"metadata":{"labels":{"app":"adservice"}},"spec":{"containers":[{"env":[{"name":"PORT","value":"9555"}],"image":"gcr.io/google-samples/microservices-demo/adservice:v0.1.3","livenessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:9555"]},"initialDelaySeconds":20,"periodSeconds":15},"name":"server","ports":[{"containerPort":9555}],"readinessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:9555"]},"initialDelaySeconds":20,"periodSeconds":15},"resources":{"limits":{"cpu":"100m","memory":"300Mi"},"requests":{"cpu":"50m","memory":"180Mi"}}}],"terminationGracePeriodSeconds":5}}}}
    creationTimestamp: "2020-01-11T07:11:51Z"
    generation: 3
    name: adservice
    namespace: hipster
    resourceVersion: "36841060"
    selfLink: /apis/apps/v1/namespaces/hipster/deployments/adservice
    uid: a424ede3-3441-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: adservice
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: adservice
      spec:
        containers:
        - env:
          - name: PORT
            value: "9555"
          image: gcr.io/google-samples/microservices-demo/adservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:9555
            failureThreshold: 3
            initialDelaySeconds: 20
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 9555
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:9555
            failureThreshold: 3
            initialDelaySeconds: 20
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 50m
              memory: 180Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    conditions:
    - lastTransitionTime: "2020-02-01T06:01:42Z"
      lastUpdateTime: "2020-02-01T06:01:42Z"
      message: ReplicaSet "adservice-84b8749d65" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-10T00:10:28Z"
      lastUpdateTime: "2020-04-10T00:10:28Z"
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: "False"
      type: Available
    observedGeneration: 3
    replicas: 1
    unavailableReplicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "4"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"cartservice","namespace":"hipster"},"spec":{"selector":{"matchLabels":{"app":"cartservice"}},"template":{"metadata":{"labels":{"app":"cartservice"}},"spec":{"containers":[{"env":[{"name":"REDIS_ADDR","value":"redis-cart:6379"},{"name":"PORT","value":"7070"},{"name":"LISTEN_ADDR","value":"0.0.0.0"}],"image":"gcr.io/google-samples/microservices-demo/cartservice:v0.1.3","livenessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:7070","-rpc-timeout=5s"]},"initialDelaySeconds":15,"periodSeconds":10},"name":"server","ports":[{"containerPort":7070}],"readinessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:7070","-rpc-timeout=5s"]},"initialDelaySeconds":15},"resources":{"limits":{"cpu":"100m","memory":"128Mi"},"requests":{"cpu":"50m","memory":"64Mi"}}}],"terminationGracePeriodSeconds":5}}}}
    creationTimestamp: "2020-01-11T07:11:49Z"
    generation: 4
    name: cartservice
    namespace: hipster
    resourceVersion: "36841169"
    selfLink: /apis/apps/v1/namespaces/hipster/deployments/cartservice
    uid: a29ac8cf-3441-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: cartservice
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cartservice
      spec:
        containers:
        - env:
          - name: REDIS_ADDR
            value: redis-cart:6379
          - name: PORT
            value: "7070"
          - name: LISTEN_ADDR
            value: 0.0.0.0
          image: gcr.io/google-samples/microservices-demo/cartservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7070
              - -rpc-timeout=5s
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 7070
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7070
              - -rpc-timeout=5s
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 25m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-02-02T23:56:47Z"
      lastUpdateTime: "2020-02-02T23:59:56Z"
      message: ReplicaSet "cartservice-55f8ccc8f4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-10T00:10:50Z"
      lastUpdateTime: "2020-04-10T00:10:50Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 4
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"checkoutservice","namespace":"hipster"},"spec":{"selector":{"matchLabels":{"app":"checkoutservice"}},"template":{"metadata":{"labels":{"app":"checkoutservice"}},"spec":{"containers":[{"env":[{"name":"PORT","value":"5050"},{"name":"PRODUCT_CATALOG_SERVICE_ADDR","value":"productcatalogservice:3550"},{"name":"SHIPPING_SERVICE_ADDR","value":"shippingservice:50051"},{"name":"PAYMENT_SERVICE_ADDR","value":"paymentservice:50051"},{"name":"EMAIL_SERVICE_ADDR","value":"emailservice:5000"},{"name":"CURRENCY_SERVICE_ADDR","value":"currencyservice:7000"},{"name":"CART_SERVICE_ADDR","value":"cartservice:7070"}],"image":"gcr.io/google-samples/microservices-demo/checkoutservice:v0.1.3","livenessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:5050"]}},"name":"server","ports":[{"containerPort":5050}],"readinessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:5050"]}},"resources":{"limits":{"cpu":"20m","memory":"128Mi"},"requests":{"cpu":"5m","memory":"64Mi"}}}]}}}}
    creationTimestamp: "2020-01-11T07:11:46Z"
    generation: 2
    name: checkoutservice
    namespace: hipster
    resourceVersion: "33302785"
    selfLink: /apis/apps/v1/namespaces/hipster/deployments/checkoutservice
    uid: a0b32397-3441-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: checkoutservice
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: checkoutservice
      spec:
        containers:
        - env:
          - name: PORT
            value: "5050"
          - name: PRODUCT_CATALOG_SERVICE_ADDR
            value: productcatalogservice:3550
          - name: SHIPPING_SERVICE_ADDR
            value: shippingservice:50051
          - name: PAYMENT_SERVICE_ADDR
            value: paymentservice:50051
          - name: EMAIL_SERVICE_ADDR
            value: emailservice:5000
          - name: CURRENCY_SERVICE_ADDR
            value: currencyservice:7000
          - name: CART_SERVICE_ADDR
            value: cartservice:7070
          image: gcr.io/google-samples/microservices-demo/checkoutservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:5050
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 5050
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:5050
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 20m
              memory: 128Mi
            requests:
              cpu: 5m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-11T07:11:46Z"
      lastUpdateTime: "2020-01-30T04:44:25Z"
      message: ReplicaSet "checkoutservice-77d8f889b7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-03-31T18:42:48Z"
      lastUpdateTime: "2020-03-31T18:42:48Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"currencyservice","namespace":"hipster"},"spec":{"selector":{"matchLabels":{"app":"currencyservice"}},"template":{"metadata":{"labels":{"app":"currencyservice"}},"spec":{"containers":[{"env":[{"name":"PORT","value":"7000"}],"image":"gcr.io/google-samples/microservices-demo/currencyservice:v0.1.3","livenessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:7000"]}},"name":"server","ports":[{"containerPort":7000,"name":"grpc"}],"readinessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:7000"]}},"resources":{"limits":{"cpu":"100m","memory":"128Mi"},"requests":{"cpu":"50m","memory":"64Mi"}}}],"terminationGracePeriodSeconds":5}}}}
    creationTimestamp: "2020-01-11T07:11:50Z"
    generation: 3
    name: currencyservice
    namespace: hipster
    resourceVersion: "36841073"
    selfLink: /apis/apps/v1/namespaces/hipster/deployments/currencyservice
    uid: a31b37af-3441-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: currencyservice
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: currencyservice
      spec:
        containers:
        - env:
          - name: PORT
            value: "7000"
          image: gcr.io/google-samples/microservices-demo/currencyservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7000
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 7000
            name: grpc
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7000
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-11T07:11:50Z"
      lastUpdateTime: "2020-01-30T04:53:16Z"
      message: ReplicaSet "currencyservice-7775f7949" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-10T00:10:29Z"
      lastUpdateTime: "2020-04-10T00:10:29Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"emailservice","namespace":"hipster"},"spec":{"selector":{"matchLabels":{"app":"emailservice"}},"template":{"metadata":{"labels":{"app":"emailservice"}},"spec":{"containers":[{"env":[{"name":"PORT","value":"8080"},{"name":"ENABLE_PROFILER","value":"0"}],"image":"gcr.io/google-samples/microservices-demo/emailservice:v0.1.3","livenessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:8080"]},"periodSeconds":5},"name":"server","ports":[{"containerPort":8080}],"readinessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:8080"]},"periodSeconds":5},"resources":{"limits":{"cpu":"100m","memory":"128Mi"},"requests":{"cpu":"50m","memory":"64Mi"}}}],"terminationGracePeriodSeconds":5}}}}
    creationTimestamp: "2020-01-11T07:11:45Z"
    generation: 3
    name: emailservice
    namespace: hipster
    resourceVersion: "36840783"
    selfLink: /apis/apps/v1/namespaces/hipster/deployments/emailservice
    uid: a0685217-3441-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: emailservice
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: emailservice
      spec:
        containers:
        - env:
          - name: PORT
            value: "8080"
          - name: ENABLE_PROFILER
            value: "0"
          image: gcr.io/google-samples/microservices-demo/emailservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:8080
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 8080
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:8080
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-11T07:11:45Z"
      lastUpdateTime: "2020-01-30T04:53:47Z"
      message: ReplicaSet "emailservice-6545668f4f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-10T00:09:29Z"
      lastUpdateTime: "2020-04-10T00:09:29Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"frontend","namespace":"hipster"},"spec":{"selector":{"matchLabels":{"app":"frontend"}},"template":{"metadata":{"annotations":{"sidecar.istio.io/rewriteAppHTTPProbers":"true"},"labels":{"app":"frontend"}},"spec":{"containers":[{"env":[{"name":"PORT","value":"8080"},{"name":"PRODUCT_CATALOG_SERVICE_ADDR","value":"productcatalogservice:3550"},{"name":"CURRENCY_SERVICE_ADDR","value":"currencyservice:7000"},{"name":"CART_SERVICE_ADDR","value":"cartservice:7070"},{"name":"RECOMMENDATION_SERVICE_ADDR","value":"recommendationservice:8080"},{"name":"SHIPPING_SERVICE_ADDR","value":"shippingservice:50051"},{"name":"CHECKOUT_SERVICE_ADDR","value":"checkoutservice:5050"},{"name":"AD_SERVICE_ADDR","value":"adservice:9555"}],"image":"gcr.io/google-samples/microservices-demo/frontend:v0.1.3","livenessProbe":{"httpGet":{"httpHeaders":[{"name":"Cookie","value":"shop_session-id=x-liveness-probe"}],"path":"/_healthz","port":8080},"initialDelaySeconds":10},"name":"server","ports":[{"containerPort":8080}],"readinessProbe":{"httpGet":{"httpHeaders":[{"name":"Cookie","value":"shop_session-id=x-readiness-probe"}],"path":"/_healthz","port":8080},"initialDelaySeconds":10},"resources":{"limits":{"cpu":"20m","memory":"128Mi"},"requests":{"cpu":"5m","memory":"64Mi"}}}]}}}}
    creationTimestamp: "2020-01-11T07:11:47Z"
    generation: 2
    name: frontend
    namespace: hipster
    resourceVersion: "36841024"
    selfLink: /apis/apps/v1/namespaces/hipster/deployments/frontend
    uid: a15c8a4d-3441-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: frontend
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          sidecar.istio.io/rewriteAppHTTPProbers: "true"
        creationTimestamp: null
        labels:
          app: frontend
      spec:
        containers:
        - env:
          - name: PORT
            value: "8080"
          - name: PRODUCT_CATALOG_SERVICE_ADDR
            value: productcatalogservice:3550
          - name: CURRENCY_SERVICE_ADDR
            value: currencyservice:7000
          - name: CART_SERVICE_ADDR
            value: cartservice:7070
          - name: RECOMMENDATION_SERVICE_ADDR
            value: recommendationservice:8080
          - name: SHIPPING_SERVICE_ADDR
            value: shippingservice:50051
          - name: CHECKOUT_SERVICE_ADDR
            value: checkoutservice:5050
          - name: AD_SERVICE_ADDR
            value: adservice:9555
          image: gcr.io/google-samples/microservices-demo/frontend:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              - name: Cookie
                value: shop_session-id=x-liveness-probe
              path: /_healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 8080
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              - name: Cookie
                value: shop_session-id=x-readiness-probe
              path: /_healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 20m
              memory: 128Mi
            requests:
              cpu: 5m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-11T07:11:47Z"
      lastUpdateTime: "2020-01-30T04:44:37Z"
      message: ReplicaSet "frontend-7dbdf6c769" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-10T00:10:22Z"
      lastUpdateTime: "2020-04-10T00:10:22Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"loadgenerator","namespace":"hipster"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"loadgenerator"}},"template":{"metadata":{"annotations":{"sidecar.istio.io/rewriteAppHTTPProbers":"true"},"labels":{"app":"loadgenerator"}},"spec":{"containers":[{"env":[{"name":"FRONTEND_ADDR","value":"frontend:80"},{"name":"USERS","value":"10"}],"image":"gcr.io/google-samples/microservices-demo/loadgenerator:v0.1.3","name":"main","resources":{"limits":{"cpu":"50m","memory":"512Mi"},"requests":{"cpu":"10m","memory":"256Mi"}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":5}}}}
    creationTimestamp: "2020-01-11T07:11:49Z"
    generation: 2
    name: loadgenerator
    namespace: hipster
    resourceVersion: "33303377"
    selfLink: /apis/apps/v1/namespaces/hipster/deployments/loadgenerator
    uid: a2e2343e-3441-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: loadgenerator
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          sidecar.istio.io/rewriteAppHTTPProbers: "true"
        creationTimestamp: null
        labels:
          app: loadgenerator
      spec:
        containers:
        - env:
          - name: FRONTEND_ADDR
            value: frontend:80
          - name: USERS
            value: "10"
          image: gcr.io/google-samples/microservices-demo/loadgenerator:v0.1.3
          imagePullPolicy: IfNotPresent
          name: main
          resources:
            limits:
              cpu: 50m
              memory: 512Mi
            requests:
              cpu: 10m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-11T07:11:49Z"
      lastUpdateTime: "2020-01-30T04:44:31Z"
      message: ReplicaSet "loadgenerator-547598db87" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-03-31T18:44:17Z"
      lastUpdateTime: "2020-03-31T18:44:17Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"paymentservice","namespace":"hipster"},"spec":{"selector":{"matchLabels":{"app":"paymentservice"}},"template":{"metadata":{"labels":{"app":"paymentservice"}},"spec":{"containers":[{"env":[{"name":"PORT","value":"50051"}],"image":"gcr.io/google-samples/microservices-demo/paymentservice:v0.1.3","livenessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:50051"]}},"name":"server","ports":[{"containerPort":50051}],"readinessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:50051"]}},"resources":{"limits":{"cpu":"100m","memory":"128Mi"},"requests":{"cpu":"50m","memory":"64Mi"}}}],"terminationGracePeriodSeconds":5}}}}
    creationTimestamp: "2020-01-11T07:11:48Z"
    generation: 3
    name: paymentservice
    namespace: hipster
    resourceVersion: "36841079"
    selfLink: /apis/apps/v1/namespaces/hipster/deployments/paymentservice
    uid: a1d5e17e-3441-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: paymentservice
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: paymentservice
      spec:
        containers:
        - env:
          - name: PORT
            value: "50051"
          image: gcr.io/google-samples/microservices-demo/paymentservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:50051
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 50051
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:50051
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-11T07:11:48Z"
      lastUpdateTime: "2020-01-30T04:53:10Z"
      message: ReplicaSet "paymentservice-6b9d88465f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-10T00:10:30Z"
      lastUpdateTime: "2020-04-10T00:10:30Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"productcatalogservice","namespace":"hipster"},"spec":{"selector":{"matchLabels":{"app":"productcatalogservice"}},"template":{"metadata":{"labels":{"app":"productcatalogservice"}},"spec":{"containers":[{"env":[{"name":"PORT","value":"3550"}],"image":"gcr.io/google-samples/microservices-demo/productcatalogservice:v0.1.3","livenessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:3550"]}},"name":"server","ports":[{"containerPort":3550}],"readinessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:3550"]}},"resources":{"limits":{"cpu":"20m","memory":"128Mi"},"requests":{"cpu":"5m","memory":"64Mi"}}}],"terminationGracePeriodSeconds":5}}}}
    creationTimestamp: "2020-01-11T07:11:48Z"
    generation: 2
    name: productcatalogservice
    namespace: hipster
    resourceVersion: "37185770"
    selfLink: /apis/apps/v1/namespaces/hipster/deployments/productcatalogservice
    uid: a2397ecc-3441-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: productcatalogservice
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: productcatalogservice
      spec:
        containers:
        - env:
          - name: PORT
            value: "3550"
          image: gcr.io/google-samples/microservices-demo/productcatalogservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:3550
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 3550
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:3550
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 20m
              memory: 128Mi
            requests:
              cpu: 5m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-11T07:11:48Z"
      lastUpdateTime: "2020-01-30T04:45:00Z"
      message: ReplicaSet "productcatalogservice-7f5dc87d7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-10T22:40:33Z"
      lastUpdateTime: "2020-04-10T22:40:33Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"recommendationservice","namespace":"hipster"},"spec":{"selector":{"matchLabels":{"app":"recommendationservice"}},"template":{"metadata":{"labels":{"app":"recommendationservice"}},"spec":{"containers":[{"env":[{"name":"PORT","value":"8080"},{"name":"PRODUCT_CATALOG_SERVICE_ADDR","value":"productcatalogservice:3550"},{"name":"ENABLE_PROFILER","value":"0"}],"image":"gcr.io/google-samples/microservices-demo/recommendationservice:v0.1.3","livenessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:8080"]},"periodSeconds":5},"name":"server","ports":[{"containerPort":8080}],"readinessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:8080"]},"periodSeconds":5},"resources":{"limits":{"cpu":"20m","memory":"450Mi"},"requests":{"cpu":"5m","memory":"220Mi"}}}],"terminationGracePeriodSeconds":5}}}}
    creationTimestamp: "2020-01-11T07:11:46Z"
    generation: 2
    name: recommendationservice
    namespace: hipster
    resourceVersion: "36841108"
    selfLink: /apis/apps/v1/namespaces/hipster/deployments/recommendationservice
    uid: a1103ee1-3441-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: recommendationservice
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: recommendationservice
      spec:
        containers:
        - env:
          - name: PORT
            value: "8080"
          - name: PRODUCT_CATALOG_SERVICE_ADDR
            value: productcatalogservice:3550
          - name: ENABLE_PROFILER
            value: "0"
          image: gcr.io/google-samples/microservices-demo/recommendationservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:8080
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 8080
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:8080
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 20m
              memory: 450Mi
            requests:
              cpu: 5m
              memory: 220Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    conditions:
    - lastTransitionTime: "2020-02-03T21:05:08Z"
      lastUpdateTime: "2020-02-03T21:05:08Z"
      message: ReplicaSet "recommendationservice-744d5589c7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-10T00:10:36Z"
      lastUpdateTime: "2020-04-10T00:10:36Z"
      message: Deployment does not have minimum availability.
      reason: MinimumReplicasUnavailable
      status: "False"
      type: Available
    observedGeneration: 2
    replicas: 1
    unavailableReplicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"redis-cart","namespace":"hipster"},"spec":{"selector":{"matchLabels":{"app":"redis-cart"}},"template":{"metadata":{"labels":{"app":"redis-cart"}},"spec":{"containers":[{"image":"redis:alpine","livenessProbe":{"periodSeconds":5,"tcpSocket":{"port":6379}},"name":"redis","ports":[{"containerPort":6379}],"readinessProbe":{"periodSeconds":5,"tcpSocket":{"port":6379}},"resources":{"limits":{"cpu":"75m","memory":"256Mi"},"requests":{"cpu":"40m","memory":"200Mi"}},"volumeMounts":[{"mountPath":"/data","name":"redis-data"}]}],"volumes":[{"emptyDir":{},"name":"redis-data"}]}}}}
    creationTimestamp: "2020-01-11T07:11:51Z"
    generation: 2
    name: redis-cart
    namespace: hipster
    resourceVersion: "33302850"
    selfLink: /apis/apps/v1/namespaces/hipster/deployments/redis-cart
    uid: a3dfa36d-3441-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: redis-cart
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: redis-cart
      spec:
        containers:
        - image: redis:alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 6379
            timeoutSeconds: 1
          name: redis
          ports:
          - containerPort: 6379
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 6379
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 75m
              memory: 256Mi
            requests:
              cpu: 40m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: redis-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: redis-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-11T07:11:51Z"
      lastUpdateTime: "2020-01-30T04:44:31Z"
      message: ReplicaSet "redis-cart-58764b9d5d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-03-31T18:42:54Z"
      lastUpdateTime: "2020-03-31T18:42:54Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"shippingservice","namespace":"hipster"},"spec":{"selector":{"matchLabels":{"app":"shippingservice"}},"template":{"metadata":{"labels":{"app":"shippingservice"}},"spec":{"containers":[{"env":[{"name":"PORT","value":"50051"}],"image":"gcr.io/google-samples/microservices-demo/shippingservice:v0.1.3","livenessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:50051"]}},"name":"server","ports":[{"containerPort":50051}],"readinessProbe":{"exec":{"command":["/bin/grpc_health_probe","-addr=:50051"]},"periodSeconds":5},"resources":{"limits":{"cpu":"20m","memory":"128Mi"},"requests":{"cpu":"10m","memory":"64Mi"}}}]}}}}
    creationTimestamp: "2020-01-11T07:11:50Z"
    generation: 2
    name: shippingservice
    namespace: hipster
    resourceVersion: "37192533"
    selfLink: /apis/apps/v1/namespaces/hipster/deployments/shippingservice
    uid: a38191a7-3441-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: shippingservice
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: shippingservice
      spec:
        containers:
        - env:
          - name: PORT
            value: "50051"
          image: gcr.io/google-samples/microservices-demo/shippingservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:50051
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 50051
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:50051
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 20m
              memory: 128Mi
            requests:
              cpu: 10m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-11T07:11:50Z"
      lastUpdateTime: "2020-01-30T04:44:36Z"
      message: ReplicaSet "shippingservice-5f96974545" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-10T23:10:24Z"
      lastUpdateTime: "2020-04-10T23:10:24Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"calico-node-autoscaler","kubernetes.io/cluster-service":"true"},"name":"calico-node-vertical-autoscaler","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"calico-node-autoscaler"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"calico-node-autoscaler"}},"spec":{"containers":[{"command":["/cpvpa","--target=daemonset/calico-node","--namespace=kube-system","--logtostderr=true","--poll-period-seconds=30","--v=2","--config-file=/etc/config/node-autoscaler"],"image":"gke.gcr.io/cpvpa-amd64:v0.7.1-gke.0","name":"autoscaler","volumeMounts":[{"mountPath":"/etc/config","name":"config"}]}],"priorityClassName":"system-cluster-critical","serviceAccountName":"calico-cpva","volumes":[{"configMap":{"name":"calico-node-vertical-autoscaler"},"name":"config"}]}}}}
    creationTimestamp: "2020-01-22T17:44:02Z"
    generation: 2
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: calico-node-autoscaler
      kubernetes.io/cluster-service: "true"
    name: calico-node-vertical-autoscaler
    namespace: kube-system
    resourceVersion: "33571651"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/calico-node-vertical-autoscaler
    uid: c737192c-3d3e-11ea-96d3-42010a80017a
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        k8s-app: calico-node-autoscaler
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-node-autoscaler
      spec:
        containers:
        - command:
          - /cpvpa
          - --target=daemonset/calico-node
          - --namespace=kube-system
          - --logtostderr=true
          - --poll-period-seconds=30
          - --v=2
          - --config-file=/etc/config/node-autoscaler
          image: gke.gcr.io/cpvpa-amd64:v0.7.1-gke.0
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-cpva
        serviceAccountName: calico-cpva
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: calico-node-vertical-autoscaler
          name: config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-22T17:44:02Z"
      lastUpdateTime: "2020-01-22T17:44:02Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"calico-typha","kubernetes.io/cluster-service":"true"},"name":"calico-typha","namespace":"kube-system"},"spec":{"revisionHistoryLimit":2,"selector":{"matchLabels":{"k8s-app":"calico-typha"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"calico-typha"}},"spec":{"containers":[{"env":[{"name":"TYPHA_LOGFILEPATH","value":"none"},{"name":"TYPHA_LOGSEVERITYSYS","value":"none"},{"name":"TYPHA_LOGSEVERITYSCREEN","value":"warning"},{"name":"TYPHA_PROMETHEUSMETRICSENABLED","value":"true"},{"name":"TYPHA_CONNECTIONREBALANCINGMODE","value":"kubernetes"},{"name":"TYPHA_REPORTINGINTERVALSECS","value":"0"},{"name":"TYPHA_PROMETHEUSMETRICSPORT","value":"9093"},{"name":"TYPHA_DATASTORETYPE","value":"kubernetes"},{"name":"TYPHA_MAXCONNECTIONSLOWERLIMIT","value":"1"},{"name":"TYPHA_HEALTHENABLED","value":"true"}],"image":"gcr.io/projectcalico-org/typha:v3.2.7","livenessProbe":{"httpGet":{"host":"localhost","path":"/liveness","port":9098},"initialDelaySeconds":30,"periodSeconds":30},"name":"calico-typha","ports":[{"containerPort":5473,"name":"calico-typha","protocol":"TCP"}],"readinessProbe":{"httpGet":{"host":"localhost","path":"/readiness","port":9098},"periodSeconds":10},"volumeMounts":[{"mountPath":"/etc/calico","name":"etc-calico","readOnly":true}]}],"hostNetwork":true,"priorityClassName":"system-cluster-critical","serviceAccountName":"calico-sa","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/etc/calico"},"name":"etc-calico"}]}}}}
    creationTimestamp: "2020-01-22T17:44:03Z"
    generation: 2
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: calico-typha
      kubernetes.io/cluster-service: "true"
    name: calico-typha
    namespace: kube-system
    resourceVersion: "33571167"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/calico-typha
    uid: c7707f92-3d3e-11ea-96d3-42010a80017a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 2
    selector:
      matchLabels:
        k8s-app: calico-typha
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-typha
      spec:
        containers:
        - env:
          - name: TYPHA_LOGFILEPATH
            value: none
          - name: TYPHA_LOGSEVERITYSYS
            value: none
          - name: TYPHA_LOGSEVERITYSCREEN
            value: warning
          - name: TYPHA_PROMETHEUSMETRICSENABLED
            value: "true"
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: kubernetes
          - name: TYPHA_REPORTINGINTERVALSECS
            value: "0"
          - name: TYPHA_PROMETHEUSMETRICSPORT
            value: "9093"
          - name: TYPHA_DATASTORETYPE
            value: kubernetes
          - name: TYPHA_MAXCONNECTIONSLOWERLIMIT
            value: "1"
          - name: TYPHA_HEALTHENABLED
            value: "true"
          image: gcr.io/projectcalico-org/typha:v3.2.7
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              host: localhost
              path: /liveness
              port: 9098
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: calico-typha
          ports:
          - containerPort: 5473
            hostPort: 5473
            name: calico-typha
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: localhost
              path: /readiness
              port: 9098
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 200m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/calico
            name: etc-calico
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-sa
        serviceAccountName: calico-sa
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - hostPath:
            path: /etc/calico
            type: ""
          name: etc-calico
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-22T17:44:03Z"
      lastUpdateTime: "2020-01-22T17:44:48Z"
      message: ReplicaSet "calico-typha-65bfd5544b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-01T13:47:52Z"
      lastUpdateTime: "2020-04-01T13:47:52Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"calico-typha-autoscaler","kubernetes.io/cluster-service":"true"},"name":"calico-typha-horizontal-autoscaler","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"calico-typha-autoscaler"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"calico-typha-autoscaler"}},"spec":{"containers":[{"command":["/cluster-proportional-autoscaler","--namespace=kube-system","--configmap=calico-typha-horizontal-autoscaler","--target=deployment/calico-typha","--logtostderr=true","--v=2"],"image":"gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0","name":"autoscaler","resources":{"limits":{"cpu":"10m"},"requests":{"cpu":"10m"}}}],"priorityClassName":"system-cluster-critical","securityContext":{"fsGroup":65534,"supplementalGroups":[65534]},"serviceAccountName":"typha-cpha"}}}}
    creationTimestamp: "2020-01-22T17:44:03Z"
    generation: 2
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: calico-typha-autoscaler
      kubernetes.io/cluster-service: "true"
    name: calico-typha-horizontal-autoscaler
    namespace: kube-system
    resourceVersion: "33302483"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/calico-typha-horizontal-autoscaler
    uid: c7889f07-3d3e-11ea-96d3-42010a80017a
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        k8s-app: calico-typha-autoscaler
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-typha-autoscaler
      spec:
        containers:
        - command:
          - /cluster-proportional-autoscaler
          - --namespace=kube-system
          - --configmap=calico-typha-horizontal-autoscaler
          - --target=deployment/calico-typha
          - --logtostderr=true
          - --v=2
          image: gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources:
            limits:
              cpu: 10m
            requests:
              cpu: 10m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          supplementalGroups:
          - 65534
        serviceAccount: typha-cpha
        serviceAccountName: typha-cpha
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-22T17:44:03Z"
      lastUpdateTime: "2020-01-22T17:44:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"calico-typha-autoscaler","kubernetes.io/cluster-service":"true"},"name":"calico-typha-vertical-autoscaler","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"calico-typha-autoscaler"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"calico-typha-autoscaler"}},"spec":{"containers":[{"command":["/cpvpa","--target=deployment/calico-typha","--namespace=kube-system","--logtostderr=true","--poll-period-seconds=30","--v=2","--config-file=/etc/config/typha-autoscaler"],"image":"gke.gcr.io/cpvpa-amd64:v0.7.1-gke.0","name":"autoscaler","volumeMounts":[{"mountPath":"/etc/config","name":"config"}]}],"priorityClassName":"system-cluster-critical","serviceAccountName":"calico-cpva","volumes":[{"configMap":{"name":"calico-typha-vertical-autoscaler"},"name":"config"}]}}}}
    creationTimestamp: "2020-01-22T17:44:03Z"
    generation: 2
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: calico-typha-autoscaler
      kubernetes.io/cluster-service: "true"
    name: calico-typha-vertical-autoscaler
    namespace: kube-system
    resourceVersion: "33573320"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/calico-typha-vertical-autoscaler
    uid: c7ac1f10-3d3e-11ea-96d3-42010a80017a
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        k8s-app: calico-typha-autoscaler
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-typha-autoscaler
      spec:
        containers:
        - command:
          - /cpvpa
          - --target=deployment/calico-typha
          - --namespace=kube-system
          - --logtostderr=true
          - --poll-period-seconds=30
          - --v=2
          - --config-file=/etc/config/typha-autoscaler
          image: gke.gcr.io/cpvpa-amd64:v0.7.1-gke.0
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-cpva
        serviceAccountName: calico-cpva
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: calico-typha-vertical-autoscaler
          name: config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-22T17:44:03Z"
      lastUpdateTime: "2020-01-22T17:44:03Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"event-exporter","kubernetes.io/cluster-service":"true","version":"v0.2.4"},"name":"event-exporter-v0.2.4","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"event-exporter","version":"v0.2.4"}},"template":{"metadata":{"labels":{"k8s-app":"event-exporter","version":"v0.2.4"}},"spec":{"containers":[{"command":["/event-exporter","-sink-opts=-stackdriver-resource-model=old"],"image":"k8s.gcr.io/event-exporter:v0.2.4","name":"event-exporter"},{"command":["/monitor","--stackdriver-prefix=container.googleapis.com/internal/addons","--api-override=https://monitoring.googleapis.com/","--source=event_exporter:http://localhost:80?whitelisted=stackdriver_sink_received_entry_count,stackdriver_sink_request_count,stackdriver_sink_successfully_sent_entry_count","--pod-id=$(POD_NAME)","--namespace-id=$(POD_NAMESPACE)"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/prometheus-to-sd:v0.5.0","name":"prometheus-to-sd-exporter"}],"serviceAccountName":"event-exporter-sa","terminationGracePeriodSeconds":30,"volumes":[{"hostPath":{"path":"/etc/ssl/certs"},"name":"ssl-certs"}]}}}}
    creationTimestamp: "2020-01-11T06:01:02Z"
    generation: 2
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: event-exporter
      kubernetes.io/cluster-service: "true"
      version: v0.2.4
    name: event-exporter-v0.2.4
    namespace: kube-system
    resourceVersion: "33571768"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/event-exporter-v0.2.4
    uid: beff6bf2-3437-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 2
    selector:
      matchLabels:
        k8s-app: event-exporter
        version: v0.2.4
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: event-exporter
          version: v0.2.4
      spec:
        containers:
        - command:
          - /event-exporter
          - -sink-opts=-stackdriver-resource-model=old
          image: k8s.gcr.io/event-exporter:v0.2.4
          imagePullPolicy: IfNotPresent
          name: event-exporter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --source=event_exporter:http://localhost:80?whitelisted=stackdriver_sink_received_entry_count,stackdriver_sink_request_count,stackdriver_sink_successfully_sent_entry_count
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd-exporter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: event-exporter-sa
        serviceAccountName: event-exporter-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /etc/ssl/certs
            type: ""
          name: ssl-certs
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-11T06:01:02Z"
      lastUpdateTime: "2020-01-11T06:01:29Z"
      message: ReplicaSet "event-exporter-v0.2.4-5f88c66fb7" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-01T13:50:07Z"
      lastUpdateTime: "2020-04-01T13:50:07Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd-gcp-scaler","version":"v0.5.1"},"name":"fluentd-gcp-scaler","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"fluentd-gcp-scaler"}},"template":{"metadata":{"labels":{"k8s-app":"fluentd-gcp-scaler"}},"spec":{"containers":[{"command":["/scaler.sh","--ds-name=fluentd-gcp-v3.2.0","--scaling-policy=fluentd-gcp-scaling-policy"],"env":[{"name":"CPU_REQUEST","value":"100m"},{"name":"MEMORY_REQUEST","value":"200Mi"},{"name":"CPU_LIMIT","value":"1"},{"name":"MEMORY_LIMIT","value":"500Mi"}],"image":"k8s.gcr.io/fluentd-gcp-scaler:0.5.2","name":"fluentd-gcp-scaler"}],"serviceAccountName":"fluentd-gcp-scaler"}}}}
    creationTimestamp: "2020-01-11T06:01:13Z"
    generation: 2
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: fluentd-gcp-scaler
      version: v0.5.1
    name: fluentd-gcp-scaler
    namespace: kube-system
    resourceVersion: "33571783"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/fluentd-gcp-scaler
    uid: c610e38a-3437-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: fluentd-gcp-scaler
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: fluentd-gcp-scaler
      spec:
        containers:
        - command:
          - /scaler.sh
          - --ds-name=fluentd-gcp-v3.2.0
          - --scaling-policy=fluentd-gcp-scaling-policy
          env:
          - name: CPU_REQUEST
            value: 100m
          - name: MEMORY_REQUEST
            value: 200Mi
          - name: CPU_LIMIT
            value: "1"
          - name: MEMORY_LIMIT
            value: 500Mi
          image: k8s.gcr.io/fluentd-gcp-scaler:0.5.2
          imagePullPolicy: IfNotPresent
          name: fluentd-gcp-scaler
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: fluentd-gcp-scaler
        serviceAccountName: fluentd-gcp-scaler
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-11T06:01:13Z"
      lastUpdateTime: "2020-03-26T11:51:49Z"
      message: ReplicaSet "fluentd-gcp-scaler-6965bb45c9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-01T13:50:09Z"
      lastUpdateTime: "2020-04-01T13:50:09Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"heapster","kubernetes.io/cluster-service":"true","version":"v1.7.2"},"name":"heapster-gke","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"heapster"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":"","seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"labels":{"k8s-app":"heapster","version":"v1.7.2"}},"spec":{"containers":[{"command":["/heapster","--source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id","--sink=stackdriver:?cluster_name=kubevious-samples\u0026use_old_resources=true\u0026use_new_resources=false\u0026min_interval_sec=100\u0026batch_export_timeout_sec=110\u0026cluster_location=us-central1-a"],"image":"gke.gcr.io/heapster:v1.7.2","livenessProbe":{"httpGet":{"path":"/healthz","port":8082,"scheme":"HTTP"},"initialDelaySeconds":180,"timeoutSeconds":5},"name":"heapster"},{"command":["/monitor","--source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count","--stackdriver-prefix=container.googleapis.com/internal/addons","--api-override=https://monitoring.googleapis.com/","--pod-id=$(POD_NAME)","--namespace-id=$(POD_NAMESPACE)"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/prometheus-to-sd:v0.5.0","name":"prom-to-sd"},{"command":["/pod_nanny","--config-dir=/etc/config","--cpu=10m","--extra-cpu=0.5m","--memory=100Mi","--extra-memory=4Mi","--threshold=5","--deployment=heapster-gke","--container=heapster","--poll-period=300000","--estimator=exponential","--minClusterSize=5"],"env":[{"name":"MY_POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"MY_POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/addon-resizer:1.8.3","name":"heapster-nanny","resources":{"limits":{"cpu":"50m","memory":"92760Ki"},"requests":{"cpu":"50m","memory":"92760Ki"}},"volumeMounts":[{"mountPath":"/etc/config","name":"heapster-config-volume"}]}],"priorityClassName":"system-cluster-critical","securityContext":{"fsGroup":65534,"supplementalGroups":[65534]},"serviceAccountName":"heapster","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"configMap":{"name":"heapster-config"},"name":"heapster-config-volume"}]}}}}
    creationTimestamp: "2020-03-26T11:51:16Z"
    generation: 2
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: heapster
      kubernetes.io/cluster-service: "true"
      version: v1.7.2
    name: heapster-gke
    namespace: kube-system
    resourceVersion: "33571900"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/heapster-gke
    uid: 19bb5a70-6f58-11ea-8ebf-42010a800207
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: heapster
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: heapster
          version: v1.7.2
      spec:
        containers:
        - command:
          - /heapster
          - --source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id
          - --sink=stackdriver:?cluster_name=kubevious-samples&use_old_resources=true&use_new_resources=false&min_interval_sec=100&batch_export_timeout_sec=110&cluster_location=us-central1-a
          image: gke.gcr.io/heapster:v1.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8082
              scheme: HTTP
            initialDelaySeconds: 180
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: heapster
          resources:
            limits:
              cpu: 13m
              memory: 120Mi
            requests:
              cpu: 13m
              memory: 120Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prom-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=10m
          - --extra-cpu=0.5m
          - --memory=100Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=heapster-gke
          - --container=heapster
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.3
          imagePullPolicy: IfNotPresent
          name: heapster-nanny
          resources:
            limits:
              cpu: 50m
              memory: 92760Ki
            requests:
              cpu: 50m
              memory: 92760Ki
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: heapster-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          supplementalGroups:
          - 65534
        serviceAccount: heapster
        serviceAccountName: heapster
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: heapster-config
          name: heapster-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-03-26T11:51:16Z"
      lastUpdateTime: "2020-03-26T11:53:57Z"
      message: ReplicaSet "heapster-gke-566bdc98db" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-01T13:50:19Z"
      lastUpdateTime: "2020-04-01T13:50:19Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kube-dns","kubernetes.io/cluster-service":"true"},"name":"kube-dns","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"kube-dns"}},"strategy":{"rollingUpdate":{"maxSurge":"10%","maxUnavailable":0}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":"","seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"labels":{"k8s-app":"kube-dns"}},"spec":{"containers":[{"args":["--domain=cluster.local.","--dns-port=10053","--config-dir=/kube-dns-config","--v=2"],"env":[{"name":"PROMETHEUS_PORT","value":"10055"}],"image":"k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4","livenessProbe":{"failureThreshold":5,"httpGet":{"path":"/healthcheck/kubedns","port":10054,"scheme":"HTTP"},"initialDelaySeconds":60,"successThreshold":1,"timeoutSeconds":5},"name":"kubedns","ports":[{"containerPort":10053,"name":"dns-local","protocol":"UDP"},{"containerPort":10053,"name":"dns-tcp-local","protocol":"TCP"},{"containerPort":10055,"name":"metrics","protocol":"TCP"}],"readinessProbe":{"httpGet":{"path":"/readiness","port":8081,"scheme":"HTTP"},"initialDelaySeconds":3,"timeoutSeconds":5},"resources":{"limits":{"memory":"170Mi"},"requests":{"cpu":"100m","memory":"70Mi"}},"volumeMounts":[{"mountPath":"/kube-dns-config","name":"kube-dns-config"}]},{"args":["-v=2","-logtostderr","-configDir=/etc/k8s/dns/dnsmasq-nanny","-restartDnsmasq=true","--","-k","--cache-size=1000","--no-negcache","--log-facility=-","--server=/cluster.local/127.0.0.1#10053","--server=/in-addr.arpa/127.0.0.1#10053","--server=/ip6.arpa/127.0.0.1#10053"],"image":"k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4","livenessProbe":{"failureThreshold":5,"httpGet":{"path":"/healthcheck/dnsmasq","port":10054,"scheme":"HTTP"},"initialDelaySeconds":60,"successThreshold":1,"timeoutSeconds":5},"name":"dnsmasq","ports":[{"containerPort":53,"name":"dns","protocol":"UDP"},{"containerPort":53,"name":"dns-tcp","protocol":"TCP"}],"resources":{"requests":{"cpu":"150m","memory":"20Mi"}},"volumeMounts":[{"mountPath":"/etc/k8s/dns/dnsmasq-nanny","name":"kube-dns-config"}]},{"args":["--v=2","--logtostderr","--probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV","--probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV"],"image":"k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4","livenessProbe":{"failureThreshold":5,"httpGet":{"path":"/metrics","port":10054,"scheme":"HTTP"},"initialDelaySeconds":60,"successThreshold":1,"timeoutSeconds":5},"name":"sidecar","ports":[{"containerPort":10054,"name":"metrics","protocol":"TCP"}],"resources":{"requests":{"cpu":"10m","memory":"20Mi"}}},{"command":["/monitor","--source=kubedns:http://localhost:10054?whitelisted=probe_kubedns_latency_ms,probe_kubedns_errors,dnsmasq_misses,dnsmasq_hits","--stackdriver-prefix=container.googleapis.com/internal/addons","--api-override=https://monitoring.googleapis.com/","--pod-id=$(POD_NAME)","--namespace-id=$(POD_NAMESPACE)","--v=2"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/prometheus-to-sd:v0.4.2","name":"prometheus-to-sd"}],"dnsPolicy":"Default","priorityClassName":"system-cluster-critical","serviceAccountName":"kube-dns","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"configMap":{"name":"kube-dns","optional":true},"name":"kube-dns-config"}]}}}}
    creationTimestamp: "2020-01-11T06:01:01Z"
    generation: 8
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
    name: kube-dns
    namespace: kube-system
    resourceVersion: "33571713"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/kube-dns
    uid: bec14d00-3437-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 2
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        containers:
        - args:
          - --domain=cluster.local.
          - --dns-port=10053
          - --config-dir=/kube-dns-config
          - --v=2
          env:
          - name: PROMETHEUS_PORT
            value: "10055"
          image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/kubedns
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kubedns
          ports:
          - containerPort: 10053
            name: dns-local
            protocol: UDP
          - containerPort: 10053
            name: dns-tcp-local
            protocol: TCP
          - containerPort: 10055
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /kube-dns-config
            name: kube-dns-config
        - args:
          - -v=2
          - -logtostderr
          - -configDir=/etc/k8s/dns/dnsmasq-nanny
          - -restartDnsmasq=true
          - --
          - -k
          - --cache-size=1000
          - --no-negcache
          - --log-facility=-
          - --server=/cluster.local/127.0.0.1#10053
          - --server=/in-addr.arpa/127.0.0.1#10053
          - --server=/ip6.arpa/127.0.0.1#10053
          image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/dnsmasq
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: dnsmasq
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          resources:
            requests:
              cpu: 150m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/k8s/dns/dnsmasq-nanny
            name: kube-dns-config
        - args:
          - --v=2
          - --logtostderr
          - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV
          - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV
          image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /metrics
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: sidecar
          ports:
          - containerPort: 10054
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=kubedns:http://localhost:10054?whitelisted=probe_kubedns_latency_ms,probe_kubedns_errors,dnsmasq_misses,dnsmasq_hits
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          - --v=2
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.4.2
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: Default
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-dns
        serviceAccountName: kube-dns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-dns
            optional: true
          name: kube-dns-config
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2020-04-01T13:50:00Z"
      lastUpdateTime: "2020-04-01T13:50:00Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 8
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kube-dns-autoscaler","kubernetes.io/cluster-service":"true"},"name":"kube-dns-autoscaler","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"kube-dns-autoscaler"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":"","seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"labels":{"k8s-app":"kube-dns-autoscaler"}},"spec":{"containers":[{"command":["/cluster-proportional-autoscaler","--namespace=kube-system","--configmap=kube-dns-autoscaler","--target=Deployment/kube-dns","--default-params={\"linear\":{\"coresPerReplica\":256,\"nodesPerReplica\":16,\"preventSinglePointFailure\":true}}","--logtostderr=true","--v=2"],"image":"gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0","name":"autoscaler","resources":{"requests":{"cpu":"20m","memory":"10Mi"}}}],"priorityClassName":"system-cluster-critical","securityContext":{"fsGroup":65534,"supplementalGroups":[65534]},"serviceAccountName":"kube-dns-autoscaler","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}]}}}}
    creationTimestamp: "2020-01-11T06:01:01Z"
    generation: 2
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: kube-dns-autoscaler
      kubernetes.io/cluster-service: "true"
    name: kube-dns-autoscaler
    namespace: kube-system
    resourceVersion: "33302452"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/kube-dns-autoscaler
    uid: bee75c2d-3437-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns-autoscaler
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: kube-dns-autoscaler
      spec:
        containers:
        - command:
          - /cluster-proportional-autoscaler
          - --namespace=kube-system
          - --configmap=kube-dns-autoscaler
          - --target=Deployment/kube-dns
          - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"preventSinglePointFailure":true}}
          - --logtostderr=true
          - --v=2
          image: gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources:
            requests:
              cpu: 20m
              memory: 10Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          supplementalGroups:
          - 65534
        serviceAccount: kube-dns-autoscaler
        serviceAccountName: kube-dns-autoscaler
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-11T06:01:01Z"
      lastUpdateTime: "2020-03-26T11:52:00Z"
      message: ReplicaSet "kube-dns-autoscaler-8687c64fc" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-03-31T18:42:14Z"
      lastUpdateTime: "2020-03-31T18:42:14Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"glbc","kubernetes.io/cluster-service":"true","kubernetes.io/name":"GLBC"},"name":"l7-default-backend","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"glbc"}},"template":{"metadata":{"annotations":{"seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"labels":{"k8s-app":"glbc","name":"glbc"}},"spec":{"containers":[{"image":"k8s.gcr.io/defaultbackend-amd64:1.5","livenessProbe":{"httpGet":{"path":"/healthz","port":8080,"scheme":"HTTP"},"initialDelaySeconds":30,"timeoutSeconds":5},"name":"default-http-backend","ports":[{"containerPort":8080}],"resources":{"limits":{"cpu":"10m","memory":"20Mi"},"requests":{"cpu":"10m","memory":"20Mi"}}}]}}}}
    creationTimestamp: "2020-01-11T06:01:01Z"
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: glbc
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: GLBC
    name: l7-default-backend
    namespace: kube-system
    resourceVersion: "33571684"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/l7-default-backend
    uid: be955236-3437-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: glbc
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: glbc
          name: glbc
      spec:
        containers:
        - image: k8s.gcr.io/defaultbackend-amd64:1.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: default-http-backend
          ports:
          - containerPort: 8080
            protocol: TCP
          resources:
            limits:
              cpu: 10m
              memory: 20Mi
            requests:
              cpu: 10m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-11T06:01:01Z"
      lastUpdateTime: "2020-01-11T06:01:31Z"
      message: ReplicaSet "l7-default-backend-fd59995cd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-01T13:49:56Z"
      lastUpdateTime: "2020-04-01T13:49:56Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "13"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"metrics-server","kubernetes.io/cluster-service":"true","version":"v0.3.1"},"name":"metrics-server-v0.3.1","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"metrics-server","version":"v0.3.1"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":"","seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"labels":{"k8s-app":"metrics-server","version":"v0.3.1"},"name":"metrics-server"},"spec":{"containers":[{"command":["/metrics-server","--metric-resolution=30s","--kubelet-port=10255","--deprecated-kubelet-completely-insecure=true","--kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP"],"image":"k8s.gcr.io/metrics-server-amd64:v0.3.1","name":"metrics-server","ports":[{"containerPort":443,"name":"https","protocol":"TCP"}]},{"command":["/pod_nanny","--config-dir=/etc/config","--cpu=40m","--extra-cpu=0.5m","--memory=35Mi","--extra-memory=4Mi","--threshold=5","--deployment=metrics-server-v0.3.1","--container=metrics-server","--poll-period=300000","--estimator=exponential","--minClusterSize=5"],"env":[{"name":"MY_POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"MY_POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"gke.gcr.io/addon-resizer:1.8.4-gke.0","name":"metrics-server-nanny","resources":{"limits":{"cpu":"100m","memory":"300Mi"},"requests":{"cpu":"5m","memory":"50Mi"}},"volumeMounts":[{"mountPath":"/etc/config","name":"metrics-server-config-volume"}]}],"priorityClassName":"system-cluster-critical","serviceAccountName":"metrics-server","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"configMap":{"name":"metrics-server-config"},"name":"metrics-server-config-volume"}]}}}}
    creationTimestamp: "2020-01-11T06:01:03Z"
    generation: 13
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: metrics-server
      kubernetes.io/cluster-service: "true"
      version: v0.3.1
    name: metrics-server-v0.3.1
    namespace: kube-system
    resourceVersion: "33571973"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/metrics-server-v0.3.1
    uid: c0161ad4-3437-11ea-9cdc-42010a8001cf
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        k8s-app: metrics-server
        version: v0.3.1
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          version: v0.3.1
        name: metrics-server
      spec:
        containers:
        - command:
          - /metrics-server
          - --metric-resolution=30s
          - --kubelet-port=10255
          - --deprecated-kubelet-completely-insecure=true
          - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
          image: k8s.gcr.io/metrics-server-amd64:v0.3.1
          imagePullPolicy: IfNotPresent
          name: metrics-server
          ports:
          - containerPort: 443
            name: https
            protocol: TCP
          resources:
            limits:
              cpu: 43m
              memory: 55Mi
            requests:
              cpu: 43m
              memory: 55Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=40m
          - --extra-cpu=0.5m
          - --memory=35Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=metrics-server-v0.3.1
          - --container=metrics-server
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: gke.gcr.io/addon-resizer:1.8.4-gke.0
          imagePullPolicy: IfNotPresent
          name: metrics-server-nanny
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 5m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: metrics-server-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: metrics-server-config
          name: metrics-server-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-11T06:01:04Z"
      lastUpdateTime: "2020-01-11T06:01:04Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 13
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2020-03-27T23:06:16Z"
    generation: 6
    name: kubevious
    namespace: kubevious
    resourceVersion: "36841064"
    selfLink: /apis/apps/v1/namespaces/kubevious/deployments/kubevious
    uid: 8f93d9b0-707f-11ea-8ebf-42010a800207
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kubevious
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/kubevious:0.5.2
          imagePullPolicy: IfNotPresent
          name: kubevious
          ports:
          - containerPort: 4000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious
        serviceAccountName: kubevious
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-03-27T23:06:16Z"
      lastUpdateTime: "2020-04-08T07:33:40Z"
      message: ReplicaSet "kubevious-865cf6dfcf" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-10T00:10:28Z"
      lastUpdateTime: "2020-04-10T00:10:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 6
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2020-04-02T02:02:58Z"
    generation: 6
    name: kubevious-parser
    namespace: kubevious
    resourceVersion: "36202566"
    selfLink: /apis/apps/v1/namespaces/kubevious/deployments/kubevious-parser
    uid: 13795f4a-7486-11ea-8ebf-42010a800207
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kubevious-parser
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-parser
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: KUBEVIOUS_COLLECTOR
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000/api/v1/collect
          image: kubevious/parser:0.5.6
          imagePullPolicy: IfNotPresent
          name: kubevious-parser
          ports:
          - containerPort: 4500
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-parser
        serviceAccountName: kubevious-parser
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-04-05T03:05:22Z"
      lastUpdateTime: "2020-04-05T03:05:22Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2020-04-02T02:02:58Z"
      lastUpdateTime: "2020-04-08T07:33:32Z"
      message: ReplicaSet "kubevious-parser-85d9bf6d8c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 6
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "8"
    creationTimestamp: "2020-03-27T23:06:16Z"
    generation: 8
    name: kubevious-ui
    namespace: kubevious
    resourceVersion: "36202512"
    selfLink: /apis/apps/v1/namespaces/kubevious/deployments/kubevious-ui
    uid: 8f945e6c-707f-11ea-8ebf-42010a800207
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kubevious-ui
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-ui
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: BACKEND_URL
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000
          - name: FORCE_HTTPS
            value: "true"
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/ui:0.5.3
          imagePullPolicy: IfNotPresent
          name: kubevious-ui
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 25m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/views/partials
            name: header-config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-ui
        serviceAccountName: kubevious-ui
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kubevious-ui-header
          name: header-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-04-01T22:00:34Z"
      lastUpdateTime: "2020-04-01T22:00:34Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2020-03-27T23:06:16Z"
      lastUpdateTime: "2020-04-08T07:33:22Z"
      message: ReplicaSet "kubevious-ui-686cfc86f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 8
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:05:49Z"
    generation: 1
    labels:
      app: openfaas
      chart: openfaas-5.4.1
      component: alertmanager
      heritage: Helm
      release: openfaas
    name: alertmanager
    namespace: openfaas
    resourceVersion: "33302683"
    selfLink: /apis/apps/v1/namespaces/openfaas/deployments/alertmanager
    uid: 486a20ed-3986-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: alertmanager
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/alertmanager-config: fc705a1674460ecc032254f930b2d2877349afe3e22607ff974a68360b57ab3e
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app: alertmanager
      spec:
        containers:
        - command:
          - alertmanager
          - --config.file=/alertmanager.yml
          - --storage.path=/alertmanager
          - --cluster.listen-address=
          image: prom/alertmanager:v0.18.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9093
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: alertmanager
          ports:
          - containerPort: 9093
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9093
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          resources:
            limits:
              memory: 50Mi
            requests:
              memory: 25Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /alertmanager.yml
            name: alertmanager-config
            subPath: alertmanager.yml
          - mountPath: /var/secrets
            name: auth
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/arch: amd64
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: alertmanager.yml
              mode: 420
              path: alertmanager.yml
            name: alertmanager-config
          name: alertmanager-config
        - name: auth
          secret:
            defaultMode: 420
            secretName: basic-auth
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:05:49Z"
      lastUpdateTime: "2020-01-18T00:06:10Z"
      message: ReplicaSet "alertmanager-8487d7f7bb" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-03-31T18:42:36Z"
      lastUpdateTime: "2020-03-31T18:42:36Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:05:49Z"
    generation: 1
    labels:
      app: openfaas
      chart: openfaas-5.4.1
      component: basic-auth-plugin
      heritage: Helm
      release: openfaas
    name: basic-auth-plugin
    namespace: openfaas
    resourceVersion: "33571823"
    selfLink: /apis/apps/v1/namespaces/openfaas/deployments/basic-auth-plugin
    uid: 48697343-3986-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: basic-auth-plugin
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io.scrape: "false"
        creationTimestamp: null
        labels:
          app: basic-auth-plugin
      spec:
        containers:
        - env:
          - name: secret_mount_path
            value: /var/secrets
          - name: basic_auth
            value: "true"
          image: openfaas/basic-auth-plugin:0.17.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: basic-auth-plugin
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 20m
              memory: 50Mi
          securityContext:
            readOnlyRootFilesystem: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/secrets
            name: auth
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/arch: amd64
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: auth
          secret:
            defaultMode: 420
            secretName: basic-auth
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:05:49Z"
      lastUpdateTime: "2020-01-18T00:06:13Z"
      message: ReplicaSet "basic-auth-plugin-6b4c47c965" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-01T13:50:13Z"
      lastUpdateTime: "2020-04-01T13:50:13Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:05:49Z"
    generation: 1
    labels:
      app: openfaas
      chart: openfaas-5.4.1
      component: faas-idler
      heritage: Helm
      release: openfaas
    name: faas-idler
    namespace: openfaas
    resourceVersion: "33302396"
    selfLink: /apis/apps/v1/namespaces/openfaas/deployments/faas-idler
    uid: 4869389d-3986-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: faas-idler
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io.scrape: "false"
        creationTimestamp: null
        labels:
          app: faas-idler
      spec:
        containers:
        - command:
          - /home/app/faas-idler
          - -dry-run=true
          env:
          - name: gateway_url
            value: http://gateway.openfaas:8080/
          - name: prometheus_host
            value: prometheus.openfaas
          - name: prometheus_port
            value: "9090"
          - name: inactivity_duration
            value: 30m
          - name: reconcile_interval
            value: 2m
          image: openfaas/faas-idler:0.2.1
          imagePullPolicy: Always
          name: faas-idler
          resources:
            requests:
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/secrets/
            name: auth
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/arch: amd64
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: auth
          secret:
            defaultMode: 420
            secretName: basic-auth
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:05:49Z"
      lastUpdateTime: "2020-01-18T00:05:58Z"
      message: ReplicaSet "faas-idler-66ff47fdf5" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-03-31T18:42:07Z"
      lastUpdateTime: "2020-03-31T18:42:07Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-18T00:05:49Z"
    generation: 2
    labels:
      app: openfaas
      chart: openfaas-5.4.1
      component: gateway
      heritage: Helm
      release: openfaas
    name: gateway
    namespace: openfaas
    resourceVersion: "33572236"
    selfLink: /apis/apps/v1/namespaces/openfaas/deployments/gateway
    uid: 4868b189-3986-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: gateway
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io.port: "8082"
          prometheus.io.scrape: "true"
        creationTimestamp: null
        labels:
          app: gateway
      spec:
        containers:
        - env:
          - name: read_timeout
            value: 65s
          - name: write_timeout
            value: 65s
          - name: upstream_timeout
            value: 60s
          - name: functions_provider_url
            value: http://127.0.0.1:8081/
          - name: direct_functions
            value: "true"
          - name: direct_functions_suffix
            value: openfaas-fn.svc.cluster.local
          - name: function_namespace
            value: openfaas-fn
          - name: faas_nats_address
            value: nats.openfaas.svc.cluster.local
          - name: faas_nats_port
            value: "4222"
          - name: faas_nats_channel
            value: faas-request
          - name: basic_auth
            value: "true"
          - name: secret_mount_path
            value: /var/secrets
          - name: auth_proxy_url
            value: http://basic-auth-plugin.openfaas:8080/validate
          - name: auth_pass_body
            value: "false"
          - name: scale_from_zero
            value: "true"
          - name: max_idle_conns
            value: "1024"
          - name: max_idle_conns_per_host
            value: "1024"
          image: openfaas/gateway:0.18.7
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: gateway
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 25m
              memory: 120Mi
          securityContext:
            readOnlyRootFilesystem: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/secrets
            name: auth
            readOnly: true
        - env:
          - name: port
            value: "8081"
          - name: function_namespace
            value: openfaas-fn
          - name: read_timeout
            value: 60s
          - name: write_timeout
            value: 60s
          - name: image_pull_policy
            value: Always
          - name: http_probe
            value: "true"
          - name: set_nonroot_user
            value: "false"
          - name: readiness_probe_initial_delay_seconds
            value: "2"
          - name: readiness_probe_timeout_seconds
            value: "1"
          - name: readiness_probe_period_seconds
            value: "2"
          - name: liveness_probe_initial_delay_seconds
            value: "2"
          - name: liveness_probe_timeout_seconds
            value: "1"
          - name: liveness_probe_period_seconds
            value: "2"
          image: openfaas/faas-netes:0.9.15
          imagePullPolicy: IfNotPresent
          name: faas-netes
          ports:
          - containerPort: 8081
            protocol: TCP
          resources:
            requests:
              cpu: 25m
              memory: 120Mi
          securityContext:
            readOnlyRootFilesystem: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: faas-netes-temp-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/arch: amd64
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: openfaas-controller
        serviceAccountName: openfaas-controller
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: faas-netes-temp-volume
        - name: auth
          secret:
            defaultMode: 420
            secretName: basic-auth
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:05:49Z"
      lastUpdateTime: "2020-03-21T23:59:38Z"
      message: ReplicaSet "gateway-d56c44b6d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-01T13:51:29Z"
      lastUpdateTime: "2020-04-01T13:51:29Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:05:49Z"
    generation: 1
    labels:
      app: openfaas
      chart: openfaas-5.4.1
      component: nats
      heritage: Helm
      release: openfaas
    name: nats
    namespace: openfaas
    resourceVersion: "33302511"
    selfLink: /apis/apps/v1/namespaces/openfaas/deployments/nats
    uid: 48686ac4-3986-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: nats
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io.scrape: "false"
          sidecar.istio.io/inject: "false"
        creationTimestamp: null
        labels:
          app: nats
      spec:
        containers:
        - args:
          - --store
          - memory
          - --cluster_id
          - faas-cluster
          command:
          - /nats-streaming-server
          image: nats-streaming:0.11.2
          imagePullPolicy: Always
          name: nats
          ports:
          - containerPort: 4222
            protocol: TCP
          resources:
            requests:
              memory: 120Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/arch: amd64
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:05:49Z"
      lastUpdateTime: "2020-01-18T00:05:57Z"
      message: ReplicaSet "nats-7666fb76bd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-03-31T18:42:19Z"
      lastUpdateTime: "2020-03-31T18:42:19Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:05:49Z"
    generation: 1
    labels:
      app: openfaas
      chart: openfaas-5.4.1
      component: prometheus
      heritage: Helm
      release: openfaas
    name: prometheus
    namespace: openfaas
    resourceVersion: "33571932"
    selfLink: /apis/apps/v1/namespaces/openfaas/deployments/prometheus
    uid: 4869f7a7-3986-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: prometheus
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/prometheus-config: c3bbdb127a0bfc5afa2014b462e13989f96cb8931d7e3650f50503b9586ed0e2
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app: prometheus
      spec:
        containers:
        - command:
          - prometheus
          - --config.file=/etc/prometheus/prometheus.yml
          image: prom/prometheus:v2.11.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: prometheus
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          resources:
            requests:
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/prometheus/prometheus.yml
            name: prometheus-config
            subPath: prometheus.yml
          - mountPath: /etc/prometheus/alert.rules.yml
            name: prometheus-config
            subPath: alert.rules.yml
          - mountPath: /prometheus/data
            name: prom-data
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/arch: amd64
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: openfaas-prometheus
        serviceAccountName: openfaas-prometheus
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: prometheus.yml
              mode: 420
              path: prometheus.yml
            - key: alert.rules.yml
              mode: 420
              path: alert.rules.yml
            name: prometheus-config
          name: prometheus-config
        - emptyDir: {}
          name: prom-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:05:49Z"
      lastUpdateTime: "2020-01-18T00:06:18Z"
      message: ReplicaSet "prometheus-6d4c6646b9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-01T13:50:23Z"
      lastUpdateTime: "2020-04-01T13:50:23Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:05:49Z"
    generation: 1
    labels:
      app: openfaas
      chart: openfaas-5.4.1
      component: queue-worker
      heritage: Helm
      release: openfaas
    name: queue-worker
    namespace: openfaas
    resourceVersion: "33572227"
    selfLink: /apis/apps/v1/namespaces/openfaas/deployments/queue-worker
    uid: 486a0cb9-3986-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: queue-worker
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io.scrape: "false"
        creationTimestamp: null
        labels:
          app: queue-worker
      spec:
        containers:
        - env:
          - name: faas_nats_address
            value: nats.openfaas.svc.cluster.local
          - name: faas_nats_durable_queue_subscription
            value: "false"
          - name: faas_nats_channel
            value: faas-request
          - name: faas_nats_queue_group
            value: faas
          - name: faas_gateway_address
            value: gateway.openfaas.svc.cluster.local
          - name: gateway_invoke
            value: "true"
          - name: faas_function_suffix
            value: .openfaas-fn.svc.cluster.local
          - name: ack_wait
            value: 60s
          - name: secret_mount_path
            value: /var/secrets
          - name: basic_auth
            value: "true"
          image: openfaas/queue-worker:0.9.0
          imagePullPolicy: Always
          name: queue-worker
          resources:
            requests:
              cpu: 50m
              memory: 120Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/secrets
            name: auth
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/arch: amd64
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: auth
          secret:
            defaultMode: 420
            secretName: basic-auth
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-18T00:05:49Z"
      lastUpdateTime: "2020-01-18T00:06:10Z"
      message: ReplicaSet "queue-worker-75658c877d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-04-01T13:51:27Z"
      lastUpdateTime: "2020-04-01T13:51:27Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"polaris","component":"dashboard"},"name":"polaris-dashboard","namespace":"polaris"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"polaris","component":"dashboard"}},"template":{"metadata":{"annotations":{"checksum/config":"8aa5a565fba7a2db98d46752087de8c1dcc83b70cd762c5829d5ba01270d54a2"},"labels":{"app":"polaris","component":"dashboard"}},"spec":{"containers":[{"command":["polaris","--dashboard","--config","/opt/app/config.yaml"],"image":"quay.io/fairwinds/polaris:0.6","imagePullPolicy":"Always","livenessProbe":{"httpGet":{"path":"/health","port":8080},"initialDelaySeconds":5,"periodSeconds":20},"name":"dashboard","ports":[{"containerPort":8080}],"readinessProbe":{"httpGet":{"path":"/health","port":8080},"initialDelaySeconds":5,"periodSeconds":20},"resources":{"limits":{"cpu":"100m","memory":"128Mi"},"requests":{"cpu":"100m","memory":"128Mi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["ALL"]},"privileged":false,"readOnlyRootFilesystem":true,"runAsNonRoot":true},"volumeMounts":[{"mountPath":"/opt/app/config.yaml","name":"config","readOnly":true,"subPath":"config.yaml"}]}],"nodeSelector":null,"serviceAccountName":"polaris-dashboard","tolerations":null,"volumes":[{"configMap":{"name":"polaris"},"name":"config"}]}}}}
    creationTimestamp: "2020-02-27T04:07:09Z"
    generation: 2
    labels:
      app: polaris
      component: dashboard
    name: polaris-dashboard
    namespace: polaris
    resourceVersion: "33302981"
    selfLink: /apis/apps/v1/namespaces/polaris/deployments/polaris-dashboard
    uid: 9ffb2e13-5916-11ea-8ab4-42010a8000a7
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: polaris
        component: dashboard
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          checksum/config: 8aa5a565fba7a2db98d46752087de8c1dcc83b70cd762c5829d5ba01270d54a2
        creationTimestamp: null
        labels:
          app: polaris
          component: dashboard
      spec:
        containers:
        - command:
          - polaris
          - --dashboard
          - --config
          - /opt/app/config.yaml
          image: quay.io/fairwinds/polaris:0.6
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          name: dashboard
          ports:
          - containerPort: 8080
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 50m
              memory: 128Mi
            requests:
              cpu: 10m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/app/config.yaml
            name: config
            readOnly: true
            subPath: config.yaml
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: polaris-dashboard
        serviceAccountName: polaris-dashboard
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: polaris
          name: config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-02-27T04:07:09Z"
      lastUpdateTime: "2020-03-21T23:58:02Z"
      message: ReplicaSet "polaris-dashboard-8554786c49" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-03-31T18:43:08Z"
      lastUpdateTime: "2020-03-31T18:43:08Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"name":"carts"},"name":"carts","namespace":"sock-shop"},"spec":{"replicas":1,"template":{"metadata":{"labels":{"name":"carts"}},"spec":{"containers":[{"env":[{"name":"ZIPKIN","value":"zipkin.jaeger.svc.cluster.local"},{"name":"JAVA_OPTS","value":"-Xms64m -Xmx128m -XX:PermSize=32m -XX:MaxPermSize=64m -XX:+UseG1GC -Djava.security.egd=file:/dev/urandom"}],"image":"weaveworksdemos/carts:0.4.8","name":"carts","ports":[{"containerPort":80}],"securityContext":{"capabilities":{"add":["NET_BIND_SERVICE"],"drop":["all"]},"readOnlyRootFilesystem":true,"runAsNonRoot":true,"runAsUser":10001},"volumeMounts":[{"mountPath":"/tmp","name":"tmp-volume"}]}],"nodeSelector":{"beta.kubernetes.io/os":"linux"},"volumes":[{"emptyDir":{"medium":"Memory"},"name":"tmp-volume"}]}}}}
    creationTimestamp: "2020-01-17T21:24:51Z"
    generation: 1
    labels:
      name: carts
    name: carts
    namespace: sock-shop
    resourceVersion: "33571840"
    selfLink: /apis/apps/v1/namespaces/sock-shop/deployments/carts
    uid: cc4244d8-396f-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        name: carts
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: carts
      spec:
        containers:
        - env:
          - name: ZIPKIN
            value: zipkin.jaeger.svc.cluster.local
          - name: JAVA_OPTS
            value: -Xms64m -Xmx128m -XX:PermSize=32m -XX:MaxPermSize=64m -XX:+UseG1GC
              -Djava.security.egd=file:/dev/urandom
          image: weaveworksdemos/carts:0.4.8
          imagePullPolicy: IfNotPresent
          name: carts
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: tmp-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-17T21:24:51Z"
      lastUpdateTime: "2020-01-17T21:24:51Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"name":"carts-db"},"name":"carts-db","namespace":"sock-shop"},"spec":{"replicas":1,"template":{"metadata":{"labels":{"name":"carts-db"}},"spec":{"containers":[{"image":"mongo","name":"carts-db","ports":[{"containerPort":27017,"name":"mongo"}],"securityContext":{"capabilities":{"add":["CHOWN","SETGID","SETUID"],"drop":["all"]},"readOnlyRootFilesystem":true},"volumeMounts":[{"mountPath":"/tmp","name":"tmp-volume"}]}],"nodeSelector":{"beta.kubernetes.io/os":"linux"},"volumes":[{"emptyDir":{"medium":"Memory"},"name":"tmp-volume"}]}}}}
    creationTimestamp: "2020-01-17T21:24:51Z"
    generation: 1
    labels:
      name: carts-db
    name: carts-db
    namespace: sock-shop
    resourceVersion: "33571748"
    selfLink: /apis/apps/v1/namespaces/sock-shop/deployments/carts-db
    uid: cbf7bfd6-396f-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        name: carts-db
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: carts-db
      spec:
        containers:
        - image: mongo
          imagePullPolicy: Always
          name: carts-db
          ports:
          - containerPort: 27017
            name: mongo
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              - SETGID
              - SETUID
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: tmp-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-17T21:24:51Z"
      lastUpdateTime: "2020-01-17T21:24:51Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"name":"catalogue"},"name":"catalogue","namespace":"sock-shop"},"spec":{"replicas":1,"template":{"metadata":{"labels":{"name":"catalogue"}},"spec":{"containers":[{"image":"weaveworksdemos/catalogue:0.3.5","name":"catalogue","ports":[{"containerPort":80}],"securityContext":{"capabilities":{"add":["NET_BIND_SERVICE"],"drop":["all"]},"readOnlyRootFilesystem":true,"runAsNonRoot":true,"runAsUser":10001}}],"nodeSelector":{"beta.kubernetes.io/os":"linux"}}}}}
    creationTimestamp: "2020-01-17T21:24:52Z"
    generation: 1
    labels:
      name: catalogue
    name: catalogue
    namespace: sock-shop
    resourceVersion: "33571678"
    selfLink: /apis/apps/v1/namespaces/sock-shop/deployments/catalogue
    uid: ccd0c82a-396f-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        name: catalogue
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: catalogue
      spec:
        containers:
        - image: weaveworksdemos/catalogue:0.3.5
          imagePullPolicy: IfNotPresent
          name: catalogue
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-17T21:24:52Z"
      lastUpdateTime: "2020-01-17T21:24:52Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"name":"catalogue-db"},"name":"catalogue-db","namespace":"sock-shop"},"spec":{"replicas":1,"template":{"metadata":{"labels":{"name":"catalogue-db"}},"spec":{"containers":[{"env":[{"name":"MYSQL_ROOT_PASSWORD","value":"fake_password"},{"name":"MYSQL_DATABASE","value":"socksdb"}],"image":"weaveworksdemos/catalogue-db:0.3.0","name":"catalogue-db","ports":[{"containerPort":3306,"name":"mysql"}]}],"nodeSelector":{"beta.kubernetes.io/os":"linux"}}}}}
    creationTimestamp: "2020-01-17T21:24:52Z"
    generation: 1
    labels:
      name: catalogue-db
    name: catalogue-db
    namespace: sock-shop
    resourceVersion: "33571911"
    selfLink: /apis/apps/v1/namespaces/sock-shop/deployments/catalogue-db
    uid: cc8f5088-396f-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        name: catalogue-db
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: catalogue-db
      spec:
        containers:
        - env:
          - name: MYSQL_ROOT_PASSWORD
            value: fake_password
          - name: MYSQL_DATABASE
            value: socksdb
          image: weaveworksdemos/catalogue-db:0.3.0
          imagePullPolicy: IfNotPresent
          name: catalogue-db
          ports:
          - containerPort: 3306
            name: mysql
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-17T21:24:52Z"
      lastUpdateTime: "2020-01-17T21:24:52Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"name":"front-end","namespace":"sock-shop"},"spec":{"replicas":1,"template":{"metadata":{"labels":{"name":"front-end"}},"spec":{"containers":[{"image":"weaveworksdemos/front-end:0.3.12","name":"front-end","ports":[{"containerPort":8079}],"resources":{"requests":{"cpu":"100m","memory":"100Mi"}},"securityContext":{"capabilities":{"drop":["all"]},"readOnlyRootFilesystem":true,"runAsNonRoot":true,"runAsUser":10001}}],"nodeSelector":{"beta.kubernetes.io/os":"linux"}}}}}
    creationTimestamp: "2020-01-17T21:24:53Z"
    generation: 1
    labels:
      name: front-end
    name: front-end
    namespace: sock-shop
    resourceVersion: "33571874"
    selfLink: /apis/apps/v1/namespaces/sock-shop/deployments/front-end
    uid: cd15ff05-396f-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        name: front-end
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: front-end
      spec:
        containers:
        - image: weaveworksdemos/front-end:0.3.12
          imagePullPolicy: IfNotPresent
          name: front-end
          ports:
          - containerPort: 8079
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            capabilities:
              drop:
              - all
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-17T21:24:53Z"
      lastUpdateTime: "2020-01-17T21:24:53Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"name":"orders"},"name":"orders","namespace":"sock-shop"},"spec":{"replicas":1,"template":{"metadata":{"labels":{"name":"orders"}},"spec":{"containers":[{"env":[{"name":"ZIPKIN","value":"zipkin.jaeger.svc.cluster.local"},{"name":"JAVA_OPTS","value":"-Xms64m -Xmx128m -XX:PermSize=32m -XX:MaxPermSize=64m -XX:+UseG1GC -Djava.security.egd=file:/dev/urandom"}],"image":"weaveworksdemos/orders:0.4.7","name":"orders","ports":[{"containerPort":80}],"securityContext":{"capabilities":{"add":["NET_BIND_SERVICE"],"drop":["all"]},"readOnlyRootFilesystem":true,"runAsNonRoot":true,"runAsUser":10001},"volumeMounts":[{"mountPath":"/tmp","name":"tmp-volume"}]}],"nodeSelector":{"beta.kubernetes.io/os":"linux"},"volumes":[{"emptyDir":{"medium":"Memory"},"name":"tmp-volume"}]}}}}
    creationTimestamp: "2020-01-17T21:24:54Z"
    generation: 1
    labels:
      name: orders
    name: orders
    namespace: sock-shop
    resourceVersion: "33571764"
    selfLink: /apis/apps/v1/namespaces/sock-shop/deployments/orders
    uid: cd9a8d49-396f-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        name: orders
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: orders
      spec:
        containers:
        - env:
          - name: ZIPKIN
            value: zipkin.jaeger.svc.cluster.local
          - name: JAVA_OPTS
            value: -Xms64m -Xmx128m -XX:PermSize=32m -XX:MaxPermSize=64m -XX:+UseG1GC
              -Djava.security.egd=file:/dev/urandom
          image: weaveworksdemos/orders:0.4.7
          imagePullPolicy: IfNotPresent
          name: orders
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: tmp-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-17T21:24:54Z"
      lastUpdateTime: "2020-01-17T21:24:54Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"name":"orders-db"},"name":"orders-db","namespace":"sock-shop"},"spec":{"replicas":1,"template":{"metadata":{"labels":{"name":"orders-db"}},"spec":{"containers":[{"image":"mongo","name":"orders-db","ports":[{"containerPort":27017,"name":"mongo"}],"securityContext":{"capabilities":{"add":["CHOWN","SETGID","SETUID"],"drop":["all"]},"readOnlyRootFilesystem":true},"volumeMounts":[{"mountPath":"/tmp","name":"tmp-volume"}]}],"nodeSelector":{"beta.kubernetes.io/os":"linux"},"volumes":[{"emptyDir":{"medium":"Memory"},"name":"tmp-volume"}]}}}}
    creationTimestamp: "2020-01-17T21:24:53Z"
    generation: 1
    labels:
      name: orders-db
    name: orders-db
    namespace: sock-shop
    resourceVersion: "33571775"
    selfLink: /apis/apps/v1/namespaces/sock-shop/deployments/orders-db
    uid: cd582567-396f-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        name: orders-db
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: orders-db
      spec:
        containers:
        - image: mongo
          imagePullPolicy: Always
          name: orders-db
          ports:
          - containerPort: 27017
            name: mongo
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              - SETGID
              - SETUID
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: tmp-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-17T21:24:53Z"
      lastUpdateTime: "2020-01-17T21:24:53Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"name":"payment"},"name":"payment","namespace":"sock-shop"},"spec":{"replicas":1,"template":{"metadata":{"labels":{"name":"payment"}},"spec":{"containers":[{"image":"weaveworksdemos/payment:0.4.3","name":"payment","ports":[{"containerPort":80}],"securityContext":{"capabilities":{"add":["NET_BIND_SERVICE"],"drop":["all"]},"readOnlyRootFilesystem":true,"runAsNonRoot":true,"runAsUser":10001}}],"nodeSelector":{"beta.kubernetes.io/os":"linux"}}}}}
    creationTimestamp: "2020-01-17T21:24:54Z"
    generation: 1
    labels:
      name: payment
    name: payment
    namespace: sock-shop
    resourceVersion: "33571919"
    selfLink: /apis/apps/v1/namespaces/sock-shop/deployments/payment
    uid: cddd5e77-396f-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        name: payment
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: payment
      spec:
        containers:
        - image: weaveworksdemos/payment:0.4.3
          imagePullPolicy: IfNotPresent
          name: payment
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-17T21:24:54Z"
      lastUpdateTime: "2020-01-17T21:24:54Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"name":"queue-master"},"name":"queue-master","namespace":"sock-shop"},"spec":{"replicas":1,"template":{"metadata":{"labels":{"name":"queue-master"}},"spec":{"containers":[{"image":"weaveworksdemos/queue-master:0.3.1","name":"queue-master","ports":[{"containerPort":80}]}],"nodeSelector":{"beta.kubernetes.io/os":"linux"}}}}}
    creationTimestamp: "2020-01-17T21:24:55Z"
    generation: 1
    labels:
      name: queue-master
    name: queue-master
    namespace: sock-shop
    resourceVersion: "33571892"
    selfLink: /apis/apps/v1/namespaces/sock-shop/deployments/queue-master
    uid: ce209a3d-396f-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        name: queue-master
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: queue-master
      spec:
        containers:
        - image: weaveworksdemos/queue-master:0.3.1
          imagePullPolicy: IfNotPresent
          name: queue-master
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-17T21:24:55Z"
      lastUpdateTime: "2020-01-17T21:24:55Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"name":"rabbitmq"},"name":"rabbitmq","namespace":"sock-shop"},"spec":{"replicas":1,"template":{"metadata":{"labels":{"name":"rabbitmq"}},"spec":{"containers":[{"image":"rabbitmq:3.6.8","name":"rabbitmq","ports":[{"containerPort":5672}],"securityContext":{"capabilities":{"add":["CHOWN","SETGID","SETUID","DAC_OVERRIDE"],"drop":["all"]},"readOnlyRootFilesystem":true}}],"nodeSelector":{"beta.kubernetes.io/os":"linux"}}}}}
    creationTimestamp: "2020-01-17T21:24:55Z"
    generation: 1
    labels:
      name: rabbitmq
    name: rabbitmq
    namespace: sock-shop
    resourceVersion: "33571646"
    selfLink: /apis/apps/v1/namespaces/sock-shop/deployments/rabbitmq
    uid: ce751d94-396f-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        name: rabbitmq
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: rabbitmq
      spec:
        containers:
        - image: rabbitmq:3.6.8
          imagePullPolicy: IfNotPresent
          name: rabbitmq
          ports:
          - containerPort: 5672
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              - SETGID
              - SETUID
              - DAC_OVERRIDE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-17T21:24:55Z"
      lastUpdateTime: "2020-01-17T21:24:55Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"name":"shipping"},"name":"shipping","namespace":"sock-shop"},"spec":{"replicas":1,"template":{"metadata":{"labels":{"name":"shipping"}},"spec":{"containers":[{"env":[{"name":"ZIPKIN","value":"zipkin.jaeger.svc.cluster.local"},{"name":"JAVA_OPTS","value":"-Xms64m -Xmx128m -XX:PermSize=32m -XX:MaxPermSize=64m -XX:+UseG1GC -Djava.security.egd=file:/dev/urandom"}],"image":"weaveworksdemos/shipping:0.4.8","name":"shipping","ports":[{"containerPort":80}],"securityContext":{"capabilities":{"add":["NET_BIND_SERVICE"],"drop":["all"]},"readOnlyRootFilesystem":true,"runAsNonRoot":true,"runAsUser":10001},"volumeMounts":[{"mountPath":"/tmp","name":"tmp-volume"}]}],"nodeSelector":{"beta.kubernetes.io/os":"linux"},"volumes":[{"emptyDir":{"medium":"Memory"},"name":"tmp-volume"}]}}}}
    creationTimestamp: "2020-01-17T21:24:56Z"
    generation: 1
    labels:
      name: shipping
    name: shipping
    namespace: sock-shop
    resourceVersion: "33571887"
    selfLink: /apis/apps/v1/namespaces/sock-shop/deployments/shipping
    uid: cebd0063-396f-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        name: shipping
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: shipping
      spec:
        containers:
        - env:
          - name: ZIPKIN
            value: zipkin.jaeger.svc.cluster.local
          - name: JAVA_OPTS
            value: -Xms64m -Xmx128m -XX:PermSize=32m -XX:MaxPermSize=64m -XX:+UseG1GC
              -Djava.security.egd=file:/dev/urandom
          image: weaveworksdemos/shipping:0.4.8
          imagePullPolicy: IfNotPresent
          name: shipping
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: tmp-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-17T21:24:56Z"
      lastUpdateTime: "2020-01-17T21:24:56Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"name":"user"},"name":"user","namespace":"sock-shop"},"spec":{"replicas":1,"template":{"metadata":{"labels":{"name":"user"}},"spec":{"containers":[{"env":[{"name":"MONGO_HOST","value":"user-db:27017"}],"image":"weaveworksdemos/user:0.4.7","name":"user","ports":[{"containerPort":80}],"securityContext":{"capabilities":{"add":["NET_BIND_SERVICE"],"drop":["all"]},"readOnlyRootFilesystem":true,"runAsNonRoot":true,"runAsUser":10001}}],"nodeSelector":{"beta.kubernetes.io/os":"linux"}}}}}
    creationTimestamp: "2020-01-17T21:24:56Z"
    generation: 1
    labels:
      name: user
    name: user
    namespace: sock-shop
    resourceVersion: "33571851"
    selfLink: /apis/apps/v1/namespaces/sock-shop/deployments/user
    uid: cf4394d3-396f-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        name: user
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: user
      spec:
        containers:
        - env:
          - name: MONGO_HOST
            value: user-db:27017
          image: weaveworksdemos/user:0.4.7
          imagePullPolicy: IfNotPresent
          name: user
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-17T21:24:57Z"
      lastUpdateTime: "2020-01-17T21:24:57Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"name":"user-db"},"name":"user-db","namespace":"sock-shop"},"spec":{"replicas":1,"template":{"metadata":{"labels":{"name":"user-db"}},"spec":{"containers":[{"image":"weaveworksdemos/user-db:0.4.0","name":"user-db","ports":[{"containerPort":27017,"name":"mongo"}],"securityContext":{"capabilities":{"add":["CHOWN","SETGID","SETUID"],"drop":["all"]},"readOnlyRootFilesystem":true},"volumeMounts":[{"mountPath":"/tmp","name":"tmp-volume"}]}],"nodeSelector":{"beta.kubernetes.io/os":"linux"},"volumes":[{"emptyDir":{"medium":"Memory"},"name":"tmp-volume"}]}}}}
    creationTimestamp: "2020-01-17T21:24:56Z"
    generation: 1
    labels:
      name: user-db
    name: user-db
    namespace: sock-shop
    resourceVersion: "33571789"
    selfLink: /apis/apps/v1/namespaces/sock-shop/deployments/user-db
    uid: cefe8b27-396f-11ea-b115-42010a8001d6
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 2147483647
    selector:
      matchLabels:
        name: user-db
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: user-db
      spec:
        containers:
        - image: weaveworksdemos/user-db:0.4.0
          imagePullPolicy: IfNotPresent
          name: user-db
          ports:
          - containerPort: 27017
            name: mongo
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              - SETGID
              - SETUID
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: tmp-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2020-01-17T21:24:56Z"
      lastUpdateTime: "2020-01-17T21:24:56Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-02-02T23:53:37Z"
    generation: 2
    labels:
      app: book-app
      pod-template-hash: 7676cf7c64
    name: book-app-7676cf7c64
    namespace: book
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: book-app
      uid: 44d4077a-34bb-11ea-9cdc-42010a8001cf
    resourceVersion: "29696833"
    selfLink: /apis/apps/v1/namespaces/book/replicasets/book-app-7676cf7c64
    uid: 3ae4b49f-4617-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: book-app
        pod-template-hash: 7676cf7c64
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: book-app
          pod-template-hash: 7676cf7c64
      spec:
        containers:
        - command:
          - /cloud_sql_proxy
          - -instances=my-project:us-central1:mysql-db-name=tcp:3306
          - -credential_file=/var/secrets/google/service-key.json
          image: gcr.io/cloudsql-docker/gce-proxy:1.11
          imagePullPolicy: IfNotPresent
          name: cloudsql-proxy
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            runAsUser: 2
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/secrets/google
            name: google-cloud-key
            readOnly: true
        - env:
          - name: MY_ENV_VAR_2
            value: abcd1234
          envFrom:
          - configMapRef:
              name: my-config-1
          image: nginx:1.16
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - containerPort: 3000
            name: default
            protocol: TCP
          resources:
            limits:
              cpu: 10m
            requests:
              cpu: 10m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: google-cloud-key
          secret:
            defaultMode: 420
            secretName: gprod-addr-main-app
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T21:42:30Z"
    generation: 2
    labels:
      app: book-app
      pod-template-hash: b7c586fd6
    name: book-app-b7c586fd6
    namespace: book
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: book-app
      uid: 44d4077a-34bb-11ea-9cdc-42010a8001cf
    resourceVersion: "29696835"
    selfLink: /apis/apps/v1/namespaces/book/replicasets/book-app-b7c586fd6
    uid: 44d50440-34bb-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: book-app
        pod-template-hash: b7c586fd6
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: book-app
          pod-template-hash: b7c586fd6
      spec:
        containers:
        - command:
          - /cloud_sql_proxy
          - -instances=my-project:us-central1:mysql-db-name=tcp:3306
          - -credential_file=/var/secrets/google/service-key.json
          image: gcr.io/cloudsql-docker/gce-proxy:1.11
          imagePullPolicy: IfNotPresent
          name: cloudsql-proxy
          resources: {}
          securityContext:
            allowPrivilegeEscalation: false
            runAsUser: 2
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/secrets/google
            name: google-cloud-key
            readOnly: true
        - env:
          - name: MY_ENV_VAR_2
            value: abcd1234
          envFrom:
          - configMapRef:
              name: my-config-1
          image: nginx:1.16
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - containerPort: 3000
            name: default
            protocol: TCP
          resources:
            limits:
              cpu: 100m
            requests:
              cpu: 50m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: google-cloud-key
          secret:
            defaultMode: 420
            secretName: gprod-addr-main-app
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T21:42:29Z"
    generation: 2
    labels:
      app: book-web
      pod-template-hash: 765d65995c
    name: book-web-765d65995c
    namespace: book
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: book-web
      uid: 4430eec8-34bb-11ea-9cdc-42010a8001cf
    resourceVersion: "9207457"
    selfLink: /apis/apps/v1/namespaces/book/replicasets/book-web-765d65995c
    uid: 4432a1b2-34bb-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: book-web
        pod-template-hash: 765d65995c
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: book-web
          pod-template-hash: 765d65995c
      spec:
        containers:
        - env:
          - name: MY_ENV_VAR_1
            value: abcd1234
          envFrom:
          - configMapRef:
              name: my-config-1
          image: nginx:1.16
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - containerPort: 4000
            name: default
            protocol: TCP
          resources:
            limits:
              cpu: 100m
            requests:
              cpu: 50m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
      deployment.kubernetes.io/revision-history: "3"
    creationTimestamp: "2020-03-02T03:43:42Z"
    generation: 4
    labels:
      app: book-web
      pod-template-hash: 7b4b9c5cc5
    name: book-web-7b4b9c5cc5
    namespace: book
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: book-web
      uid: 4430eec8-34bb-11ea-9cdc-42010a8001cf
    resourceVersion: "21413525"
    selfLink: /apis/apps/v1/namespaces/book/replicasets/book-web-7b4b9c5cc5
    uid: 0300fe7d-5c38-11ea-8ab4-42010a8000a7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: book-web
        pod-template-hash: 7b4b9c5cc5
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: book-web
          pod-template-hash: 7b4b9c5cc5
      spec:
        containers:
        - env:
          - name: MY_ENV_VAR_1
            value: abcd1234
          envFrom:
          - configMapRef:
              name: my-config-1
          image: nginx:1.16
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - containerPort: 3500
            name: default
            protocol: TCP
          resources:
            limits:
              cpu: 15m
            requests:
              cpu: 10m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "0"
      deployment.kubernetes.io/max-replicas: "0"
      deployment.kubernetes.io/revision: "6"
      deployment.kubernetes.io/revision-history: 2,4
    creationTimestamp: "2020-02-02T23:53:23Z"
    generation: 6
    labels:
      app: book-web
      pod-template-hash: b476b75d7
    name: book-web-b476b75d7
    namespace: book
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: book-web
      uid: 4430eec8-34bb-11ea-9cdc-42010a8001cf
    resourceVersion: "29699482"
    selfLink: /apis/apps/v1/namespaces/book/replicasets/book-web-b476b75d7
    uid: 32db37d4-4617-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: book-web
        pod-template-hash: b476b75d7
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: book-web
          pod-template-hash: b476b75d7
      spec:
        containers:
        - env:
          - name: MY_ENV_VAR_1
            value: abcd1234
          envFrom:
          - configMapRef:
              name: my-config-1
          image: nginx:1.16
          imagePullPolicy: IfNotPresent
          name: nginx
          ports:
          - containerPort: 4000
            name: default
            protocol: TCP
          resources:
            limits:
              cpu: 15m
            requests:
              cpu: 10m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 6
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: cainjector
      app.kubernetes.io/instance: gitlab
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cainjector
      helm.sh/chart: cainjector-v0.10.1
      pod-template-hash: 5d757b9fdd
    name: gitlab-cainjector-5d757b9fdd
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-cainjector
      uid: 995eb501-398b-11ea-b115-42010a8001d6
    resourceVersion: "37202422"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-cainjector-5d757b9fdd
    uid: 99716a60-398b-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: cainjector
        app.kubernetes.io/instance: gitlab
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: cainjector
        pod-template-hash: 5d757b9fdd
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cainjector
          app.kubernetes.io/instance: gitlab
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cainjector
          helm.sh/chart: cainjector-v0.10.1
          pod-template-hash: 5d757b9fdd
      spec:
        containers:
        - args:
          - --v=2
          - --leader-election-namespace=$(POD_NAMESPACE)
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-cainjector:v0.10.1
          imagePullPolicy: IfNotPresent
          name: cainjector
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: gitlab-cainjector
        serviceAccountName: gitlab-cainjector
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: cert-manager
      app.kubernetes.io/instance: gitlab
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: cert-manager
      helm.sh/chart: certmanager-v0.10.1
      pod-template-hash: 5ffcc7f99f
    name: gitlab-cert-manager-5ffcc7f99f
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-cert-manager
      uid: 99614a55-398b-11ea-b115-42010a8001d6
    resourceVersion: "37198416"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-cert-manager-5ffcc7f99f
    uid: 99816455-398b-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: cert-manager
        app.kubernetes.io/instance: gitlab
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: cert-manager
        pod-template-hash: 5ffcc7f99f
    template:
      metadata:
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "9402"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: cert-manager
          app.kubernetes.io/instance: gitlab
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: cert-manager
          helm.sh/chart: certmanager-v0.10.1
          pod-template-hash: 5ffcc7f99f
      spec:
        containers:
        - args:
          - --v=2
          - --cluster-resource-namespace=$(POD_NAMESPACE)
          - --leader-election-namespace=$(POD_NAMESPACE)
          - --webhook-namespace=$(POD_NAMESPACE)
          - --webhook-ca-secret=gitlab-cert-manager-webhook-ca
          - --webhook-serving-secret=gitlab-cert-manager-webhook-tls
          - --webhook-dns-names=gitlab-cert-manager-webhook,gitlab-cert-manager-webhook.gitlab,gitlab-cert-manager-webhook.gitlab.svc
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-controller:v0.10.1
          imagePullPolicy: IfNotPresent
          name: certmanager
          ports:
          - containerPort: 9402
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: gitlab-cert-manager
        serviceAccountName: gitlab-cert-manager
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: webhook
      app.kubernetes.io/instance: gitlab
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: webhook
      helm.sh/chart: certmanager-v0.10.1
      pod-template-hash: 76d9d9cc69
    name: gitlab-cert-manager-webhook-76d9d9cc69
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-cert-manager-webhook
      uid: 99637e48-398b-11ea-b115-42010a8001d6
    resourceVersion: "33571689"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-cert-manager-webhook-76d9d9cc69
    uid: 9992e00f-398b-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: webhook
        app.kubernetes.io/instance: gitlab
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: webhook
        pod-template-hash: 76d9d9cc69
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: webhook
          app.kubernetes.io/instance: gitlab
          app.kubernetes.io/managed-by: Helm
          app.kubernetes.io/name: webhook
          helm.sh/chart: certmanager-v0.10.1
          pod-template-hash: 76d9d9cc69
      spec:
        containers:
        - args:
          - --v=2
          - --secure-port=6443
          - --tls-cert-file=/certs/tls.crt
          - --tls-private-key-file=/certs/tls.key
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/jetstack/cert-manager-webhook:v0.10.1
          imagePullPolicy: IfNotPresent
          name: certmanager
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /certs
            name: certs
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: gitlab-cert-manager-webhook
        serviceAccountName: gitlab-cert-manager-webhook
        terminationGracePeriodSeconds: 30
        volumes:
        - name: certs
          secret:
            defaultMode: 420
            secretName: gitlab-cert-manager-webhook-tls
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: gitlab-exporter
      pod-template-hash: 868bc56dd8
      release: gitlab
    name: gitlab-gitlab-exporter-868bc56dd8
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-gitlab-exporter
      uid: 9963d015-398b-11ea-b115-42010a8001d6
    resourceVersion: "33303021"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-gitlab-exporter-868bc56dd8
    uid: 9981cadc-398b-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: gitlab-exporter
        pod-template-hash: 868bc56dd8
        release: gitlab
    template:
      metadata:
        annotations:
          checksum/config: 1aebe5270e7534dc7510737e5fcfb26b9b6ca0f289d95563a9cdddae80ad7e24
          prometheus.io/path: /metrics
          prometheus.io/port: "9168"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: gitlab-exporter
          pod-template-hash: 868bc56dd8
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: gitlab-exporter
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab-exporter/templates
          - name: CONFIG_DIRECTORY
            value: /etc/gitlab-exporter
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-exporter:5.1.0
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -f 'gitlab-exporter'
          livenessProbe:
            exec:
              command:
              - pgrep
              - -f
              - gitlab-exporter
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: gitlab-exporter
          ports:
          - containerPort: 9168
            name: gitlab-exporter
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - pgrep
              - -f
              - gitlab-exporter
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 50m
              memory: 100M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab-exporter/templates/gitlab-exporter.yml.erb
            name: gitlab-exporter-config
            subPath: gitlab-exporter.yml.erb
          - mountPath: /etc/gitlab
            name: gitlab-exporter-secrets
            readOnly: true
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: gitlab-exporter-config
            readOnly: true
          - mountPath: /init-config
            name: init-gitlab-exporter-secrets
            readOnly: true
          - mountPath: /init-secrets
            name: gitlab-exporter-secrets
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: gitlab-gitlab-exporter
          name: gitlab-exporter-config
        - name: init-gitlab-exporter-secrets
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: postgres-password
                  path: postgres/psql-password
                name: gitlab-postgresql-password
            - secret:
                items:
                - key: secret
                  path: redis/password
                name: gitlab-redis-secret
        - emptyDir:
            medium: Memory
          name: gitlab-exporter-secrets
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: gitlab-gitlab-runner
      chart: gitlab-runner-0.12.0
      heritage: Helm
      pod-template-hash: 65ff67bdd8
      release: gitlab
    name: gitlab-gitlab-runner-65ff67bdd8
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-gitlab-runner
      uid: 9960a25d-398b-11ea-b115-42010a8001d6
    resourceVersion: "37201638"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-gitlab-runner-65ff67bdd8
    uid: 997138d9-398b-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: gitlab-gitlab-runner
        pod-template-hash: 65ff67bdd8
    template:
      metadata:
        annotations:
          checksum/configmap: b6037b659634c6ed64ec4ba6a1ff81db0af76d05c0b3be594c981a6274cb551d
          checksum/secrets: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
          prometheus.io/port: "9252"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: gitlab-gitlab-runner
          chart: gitlab-runner-0.12.0
          heritage: Helm
          pod-template-hash: 65ff67bdd8
          release: gitlab
      spec:
        containers:
        - command:
          - /bin/bash
          - /scripts/entrypoint
          env:
          - name: CI_SERVER_URL
            value: https://gitlab.example.com
          - name: CLONE_URL
          - name: RUNNER_REQUEST_CONCURRENCY
            value: "1"
          - name: RUNNER_EXECUTOR
            value: kubernetes
          - name: REGISTER_LOCKED
            value: "false"
          - name: RUNNER_TAG_LIST
          - name: RUNNER_OUTPUT_LIMIT
            value: "4096"
          - name: KUBERNETES_IMAGE
            value: ubuntu:16.04
          - name: KUBERNETES_NAMESPACE
            value: gitlab
          - name: KUBERNETES_POLL_TIMEOUT
            value: "180"
          - name: KUBERNETES_CPU_LIMIT
          - name: KUBERNETES_MEMORY_LIMIT
          - name: KUBERNETES_CPU_REQUEST
          - name: KUBERNETES_MEMORY_REQUEST
          - name: KUBERNETES_SERVICE_ACCOUNT
          - name: KUBERNETES_SERVICE_CPU_LIMIT
          - name: KUBERNETES_SERVICE_MEMORY_LIMIT
          - name: KUBERNETES_SERVICE_CPU_REQUEST
          - name: KUBERNETES_SERVICE_MEMORY_REQUEST
          - name: KUBERNETES_HELPER_CPU_LIMIT
          - name: KUBERNETES_HELPER_MEMORY_LIMIT
          - name: KUBERNETES_HELPER_CPU_REQUEST
          - name: KUBERNETES_HELPER_MEMORY_REQUEST
          - name: KUBERNETES_HELPER_IMAGE
          - name: KUBERNETES_PULL_POLICY
          - name: CACHE_TYPE
            value: s3
          - name: CACHE_PATH
            value: gitlab-runner
          - name: CACHE_SHARED
            value: "true"
          - name: CACHE_S3_SERVER_ADDRESS
            value: minio.example.com
          - name: CACHE_S3_BUCKET_NAME
            value: runner-cache
          - name: CACHE_S3_BUCKET_LOCATION
            value: us-east-1
          image: gitlab/gitlab-runner:alpine-v12.6.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/bash
              - /scripts/check-live
            failureThreshold: 3
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: gitlab-gitlab-runner
          ports:
          - containerPort: 9252
            name: metrics
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /usr/bin/pgrep
              - gitlab.*runner
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /secrets
            name: runner-secrets
          - mountPath: /home/gitlab-runner/.gitlab-runner
            name: etc-gitlab-runner
          - mountPath: /scripts
            name: scripts
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - /config/configure
          env:
          - name: CI_SERVER_URL
            value: https://gitlab.example.com
          - name: CLONE_URL
          - name: RUNNER_REQUEST_CONCURRENCY
            value: "1"
          - name: RUNNER_EXECUTOR
            value: kubernetes
          - name: REGISTER_LOCKED
            value: "false"
          - name: RUNNER_TAG_LIST
          - name: RUNNER_OUTPUT_LIMIT
            value: "4096"
          - name: KUBERNETES_IMAGE
            value: ubuntu:16.04
          - name: KUBERNETES_NAMESPACE
            value: gitlab
          - name: KUBERNETES_POLL_TIMEOUT
            value: "180"
          - name: KUBERNETES_CPU_LIMIT
          - name: KUBERNETES_MEMORY_LIMIT
          - name: KUBERNETES_CPU_REQUEST
          - name: KUBERNETES_MEMORY_REQUEST
          - name: KUBERNETES_SERVICE_ACCOUNT
          - name: KUBERNETES_SERVICE_CPU_LIMIT
          - name: KUBERNETES_SERVICE_MEMORY_LIMIT
          - name: KUBERNETES_SERVICE_CPU_REQUEST
          - name: KUBERNETES_SERVICE_MEMORY_REQUEST
          - name: KUBERNETES_HELPER_CPU_LIMIT
          - name: KUBERNETES_HELPER_MEMORY_LIMIT
          - name: KUBERNETES_HELPER_CPU_REQUEST
          - name: KUBERNETES_HELPER_MEMORY_REQUEST
          - name: KUBERNETES_HELPER_IMAGE
          - name: KUBERNETES_PULL_POLICY
          - name: CACHE_TYPE
            value: s3
          - name: CACHE_PATH
            value: gitlab-runner
          - name: CACHE_SHARED
            value: "true"
          - name: CACHE_S3_SERVER_ADDRESS
            value: minio.example.com
          - name: CACHE_S3_BUCKET_NAME
            value: runner-cache
          - name: CACHE_S3_BUCKET_LOCATION
            value: us-east-1
          image: gitlab/gitlab-runner:alpine-v12.6.0
          imagePullPolicy: IfNotPresent
          name: configure
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /secrets
            name: runner-secrets
          - mountPath: /config
            name: scripts
            readOnly: true
          - mountPath: /init-secrets
            name: init-runner-secrets
            readOnly: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65533
          runAsUser: 100
        serviceAccount: gitlab-gitlab-runner
        serviceAccountName: gitlab-gitlab-runner
        terminationGracePeriodSeconds: 3600
        volumes:
        - emptyDir:
            medium: Memory
          name: runner-secrets
        - emptyDir:
            medium: Memory
          name: etc-gitlab-runner
        - name: init-runner-secrets
          projected:
            defaultMode: 420
            sources:
            - secret:
                name: gitlab-minio-secret
            - secret:
                items:
                - key: runner-registration-token
                  path: runner-registration-token
                - key: runner-token
                  path: runner-token
                name: gitlab-gitlab-runner-secret
        - configMap:
            defaultMode: 420
            name: gitlab-gitlab-runner
          name: scripts
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 85
    labels:
      app: gitlab-shell
      pod-template-hash: 7b89877f4c
      release: gitlab
    name: gitlab-gitlab-shell-7b89877f4c
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-gitlab-shell
      uid: 99655ee5-398b-11ea-b115-42010a8001d6
    resourceVersion: "33574633"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-gitlab-shell-7b89877f4c
    uid: 99a6c2f9-398b-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: gitlab-shell
        pod-template-hash: 7b89877f4c
        release: gitlab
    template:
      metadata:
        annotations:
          checksum/config: ba8fbd9408a25a13f7c9d3682c4bed3a4e3645216c471e54e71035dd3c0d5bad
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: gitlab-shell
          pod-template-hash: 7b89877f4c
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: gitlab-shell
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /etc/gitlab-shell
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab-shell
          - name: KEYS_DIRECTORY
            value: /etc/gitlab-secrets/ssh
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-shell:v10.3.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /scripts/healthcheck
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: gitlab-shell
          ports:
          - containerPort: 2222
            name: ssh
            protocol: TCP
          resources:
            requests:
              cpu: "0"
              memory: 6M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/gitlab-shell
            name: shell-config
          - mountPath: /etc/gitlab-secrets
            name: shell-secrets
            readOnly: true
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: shell-config
            readOnly: true
          - mountPath: /init-config
            name: shell-init-secrets
            readOnly: true
          - mountPath: /init-secrets
            name: shell-secrets
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: gitlab-gitlab-shell
          name: shell-config
        - name: shell-init-secrets
          projected:
            defaultMode: 288
            sources:
            - secret:
                name: gitlab-gitlab-shell-host-keys
            - secret:
                items:
                - key: secret
                  path: shell/.gitlab_shell_secret
                name: gitlab-gitlab-shell-secret
            - secret:
                items:
                - key: secret
                  path: redis/password
                name: gitlab-redis-secret
        - emptyDir:
            medium: Memory
          name: shell-secrets
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 85
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: minio
      chart: minio-0.4.3
      component: app
      heritage: Helm
      pod-template-hash: 79db4985c4
      release: gitlab
    name: gitlab-minio-79db4985c4
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-minio
      uid: 99615b81-398b-11ea-b115-42010a8001d6
    resourceVersion: "33571992"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-minio-79db4985c4
    uid: 997377fd-398b-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: minio
        component: app
        pod-template-hash: 79db4985c4
        release: gitlab
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: minio
          chart: minio-0.4.3
          component: app
          heritage: Helm
          pod-template-hash: 79db4985c4
          release: gitlab
        name: gitlab-minio
      spec:
        containers:
        - args:
          - -C
          - /tmp/.minio
          - --quiet
          - server
          - /export
          image: minio/minio:RELEASE.2017-12-28T01-21-00Z
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 9000
            timeoutSeconds: 1
          name: minio
          ports:
          - containerPort: 9000
            name: service
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /export
            name: export
          - mountPath: /tmp/.minio
            name: minio-server-config
          - mountPath: /podinfo
            name: podinfo
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: IfNotPresent
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: minio-configuration
          - mountPath: /minio
            name: minio-server-config
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - downwardAPI:
            defaultMode: 420
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.labels
              path: labels
          name: podinfo
        - name: export
          persistentVolumeClaim:
            claimName: gitlab-minio
        - name: minio-configuration
          projected:
            defaultMode: 420
            sources:
            - configMap:
                name: gitlab-minio-config-cm
            - secret:
                name: gitlab-minio-secret
        - emptyDir:
            medium: Memory
          name: minio-server-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "3"
      deployment.kubernetes.io/max-replicas: "4"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 4
    labels:
      app: nginx-ingress
      component: controller
      pod-template-hash: 5865798d7
      release: gitlab
    name: gitlab-nginx-ingress-controller-5865798d7
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-nginx-ingress-controller
      uid: 995e41f0-398b-11ea-b115-42010a8001d6
    resourceVersion: "7563766"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-nginx-ingress-controller-5865798d7
    uid: 9971096c-398b-11ea-b115-42010a8001d6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: nginx-ingress
        component: controller
        pod-template-hash: 5865798d7
        release: gitlab
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: controller
          pod-template-hash: 5865798d7
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: nginx-ingress
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - /nginx-ingress-controller
          - --default-backend-service=gitlab/gitlab-nginx-ingress-default-backend
          - --publish-service=gitlab/gitlab-nginx-ingress-controller
          - --election-id=ingress-controller-leader
          - --ingress-class=gitlab-nginx
          - --configmap=gitlab/gitlab-nginx-ingress-controller
          - --tcp-services-configmap=gitlab/gitlab-nginx-ingress-tcp
          - --watch-namespace=gitlab
          - --force-namespace-isolation
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: nginx-ingress-controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          - containerPort: 18080
            name: stats
            protocol: TCP
          - containerPort: 10254
            name: metrics
            protocol: TCP
          - containerPort: 22
            name: gitlab-shell
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            runAsUser: 33
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: gitlab-nginx-ingress
        serviceAccountName: gitlab-nginx-ingress
        terminationGracePeriodSeconds: 60
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "3"
      deployment.kubernetes.io/max-replicas: "4"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-02-04T05:21:21Z"
    generation: 3
    labels:
      app: nginx-ingress
      component: controller
      pod-template-hash: 7db754f856
      release: gitlab
    name: gitlab-nginx-ingress-controller-7db754f856
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-nginx-ingress-controller
      uid: 995e41f0-398b-11ea-b115-42010a8001d6
    resourceVersion: "37062132"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-nginx-ingress-controller-7db754f856
    uid: 2e1a3417-470e-11ea-96d3-42010a80017a
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: nginx-ingress
        component: controller
        pod-template-hash: 7db754f856
        release: gitlab
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: controller
          pod-template-hash: 7db754f856
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: nginx-ingress
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - /nginx-ingress-controller
          - --default-backend-service=gitlab/gitlab-nginx-ingress-default-backend
          - --publish-service=gitlab/gitlab-nginx-ingress-controller
          - --election-id=ingress-controller-leader
          - --ingress-class=gitlab-nginx
          - --configmap=gitlab/gitlab-nginx-ingress-controller
          - --tcp-services-configmap=gitlab/gitlab-nginx-ingress-tcp
          - --watch-namespace=gitlab
          - --force-namespace-isolation
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: nginx-ingress-controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          - containerPort: 18080
            name: stats
            protocol: TCP
          - containerPort: 10254
            name: metrics
            protocol: TCP
          - containerPort: 22
            name: gitlab-shell
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 15m
              memory: 15Mi
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            runAsUser: 33
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: gitlab-nginx-ingress
        serviceAccountName: gitlab-nginx-ingress
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 3
    fullyLabeledReplicas: 3
    observedGeneration: 3
    readyReplicas: 3
    replicas: 3
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "3"
      deployment.kubernetes.io/max-replicas: "4"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-30T04:31:53Z"
    generation: 6
    labels:
      app: nginx-ingress
      component: controller
      pod-template-hash: fcf57c789
      release: gitlab
    name: gitlab-nginx-ingress-controller-fcf57c789
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-nginx-ingress-controller
      uid: 995e41f0-398b-11ea-b115-42010a8001d6
    resourceVersion: "9738605"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-nginx-ingress-controller-fcf57c789
    uid: 70cdca1e-4319-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: nginx-ingress
        component: controller
        pod-template-hash: fcf57c789
        release: gitlab
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: controller
          pod-template-hash: fcf57c789
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: nginx-ingress
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - args:
          - /nginx-ingress-controller
          - --default-backend-service=gitlab/gitlab-nginx-ingress-default-backend
          - --publish-service=gitlab/gitlab-nginx-ingress-controller
          - --election-id=ingress-controller-leader
          - --ingress-class=gitlab-nginx
          - --configmap=gitlab/gitlab-nginx-ingress-controller
          - --tcp-services-configmap=gitlab/gitlab-nginx-ingress-tcp
          - --watch-namespace=gitlab
          - --force-namespace-isolation
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: nginx-ingress-controller
          ports:
          - containerPort: 80
            name: http
            protocol: TCP
          - containerPort: 443
            name: https
            protocol: TCP
          - containerPort: 18080
            name: stats
            protocol: TCP
          - containerPort: 10254
            name: metrics
            protocol: TCP
          - containerPort: 22
            name: gitlab-shell
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 15m
              memory: 100Mi
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - ALL
            runAsUser: 33
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: gitlab-nginx-ingress
        serviceAccountName: gitlab-nginx-ingress
        terminationGracePeriodSeconds: 60
  status:
    observedGeneration: 6
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: nginx-ingress
      component: default-backend
      pod-template-hash: 7f87d67c8
      release: gitlab
    name: gitlab-nginx-ingress-default-backend-7f87d67c8
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-nginx-ingress-default-backend
      uid: 99612109-398b-11ea-b115-42010a8001d6
    resourceVersion: "33571813"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-nginx-ingress-default-backend-7f87d67c8
    uid: 9981338c-398b-11ea-b115-42010a8001d6
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: nginx-ingress
        component: default-backend
        pod-template-hash: 7f87d67c8
        release: gitlab
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx-ingress
          component: default-backend
          pod-template-hash: 7f87d67c8
          release: gitlab
      spec:
        containers:
        - image: k8s.gcr.io/defaultbackend:1.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: nginx-ingress-default-backend
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 5m
              memory: 5Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 60
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-30T04:40:42Z"
    generation: 2
    labels:
      app: postgresql
      pod-template-hash: 556dc9c6d4
      release: gitlab
    name: gitlab-postgresql-556dc9c6d4
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-postgresql
      uid: 99622113-398b-11ea-b115-42010a8001d6
    resourceVersion: "9737715"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-postgresql-556dc9c6d4
    uid: ac03835f-431a-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: postgresql
        pod-template-hash: 556dc9c6d4
        release: gitlab
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: postgresql
          pod-template-hash: 556dc9c6d4
          release: gitlab
      spec:
        containers:
        - env:
          - name: POSTGRES_USER
            value: gitlab
          - name: PGUSER
            value: gitlab
          - name: POSTGRES_DB
            value: gitlabhq_production
          - name: POSTGRES_INITDB_ARGS
          - name: PGDATA
            value: /var/lib/postgresql/data/pgdata
          - name: POSTGRES_PASSWORD_FILE
            value: /conf/postgres-password
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: postgres:9.6.8
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - exec pg_isready --host $POD_IP
            failureThreshold: 6
            initialDelaySeconds: 120
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: gitlab-postgresql
          ports:
          - containerPort: 5432
            name: postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - sh
              - -c
              - exec pg_isready --host $POD_IP
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              cpu: 10m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/postgresql/data/pgdata
            name: data
            subPath: postgresql-db
          - mountPath: /conf
            name: password-file
            readOnly: true
        - env:
          - name: DATA_SOURCE_NAME
            value: postgresql://gitlab@127.0.0.1:5432?sslmode=disable
          image: wrouesnel/postgres_exporter:v0.1.1
          imagePullPolicy: IfNotPresent
          name: metrics
          ports:
          - containerPort: 9187
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 12m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: gitlab-postgresql
        - name: password-file
          secret:
            defaultMode: 420
            items:
            - key: postgres-password
              path: postgres-password
            secretName: gitlab-postgresql-password
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-02-04T05:20:47Z"
    generation: 1
    labels:
      app: postgresql
      pod-template-hash: 8466fc49b6
      release: gitlab
    name: gitlab-postgresql-8466fc49b6
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-postgresql
      uid: 99622113-398b-11ea-b115-42010a8001d6
    resourceVersion: "33571956"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-postgresql-8466fc49b6
    uid: 19bcff3a-470e-11ea-96d3-42010a80017a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: postgresql
        pod-template-hash: 8466fc49b6
        release: gitlab
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: postgresql
          pod-template-hash: 8466fc49b6
          release: gitlab
      spec:
        containers:
        - env:
          - name: POSTGRES_USER
            value: gitlab
          - name: PGUSER
            value: gitlab
          - name: POSTGRES_DB
            value: gitlabhq_production
          - name: POSTGRES_INITDB_ARGS
          - name: PGDATA
            value: /var/lib/postgresql/data/pgdata
          - name: POSTGRES_PASSWORD_FILE
            value: /conf/postgres-password
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: postgres:9.6.8
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - exec pg_isready --host $POD_IP
            failureThreshold: 6
            initialDelaySeconds: 120
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: gitlab-postgresql
          ports:
          - containerPort: 5432
            name: postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - sh
              - -c
              - exec pg_isready --host $POD_IP
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              cpu: 10m
              memory: 25Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/postgresql/data/pgdata
            name: data
            subPath: postgresql-db
          - mountPath: /conf
            name: password-file
            readOnly: true
        - env:
          - name: DATA_SOURCE_NAME
            value: postgresql://gitlab@127.0.0.1:5432?sslmode=disable
          image: wrouesnel/postgres_exporter:v0.1.1
          imagePullPolicy: IfNotPresent
          name: metrics
          ports:
          - containerPort: 9187
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 12m
              memory: 25Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: gitlab-postgresql
        - name: password-file
          secret:
            defaultMode: 420
            items:
            - key: postgres-password
              path: postgres-password
            secretName: gitlab-postgresql-password
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 2
    labels:
      app: postgresql
      pod-template-hash: cb4c58788
      release: gitlab
    name: gitlab-postgresql-cb4c58788
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-postgresql
      uid: 99622113-398b-11ea-b115-42010a8001d6
    resourceVersion: "7565780"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-postgresql-cb4c58788
    uid: 99821585-398b-11ea-b115-42010a8001d6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: postgresql
        pod-template-hash: cb4c58788
        release: gitlab
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: postgresql
          pod-template-hash: cb4c58788
          release: gitlab
      spec:
        containers:
        - env:
          - name: POSTGRES_USER
            value: gitlab
          - name: PGUSER
            value: gitlab
          - name: POSTGRES_DB
            value: gitlabhq_production
          - name: POSTGRES_INITDB_ARGS
          - name: PGDATA
            value: /var/lib/postgresql/data/pgdata
          - name: POSTGRES_PASSWORD_FILE
            value: /conf/postgres-password
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: postgres:9.6.8
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - exec pg_isready --host $POD_IP
            failureThreshold: 6
            initialDelaySeconds: 120
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: gitlab-postgresql
          ports:
          - containerPort: 5432
            name: postgresql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - sh
              - -c
              - exec pg_isready --host $POD_IP
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/postgresql/data/pgdata
            name: data
            subPath: postgresql-db
          - mountPath: /conf
            name: password-file
            readOnly: true
        - env:
          - name: DATA_SOURCE_NAME
            value: postgresql://gitlab@127.0.0.1:5432?sslmode=disable
          image: wrouesnel/postgres_exporter:v0.1.1
          imagePullPolicy: IfNotPresent
          name: metrics
          ports:
          - containerPort: 9187
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data
          persistentVolumeClaim:
            claimName: gitlab-postgresql
        - name: password-file
          secret:
            defaultMode: 420
            items:
            - key: postgres-password
              path: postgres-password
            secretName: gitlab-postgresql-password
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:53Z"
    generation: 1
    labels:
      app: prometheus
      chart: prometheus-9.0.0
      component: server
      heritage: Helm
      pod-template-hash: cf7649bb9
      release: gitlab
    name: gitlab-prometheus-server-cf7649bb9
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-prometheus-server
      uid: 9976d02c-398b-11ea-b115-42010a8001d6
    resourceVersion: "37200146"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-prometheus-server-cf7649bb9
    uid: 9a191393-398b-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: prometheus
        component: server
        pod-template-hash: cf7649bb9
        release: gitlab
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: prometheus
          chart: prometheus-9.0.0
          component: server
          heritage: Helm
          pod-template-hash: cf7649bb9
          release: gitlab
      spec:
        containers:
        - args:
          - --volume-dir=/etc/config
          - --webhook-url=http://127.0.0.1:9090/-/reload
          image: jimmidyson/configmap-reload:v0.2.2
          imagePullPolicy: IfNotPresent
          name: prometheus-server-configmap-reload
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
            readOnly: true
        - args:
          - --storage.tsdb.retention.time=15d
          - --config.file=/etc/config/prometheus.yml
          - --storage.tsdb.path=/data
          - --web.console.libraries=/etc/prometheus/console_libraries
          - --web.console.templates=/etc/prometheus/consoles
          - --web.enable-lifecycle
          image: prom/prometheus:v2.11.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: prometheus-server
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config-volume
          - mountPath: /data
            name: storage-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: gitlab-prometheus-server
        serviceAccountName: gitlab-prometheus-server
        terminationGracePeriodSeconds: 300
        volumes:
        - configMap:
            defaultMode: 420
            name: gitlab-prometheus-server
          name: config-volume
        - name: storage-volume
          persistentVolumeClaim:
            claimName: gitlab-prometheus-server
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "3"
      deployment.kubernetes.io/max-replicas: "4"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-30T04:41:28Z"
    generation: 6
    labels:
      app: redis
      chart: redis-ha-0.1.0
      heritage: Helm
      name: redis-sentinel
      pod-template-hash: 5dbfcc649f
      release: gitlab
    name: gitlab-redis-sentinel-5dbfcc649f
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-redis-sentinel
      uid: 9960bf23-398b-11ea-b115-42010a8001d6
    resourceVersion: "9737754"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-redis-sentinel-5dbfcc649f
    uid: c76a373c-431a-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: redis
        chart: redis-ha-0.1.0
        heritage: Helm
        name: redis-sentinel
        pod-template-hash: 5dbfcc649f
        release: gitlab
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: redis
          chart: redis-ha-0.1.0
          heritage: Helm
          name: redis-sentinel
          pod-template-hash: 5dbfcc649f
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - redis
                  - key: release
                    operator: In
                    values:
                    - gitlab
                  - key: redis-role
                    operator: In
                    values:
                    - sentinel
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: SENTINEL
            value: "true"
          - name: REDIS_CHART_PREFIX
            value: gitlab-redis
          - name: REDIS_PASSWORD_FILE
            value: /config/password
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-redis-ha:732704f18e34ba469df34b10c3b2465e0469d484
          imagePullPolicy: Always
          name: sentinel
          ports:
          - containerPort: 26379
            protocol: TCP
          resources:
            limits:
              memory: 200Mi
            requests:
              cpu: 13m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config/
            name: gitlab-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: gitlab-redis
        serviceAccountName: gitlab-redis
        terminationGracePeriodSeconds: 30
        volumes:
        - name: gitlab-config
          projected:
            defaultMode: 420
            sources:
            - secret:
                items:
                - key: secret
                  path: password
                name: gitlab-redis-secret
  status:
    observedGeneration: 6
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-02-04T05:20:04Z"
    generation: 4
    labels:
      app: redis
      chart: redis-ha-0.1.0
      heritage: Helm
      name: redis-sentinel
      pod-template-hash: 69879b577
      release: gitlab
    name: gitlab-redis-sentinel-69879b577
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-redis-sentinel
      uid: 9960bf23-398b-11ea-b115-42010a8001d6
    resourceVersion: "33571940"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-redis-sentinel-69879b577
    uid: 0061c6c4-470e-11ea-96d3-42010a80017a
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: redis
        chart: redis-ha-0.1.0
        heritage: Helm
        name: redis-sentinel
        pod-template-hash: 69879b577
        release: gitlab
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: redis
          chart: redis-ha-0.1.0
          heritage: Helm
          name: redis-sentinel
          pod-template-hash: 69879b577
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - redis
                  - key: release
                    operator: In
                    values:
                    - gitlab
                  - key: redis-role
                    operator: In
                    values:
                    - sentinel
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: SENTINEL
            value: "true"
          - name: REDIS_CHART_PREFIX
            value: gitlab-redis
          - name: REDIS_PASSWORD_FILE
            value: /config/password
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-redis-ha:732704f18e34ba469df34b10c3b2465e0469d484
          imagePullPolicy: Always
          name: sentinel
          ports:
          - containerPort: 26379
            protocol: TCP
          resources:
            limits:
              memory: 30Mi
            requests:
              cpu: 13m
              memory: 30Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config/
            name: gitlab-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: gitlab-redis
        serviceAccountName: gitlab-redis
        terminationGracePeriodSeconds: 30
        volumes:
        - name: gitlab-config
          projected:
            defaultMode: 420
            sources:
            - secret:
                items:
                - key: secret
                  path: password
                name: gitlab-redis-secret
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 4
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "3"
      deployment.kubernetes.io/max-replicas: "4"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 4
    labels:
      app: redis
      chart: redis-ha-0.1.0
      heritage: Helm
      name: redis-sentinel
      pod-template-hash: 7dd554445c
      release: gitlab
    name: gitlab-redis-sentinel-7dd554445c
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-redis-sentinel
      uid: 9960bf23-398b-11ea-b115-42010a8001d6
    resourceVersion: "7566195"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-redis-sentinel-7dd554445c
    uid: 99708c84-398b-11ea-b115-42010a8001d6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: redis
        chart: redis-ha-0.1.0
        heritage: Helm
        name: redis-sentinel
        pod-template-hash: 7dd554445c
        release: gitlab
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: redis
          chart: redis-ha-0.1.0
          heritage: Helm
          name: redis-sentinel
          pod-template-hash: 7dd554445c
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - redis
                  - key: release
                    operator: In
                    values:
                    - gitlab
                  - key: redis-role
                    operator: In
                    values:
                    - sentinel
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: SENTINEL
            value: "true"
          - name: REDIS_CHART_PREFIX
            value: gitlab-redis
          - name: REDIS_PASSWORD_FILE
            value: /config/password
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-redis-ha:732704f18e34ba469df34b10c3b2465e0469d484
          imagePullPolicy: Always
          name: sentinel
          ports:
          - containerPort: 26379
            protocol: TCP
          resources:
            limits:
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config/
            name: gitlab-config
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: gitlab-redis
        serviceAccountName: gitlab-redis
        terminationGracePeriodSeconds: 30
        volumes:
        - name: gitlab-config
          projected:
            defaultMode: 420
            sources:
            - secret:
                items:
                - key: secret
                  path: password
                name: gitlab-redis-secret
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 68
    labels:
      app: registry
      pod-template-hash: 74c959fc8
      release: gitlab
    name: gitlab-registry-74c959fc8
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-registry
      uid: 99636a67-398b-11ea-b115-42010a8001d6
    resourceVersion: "37196456"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-registry-74c959fc8
    uid: 99924f2e-398b-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: registry
        pod-template-hash: 74c959fc8
        release: gitlab
    template:
      metadata:
        annotations:
          checksum/configmap: 286c8cfd6e68b01cac791db6c7970b093f059b17c007ea126964d5c8f5cc9288
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: registry
          pod-template-hash: 74c959fc8
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: registry
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - image: registry:2.7.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /debug/health
              port: 5001
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: registry
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /debug/health
              port: 5001
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 50m
              memory: 32Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/docker/registry/
            name: registry-server-config
            readOnly: true
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: IfNotPresent
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: registry-secrets
          - mountPath: /registry
            name: registry-server-config
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: registry-server-config
        - name: registry-secrets
          projected:
            defaultMode: 420
            sources:
            - configMap:
                name: gitlab-registry
            - secret:
                items:
                - key: registry-auth.crt
                  path: certificate.crt
                name: gitlab-registry-secret
            - secret:
                items:
                - key: secret
                  path: httpSecret
                name: gitlab-registry-httpsecret
            - secret:
                name: gitlab-minio-secret
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 68
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-02-02T23:44:00Z"
    generation: 9
    labels:
      app: sidekiq
      pod-template-hash: 5dd9f54858
      release: gitlab
    name: gitlab-sidekiq-all-in-1-5dd9f54858
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-sidekiq-all-in-1
      uid: 9962f9f2-398b-11ea-b115-42010a8001d6
    resourceVersion: "9894962"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-sidekiq-all-in-1-5dd9f54858
    uid: e304c467-4615-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: sidekiq
        pod-template-hash: 5dd9f54858
        release: gitlab
    template:
      metadata:
        annotations:
          checksum/configmap: 3a9f53018b334d9fcb2222de4dabc983f5b6eed7c75910d315de61d1d88a0d91
          checksum/configmap-pod: 1c9250b5fb48aeff1c2d2554f323f88f9db01c254bdaef83cf2dc26ec784ebea
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
          prometheus.io/port: "3807"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: sidekiq
          pod-template-hash: 5dd9f54858
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: sidekiq
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: prometheus_multiproc_dir
            value: /metrics
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          - name: SIDEKIQ_CONCURRENCY
            value: "25"
          - name: SIDEKIQ_TIMEOUT
            value: "5"
          - name: SIDEKIQ_DAEMON_MEMORY_KILLER
            value: "0"
          - name: SIDEKIQ_MEMORY_KILLER_CHECK_INTERVAL
            value: "3"
          - name: SIDEKIQ_MEMORY_KILLER_MAX_RSS
            value: "2000000"
          - name: SIDEKIQ_MEMORY_KILLER_GRACE_TIME
            value: "900"
          - name: SIDEKIQ_MEMORY_KILLER_SHUTDOWN_WAIT
            value: "30"
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-sidekiq-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -f 'sidekiq'
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 3807
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: sidekiq
          ports:
          - containerPort: 3807
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 3807
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            requests:
              cpu: 10m
              memory: 650M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /metrics
            name: sidekiq-metrics
          - mountPath: /var/opt/gitlab/templates
            name: sidekiq-config
            readOnly: true
          - mountPath: /etc/gitlab
            name: sidekiq-secrets
            readOnly: true
          - mountPath: /srv/gitlab/config/secrets.yml
            name: sidekiq-secrets
            subPath: rails-secrets/secrets.yml
          - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
            name: sidekiq-config
            subPath: smtp_settings.rb
          - mountPath: /srv/gitlab/INSTALLATION_TYPE
            name: sidekiq-config
            subPath: installation_type
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: sidekiq-config
            readOnly: true
          - mountPath: /init-config
            name: init-sidekiq-secrets
            readOnly: true
          - mountPath: /init-secrets
            name: sidekiq-secrets
        - args:
          - /scripts/wait-for-deps
          env:
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          - name: SIDEKIQ_CONCURRENCY
            value: "25"
          - name: SIDEKIQ_TIMEOUT
            value: "5"
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-sidekiq-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          name: dependencies
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab/templates
            name: sidekiq-config
            readOnly: true
          - mountPath: /etc/gitlab
            name: sidekiq-secrets
            readOnly: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: sidekiq-metrics
        - name: sidekiq-config
          projected:
            defaultMode: 420
            sources:
            - configMap:
                name: gitlab-sidekiq
            - configMap:
                name: gitlab-sidekiq-all-in-1
        - name: init-sidekiq-secrets
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: secrets.yml
                  path: rails-secrets/secrets.yml
                name: gitlab-rails-secret
            - secret:
                items:
                - key: token
                  path: gitaly/gitaly_token
                name: gitlab-gitaly-secret
            - secret:
                items:
                - key: secret
                  path: redis/password
                name: gitlab-redis-secret
            - secret:
                items:
                - key: postgres-password
                  path: postgres/psql-password
                name: gitlab-postgresql-password
            - secret:
                items:
                - key: registry-auth.key
                  path: registry/gitlab-registry.key
                name: gitlab-registry-secret
            - secret:
                items:
                - key: accesskey
                  path: minio/accesskey
                - key: secretkey
                  path: minio/secretkey
                name: gitlab-minio-secret
        - emptyDir:
            medium: Memory
          name: sidekiq-secrets
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    observedGeneration: 9
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-02-04T04:48:31Z"
    generation: 37
    labels:
      app: sidekiq
      pod-template-hash: 649889b7c6
      release: gitlab
    name: gitlab-sidekiq-all-in-1-649889b7c6
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-sidekiq-all-in-1
      uid: 9962f9f2-398b-11ea-b115-42010a8001d6
    resourceVersion: "33574602"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-sidekiq-all-in-1-649889b7c6
    uid: 977edeb1-4709-11ea-96d3-42010a80017a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: sidekiq
        pod-template-hash: 649889b7c6
        release: gitlab
    template:
      metadata:
        annotations:
          checksum/configmap: 3a9f53018b334d9fcb2222de4dabc983f5b6eed7c75910d315de61d1d88a0d91
          checksum/configmap-pod: 1c9250b5fb48aeff1c2d2554f323f88f9db01c254bdaef83cf2dc26ec784ebea
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
          prometheus.io/port: "3807"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: sidekiq
          pod-template-hash: 649889b7c6
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: sidekiq
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: prometheus_multiproc_dir
            value: /metrics
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          - name: SIDEKIQ_CONCURRENCY
            value: "25"
          - name: SIDEKIQ_TIMEOUT
            value: "5"
          - name: SIDEKIQ_DAEMON_MEMORY_KILLER
            value: "0"
          - name: SIDEKIQ_MEMORY_KILLER_CHECK_INTERVAL
            value: "3"
          - name: SIDEKIQ_MEMORY_KILLER_MAX_RSS
            value: "2000000"
          - name: SIDEKIQ_MEMORY_KILLER_GRACE_TIME
            value: "900"
          - name: SIDEKIQ_MEMORY_KILLER_SHUTDOWN_WAIT
            value: "30"
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-sidekiq-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -f 'sidekiq'
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /liveness
              port: 3807
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: sidekiq
          ports:
          - containerPort: 3807
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 3807
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            requests:
              cpu: 10m
              memory: 50M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /metrics
            name: sidekiq-metrics
          - mountPath: /var/opt/gitlab/templates
            name: sidekiq-config
            readOnly: true
          - mountPath: /etc/gitlab
            name: sidekiq-secrets
            readOnly: true
          - mountPath: /srv/gitlab/config/secrets.yml
            name: sidekiq-secrets
            subPath: rails-secrets/secrets.yml
          - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
            name: sidekiq-config
            subPath: smtp_settings.rb
          - mountPath: /srv/gitlab/INSTALLATION_TYPE
            name: sidekiq-config
            subPath: installation_type
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: sidekiq-config
            readOnly: true
          - mountPath: /init-config
            name: init-sidekiq-secrets
            readOnly: true
          - mountPath: /init-secrets
            name: sidekiq-secrets
        - args:
          - /scripts/wait-for-deps
          env:
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          - name: SIDEKIQ_CONCURRENCY
            value: "25"
          - name: SIDEKIQ_TIMEOUT
            value: "5"
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-sidekiq-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          name: dependencies
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab/templates
            name: sidekiq-config
            readOnly: true
          - mountPath: /etc/gitlab
            name: sidekiq-secrets
            readOnly: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: sidekiq-metrics
        - name: sidekiq-config
          projected:
            defaultMode: 420
            sources:
            - configMap:
                name: gitlab-sidekiq
            - configMap:
                name: gitlab-sidekiq-all-in-1
        - name: init-sidekiq-secrets
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: secrets.yml
                  path: rails-secrets/secrets.yml
                name: gitlab-rails-secret
            - secret:
                items:
                - key: token
                  path: gitaly/gitaly_token
                name: gitlab-gitaly-secret
            - secret:
                items:
                - key: secret
                  path: redis/password
                name: gitlab-redis-secret
            - secret:
                items:
                - key: postgres-password
                  path: postgres/psql-password
                name: gitlab-postgresql-password
            - secret:
                items:
                - key: registry-auth.key
                  path: registry/gitlab-registry.key
                name: gitlab-registry-secret
            - secret:
                items:
                - key: accesskey
                  path: minio/accesskey
                - key: secretkey
                  path: minio/secretkey
                name: gitlab-minio-secret
        - emptyDir:
            medium: Memory
          name: sidekiq-secrets
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 37
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 1
    labels:
      app: task-runner
      pod-template-hash: 9f9cf668f
      release: gitlab
    name: gitlab-task-runner-9f9cf668f
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-task-runner
      uid: 9965278e-398b-11ea-b115-42010a8001d6
    resourceVersion: "33302859"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-task-runner-9f9cf668f
    uid: 999f2071-398b-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: task-runner
        pod-template-hash: 9f9cf668f
        release: gitlab
    template:
      metadata:
        annotations:
          checksum/config: f51dac24c133254fe6c63906a579c8d3d58b44036d8737eb8744b99cef3feb8a
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
        creationTimestamp: null
        labels:
          app: task-runner
          pod-template-hash: 9f9cf668f
          release: gitlab
      spec:
        containers:
        - args:
          - /bin/bash
          - -c
          - cp -v -r -L /etc/gitlab/.s3cfg $HOME/.s3cfg && while sleep 3600; do :;
            done
          env:
          - name: ARTIFACTS_BUCKET_NAME
            value: gitlab-artifacts
          - name: REGISTRY_BUCKET_NAME
            value: registry
          - name: LFS_BUCKET_NAME
            value: git-lfs
          - name: UPLOADS_BUCKET_NAME
            value: gitlab-uploads
          - name: PACKAGES_BUCKET_NAME
            value: gitlab-packages
          - name: BACKUP_BUCKET_NAME
            value: gitlab-backups
          - name: BACKUP_BACKEND
            value: s3
          - name: TMP_BUCKET_NAME
            value: tmp
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: ENABLE_BOOTSNAP
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-task-runner-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          name: task-runner
          resources:
            requests:
              cpu: 50m
              memory: 350M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab/templates
            name: task-runner-config
          - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
            name: task-runner-config
            subPath: smtp_settings.rb
          - mountPath: /etc/gitlab
            name: task-runner-secrets
            readOnly: true
          - mountPath: /srv/gitlab/config/secrets.yml
            name: task-runner-secrets
            subPath: rails-secrets/secrets.yml
          - mountPath: /srv/gitlab/tmp
            name: task-runner-tmp
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: task-runner-config
            readOnly: true
          - mountPath: /init-config
            name: init-task-runner-secrets
            readOnly: true
          - mountPath: /init-secrets
            name: task-runner-secrets
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - name: task-runner-config
          projected:
            defaultMode: 420
            sources:
            - configMap:
                name: gitlab-task-runner
        - emptyDir: {}
          name: task-runner-tmp
        - name: init-task-runner-secrets
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: secrets.yml
                  path: rails-secrets/secrets.yml
                name: gitlab-rails-secret
            - secret:
                items:
                - key: secret
                  path: shell/.gitlab_shell_secret
                name: gitlab-gitlab-shell-secret
            - secret:
                items:
                - key: token
                  path: gitaly/gitaly_token
                name: gitlab-gitaly-secret
            - secret:
                items:
                - key: secret
                  path: redis/password
                name: gitlab-redis-secret
            - secret:
                items:
                - key: postgres-password
                  path: postgres/psql-password
                name: gitlab-postgresql-password
            - secret:
                items:
                - key: registry-auth.key
                  path: registry/gitlab-registry.key
                name: gitlab-registry-secret
            - secret:
                items:
                - key: accesskey
                  path: minio/accesskey
                - key: secretkey
                  path: minio/secretkey
                name: gitlab-minio-secret
        - emptyDir:
            medium: Memory
          name: task-runner-secrets
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:43:53Z"
    generation: 4
    labels:
      app: unicorn
      pod-template-hash: 57fb68c97f
      release: gitlab
    name: gitlab-unicorn-57fb68c97f
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-unicorn
      uid: 9968e6a8-398b-11ea-b115-42010a8001d6
    resourceVersion: "7565343"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-unicorn-57fb68c97f
    uid: 99c6cd26-398b-11ea-b115-42010a8001d6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: unicorn
        pod-template-hash: 57fb68c97f
        release: gitlab
    template:
      metadata:
        annotations:
          checksum/config: d021e46742b5d6dd76265a6b1dd9c78cdeb6db74842f2cdaa710f411a97fbabb
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
          prometheus.io/path: /-/metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: unicorn
          pod-template-hash: 57fb68c97f
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: unicorn
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: TMPDIR
            value: /tmp/gitlab
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          - name: prometheus_multiproc_dir
            value: /metrics
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -SIGQUIT -f 'unicorn master'
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/liveness
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: unicorn
          ports:
          - containerPort: 8080
            name: unicorn
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/readiness
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            requests:
              cpu: 300m
              memory: 1200M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /metrics
            name: unicorn-metrics
          - mountPath: /var/opt/gitlab/templates
            name: unicorn-config
          - mountPath: /etc/gitlab
            name: unicorn-secrets
            readOnly: true
          - mountPath: /srv/gitlab/config/secrets.yml
            name: unicorn-secrets
            subPath: rails-secrets/secrets.yml
          - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
            name: unicorn-config
            subPath: smtp_settings.rb
          - mountPath: /srv/gitlab/INSTALLATION_TYPE
            name: unicorn-config
            subPath: installation_type
          - mountPath: /srv/gitlab/public/uploads/tmp
            name: shared-upload-directory
          - mountPath: /srv/gitlab/shared
            name: shared-artifact-directory
          - mountPath: /tmp
            name: shared-tmp
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        - env:
          - name: TMPDIR
            value: /tmp/gitlab
          - name: GITLAB_WORKHORSE_EXTRA_ARGS
          - name: GITLAB_WORKHORSE_LISTEN_PORT
            value: "8181"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /scripts/healthcheck
            failureThreshold: 3
            initialDelaySeconds: 20
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: gitlab-workhorse
          ports:
          - containerPort: 8181
            name: workhorse
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /scripts/healthcheck
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            requests:
              cpu: 100m
              memory: 100M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab/templates
            name: workhorse-config
          - mountPath: /etc/gitlab
            name: workhorse-secrets
            readOnly: true
          - mountPath: /srv/gitlab/public/uploads/tmp
            name: shared-upload-directory
          - mountPath: /srv/gitlab/shared
            name: shared-artifact-directory
          - mountPath: /tmp
            name: shared-tmp
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - args:
          - -c
          - sh -x /config-unicorn/configure ; sh -x /config-workhorse/configure ;
            mkdir -p -m 3770 /tmp/gitlab
          command:
          - sh
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config-unicorn
            name: unicorn-config
            readOnly: true
          - mountPath: /config-workhorse
            name: workhorse-config
            readOnly: true
          - mountPath: /init-config
            name: init-unicorn-secrets
            readOnly: true
          - mountPath: /init-secrets
            name: unicorn-secrets
          - mountPath: /init-secrets-workhorse
            name: workhorse-secrets
          - mountPath: /tmp
            name: shared-tmp
        - args:
          - /scripts/wait-for-deps
          env:
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          - name: WORKHORSE_ARCHIVE_CACHE_DISABLED
            value: "1"
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          name: dependencies
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab/templates
            name: unicorn-config
          - mountPath: /etc/gitlab
            name: unicorn-secrets
            readOnly: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: shared-tmp
        - emptyDir:
            medium: Memory
          name: unicorn-metrics
        - configMap:
            defaultMode: 420
            name: gitlab-unicorn
          name: unicorn-config
        - configMap:
            defaultMode: 420
            name: gitlab-workhorse-config
          name: workhorse-config
        - name: init-unicorn-secrets
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: secrets.yml
                  path: rails-secrets/secrets.yml
                name: gitlab-rails-secret
            - secret:
                items:
                - key: secret
                  path: shell/.gitlab_shell_secret
                name: gitlab-gitlab-shell-secret
            - secret:
                items:
                - key: token
                  path: gitaly/gitaly_token
                name: gitlab-gitaly-secret
            - secret:
                items:
                - key: secret
                  path: redis/password
                name: gitlab-redis-secret
            - secret:
                items:
                - key: postgres-password
                  path: postgres/psql-password
                name: gitlab-postgresql-password
            - secret:
                items:
                - key: registry-auth.key
                  path: registry/gitlab-registry.key
                name: gitlab-registry-secret
            - secret:
                items:
                - key: shared_secret
                  path: gitlab-workhorse/secret
                name: gitlab-gitlab-workhorse-secret
            - secret:
                items:
                - key: accesskey
                  path: minio/accesskey
                - key: secretkey
                  path: minio/secretkey
                name: gitlab-minio-secret
        - emptyDir:
            medium: Memory
          name: unicorn-secrets
        - emptyDir:
            medium: Memory
          name: workhorse-secrets
        - emptyDir: {}
          name: shared-upload-directory
        - emptyDir: {}
          name: shared-artifact-directory
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-02-02T23:45:00Z"
    generation: 15
    labels:
      app: unicorn
      pod-template-hash: 84c5c578b5
      release: gitlab
    name: gitlab-unicorn-84c5c578b5
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-unicorn
      uid: 9968e6a8-398b-11ea-b115-42010a8001d6
    resourceVersion: "9901661"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-unicorn-84c5c578b5
    uid: 06ba6bb7-4616-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: unicorn
        pod-template-hash: 84c5c578b5
        release: gitlab
    template:
      metadata:
        annotations:
          checksum/config: d021e46742b5d6dd76265a6b1dd9c78cdeb6db74842f2cdaa710f411a97fbabb
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
          prometheus.io/path: /-/metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: unicorn
          pod-template-hash: 84c5c578b5
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: unicorn
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: TMPDIR
            value: /tmp/gitlab
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          - name: prometheus_multiproc_dir
            value: /metrics
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -SIGQUIT -f 'unicorn master'
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/liveness
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: unicorn
          ports:
          - containerPort: 8080
            name: unicorn
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/readiness
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            requests:
              cpu: 10m
              memory: 400M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /metrics
            name: unicorn-metrics
          - mountPath: /var/opt/gitlab/templates
            name: unicorn-config
          - mountPath: /etc/gitlab
            name: unicorn-secrets
            readOnly: true
          - mountPath: /srv/gitlab/config/secrets.yml
            name: unicorn-secrets
            subPath: rails-secrets/secrets.yml
          - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
            name: unicorn-config
            subPath: smtp_settings.rb
          - mountPath: /srv/gitlab/INSTALLATION_TYPE
            name: unicorn-config
            subPath: installation_type
          - mountPath: /srv/gitlab/public/uploads/tmp
            name: shared-upload-directory
          - mountPath: /srv/gitlab/shared
            name: shared-artifact-directory
          - mountPath: /tmp
            name: shared-tmp
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        - env:
          - name: TMPDIR
            value: /tmp/gitlab
          - name: GITLAB_WORKHORSE_EXTRA_ARGS
          - name: GITLAB_WORKHORSE_LISTEN_PORT
            value: "8181"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /scripts/healthcheck
            failureThreshold: 3
            initialDelaySeconds: 20
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: gitlab-workhorse
          ports:
          - containerPort: 8181
            name: workhorse
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /scripts/healthcheck
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            requests:
              cpu: 10m
              memory: 100M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab/templates
            name: workhorse-config
          - mountPath: /etc/gitlab
            name: workhorse-secrets
            readOnly: true
          - mountPath: /srv/gitlab/public/uploads/tmp
            name: shared-upload-directory
          - mountPath: /srv/gitlab/shared
            name: shared-artifact-directory
          - mountPath: /tmp
            name: shared-tmp
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - args:
          - -c
          - sh -x /config-unicorn/configure ; sh -x /config-workhorse/configure ;
            mkdir -p -m 3770 /tmp/gitlab
          command:
          - sh
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config-unicorn
            name: unicorn-config
            readOnly: true
          - mountPath: /config-workhorse
            name: workhorse-config
            readOnly: true
          - mountPath: /init-config
            name: init-unicorn-secrets
            readOnly: true
          - mountPath: /init-secrets
            name: unicorn-secrets
          - mountPath: /init-secrets-workhorse
            name: workhorse-secrets
          - mountPath: /tmp
            name: shared-tmp
        - args:
          - /scripts/wait-for-deps
          env:
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          - name: WORKHORSE_ARCHIVE_CACHE_DISABLED
            value: "1"
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          name: dependencies
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab/templates
            name: unicorn-config
          - mountPath: /etc/gitlab
            name: unicorn-secrets
            readOnly: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: shared-tmp
        - emptyDir:
            medium: Memory
          name: unicorn-metrics
        - configMap:
            defaultMode: 420
            name: gitlab-unicorn
          name: unicorn-config
        - configMap:
            defaultMode: 420
            name: gitlab-workhorse-config
          name: workhorse-config
        - name: init-unicorn-secrets
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: secrets.yml
                  path: rails-secrets/secrets.yml
                name: gitlab-rails-secret
            - secret:
                items:
                - key: secret
                  path: shell/.gitlab_shell_secret
                name: gitlab-gitlab-shell-secret
            - secret:
                items:
                - key: token
                  path: gitaly/gitaly_token
                name: gitlab-gitaly-secret
            - secret:
                items:
                - key: secret
                  path: redis/password
                name: gitlab-redis-secret
            - secret:
                items:
                - key: postgres-password
                  path: postgres/psql-password
                name: gitlab-postgresql-password
            - secret:
                items:
                - key: registry-auth.key
                  path: registry/gitlab-registry.key
                name: gitlab-registry-secret
            - secret:
                items:
                - key: shared_secret
                  path: gitlab-workhorse/secret
                name: gitlab-gitlab-workhorse-secret
            - secret:
                items:
                - key: accesskey
                  path: minio/accesskey
                - key: secretkey
                  path: minio/secretkey
                name: gitlab-minio-secret
        - emptyDir:
            medium: Memory
          name: unicorn-secrets
        - emptyDir:
            medium: Memory
          name: workhorse-secrets
        - emptyDir: {}
          name: shared-upload-directory
        - emptyDir: {}
          name: shared-artifact-directory
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    observedGeneration: 15
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2020-02-04T04:49:18Z"
    generation: 30
    labels:
      app: unicorn
      pod-template-hash: 84d7dc6557
      release: gitlab
    name: gitlab-unicorn-84d7dc6557
    namespace: gitlab
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gitlab-unicorn
      uid: 9968e6a8-398b-11ea-b115-42010a8001d6
    resourceVersion: "36422529"
    selfLink: /apis/apps/v1/namespaces/gitlab/replicasets/gitlab-unicorn-84d7dc6557
    uid: b3bedb01-4709-11ea-96d3-42010a80017a
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: unicorn
        pod-template-hash: 84d7dc6557
        release: gitlab
    template:
      metadata:
        annotations:
          checksum/config: d021e46742b5d6dd76265a6b1dd9c78cdeb6db74842f2cdaa710f411a97fbabb
          cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
          prometheus.io/path: /-/metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: unicorn
          pod-template-hash: 84d7dc6557
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: unicorn
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: TMPDIR
            value: /tmp/gitlab
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          - name: prometheus_multiproc_dir
            value: /metrics
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/bash
                - -c
                - pkill -SIGQUIT -f 'unicorn master'
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/liveness
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 20
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: unicorn
          ports:
          - containerPort: 8080
            name: unicorn
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/readiness
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            requests:
              cpu: 10m
              memory: 40M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /metrics
            name: unicorn-metrics
          - mountPath: /var/opt/gitlab/templates
            name: unicorn-config
          - mountPath: /etc/gitlab
            name: unicorn-secrets
            readOnly: true
          - mountPath: /srv/gitlab/config/secrets.yml
            name: unicorn-secrets
            subPath: rails-secrets/secrets.yml
          - mountPath: /srv/gitlab/config/initializers/smtp_settings.rb
            name: unicorn-config
            subPath: smtp_settings.rb
          - mountPath: /srv/gitlab/INSTALLATION_TYPE
            name: unicorn-config
            subPath: installation_type
          - mountPath: /srv/gitlab/public/uploads/tmp
            name: shared-upload-directory
          - mountPath: /srv/gitlab/shared
            name: shared-artifact-directory
          - mountPath: /tmp
            name: shared-tmp
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        - env:
          - name: TMPDIR
            value: /tmp/gitlab
          - name: GITLAB_WORKHORSE_EXTRA_ARGS
          - name: GITLAB_WORKHORSE_LISTEN_PORT
            value: "8181"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-workhorse-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /scripts/healthcheck
            failureThreshold: 3
            initialDelaySeconds: 20
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 30
          name: gitlab-workhorse
          ports:
          - containerPort: 8181
            name: workhorse
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /scripts/healthcheck
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          resources:
            requests:
              cpu: 10m
              memory: 40M
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab/templates
            name: workhorse-config
          - mountPath: /etc/gitlab
            name: workhorse-secrets
            readOnly: true
          - mountPath: /srv/gitlab/public/uploads/tmp
            name: shared-upload-directory
          - mountPath: /srv/gitlab/shared
            name: shared-artifact-directory
          - mountPath: /tmp
            name: shared-tmp
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - args:
          - -c
          - sh -x /config-unicorn/configure ; sh -x /config-workhorse/configure ;
            mkdir -p -m 3770 /tmp/gitlab
          command:
          - sh
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config-unicorn
            name: unicorn-config
            readOnly: true
          - mountPath: /config-workhorse
            name: workhorse-config
            readOnly: true
          - mountPath: /init-config
            name: init-unicorn-secrets
            readOnly: true
          - mountPath: /init-secrets
            name: unicorn-secrets
          - mountPath: /init-secrets-workhorse
            name: workhorse-secrets
          - mountPath: /tmp
            name: shared-tmp
        - args:
          - /scripts/wait-for-deps
          env:
          - name: GITALY_FEATURE_DEFAULT_ON
            value: "1"
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          - name: WORKHORSE_ARCHIVE_CACHE_DISABLED
            value: "1"
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-unicorn-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          name: dependencies
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab/templates
            name: unicorn-config
          - mountPath: /etc/gitlab
            name: unicorn-secrets
            readOnly: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: shared-tmp
        - emptyDir:
            medium: Memory
          name: unicorn-metrics
        - configMap:
            defaultMode: 420
            name: gitlab-unicorn
          name: unicorn-config
        - configMap:
            defaultMode: 420
            name: gitlab-workhorse-config
          name: workhorse-config
        - name: init-unicorn-secrets
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: secrets.yml
                  path: rails-secrets/secrets.yml
                name: gitlab-rails-secret
            - secret:
                items:
                - key: secret
                  path: shell/.gitlab_shell_secret
                name: gitlab-gitlab-shell-secret
            - secret:
                items:
                - key: token
                  path: gitaly/gitaly_token
                name: gitlab-gitaly-secret
            - secret:
                items:
                - key: secret
                  path: redis/password
                name: gitlab-redis-secret
            - secret:
                items:
                - key: postgres-password
                  path: postgres/psql-password
                name: gitlab-postgresql-password
            - secret:
                items:
                - key: registry-auth.key
                  path: registry/gitlab-registry.key
                name: gitlab-registry-secret
            - secret:
                items:
                - key: shared_secret
                  path: gitlab-workhorse/secret
                name: gitlab-gitlab-workhorse-secret
            - secret:
                items:
                - key: accesskey
                  path: minio/accesskey
                - key: secretkey
                  path: minio/secretkey
                name: gitlab-minio-secret
        - emptyDir:
            medium: Memory
          name: unicorn-secrets
        - emptyDir:
            medium: Memory
          name: workhorse-secrets
        - emptyDir: {}
          name: shared-upload-directory
        - emptyDir: {}
          name: shared-artifact-directory
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 30
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T07:11:51Z"
    generation: 2
    labels:
      app: adservice
      pod-template-hash: 78bf7fbd8c
    name: adservice-78bf7fbd8c
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: adservice
      uid: a424ede3-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "8455055"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/adservice-78bf7fbd8c
    uid: a4263c30-3441-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: adservice
        pod-template-hash: 78bf7fbd8c
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: adservice
          pod-template-hash: 78bf7fbd8c
      spec:
        containers:
        - env:
          - name: PORT
            value: "9555"
          image: gcr.io/google-samples/microservices-demo/adservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:9555
            failureThreshold: 3
            initialDelaySeconds: 20
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 9555
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:9555
            failureThreshold: 3
            initialDelaySeconds: 20
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 300m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 180Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-01-30T04:52:49Z"
    generation: 2
    labels:
      app: adservice
      pod-template-hash: 84b8749d65
    name: adservice-84b8749d65
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: adservice
      uid: a424ede3-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "36841059"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/adservice-84b8749d65
    uid: 5d739cff-431c-11ea-96d3-42010a80017a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: adservice
        pod-template-hash: 84b8749d65
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: adservice
          pod-template-hash: 84b8749d65
      spec:
        containers:
        - env:
          - name: PORT
            value: "9555"
          image: gcr.io/google-samples/microservices-demo/adservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:9555
            failureThreshold: 3
            initialDelaySeconds: 20
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 9555
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:9555
            failureThreshold: 3
            initialDelaySeconds: 20
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 50m
              memory: 180Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    fullyLabeledReplicas: 1
    observedGeneration: 2
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-30T04:44:23Z"
    generation: 2
    labels:
      app: adservice
      pod-template-hash: b9cc4b8c5
    name: adservice-b9cc4b8c5
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: adservice
      uid: a424ede3-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "7570190"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/adservice-b9cc4b8c5
    uid: 2feea957-431b-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: adservice
        pod-template-hash: b9cc4b8c5
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: adservice
          pod-template-hash: b9cc4b8c5
      spec:
        containers:
        - env:
          - name: PORT
            value: "9555"
          image: gcr.io/google-samples/microservices-demo/adservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:9555
            failureThreshold: 3
            initialDelaySeconds: 20
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 9555
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:9555
            failureThreshold: 3
            initialDelaySeconds: 20
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 30m
              memory: 300Mi
            requests:
              cpu: 10m
              memory: 180Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2020-01-30T18:46:38Z"
    generation: 1
    labels:
      app: cartservice
      pod-template-hash: 55f8ccc8f4
    name: cartservice-55f8ccc8f4
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cartservice
      uid: a29ac8cf-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "36841168"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/cartservice-55f8ccc8f4
    uid: d8fc4c22-4390-11ea-96d3-42010a80017a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: cartservice
        pod-template-hash: 55f8ccc8f4
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cartservice
          pod-template-hash: 55f8ccc8f4
      spec:
        containers:
        - env:
          - name: REDIS_ADDR
            value: redis-cart:6379
          - name: PORT
            value: "7070"
          - name: LISTEN_ADDR
            value: 0.0.0.0
          image: gcr.io/google-samples/microservices-demo/cartservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7070
              - -rpc-timeout=5s
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 7070
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7070
              - -rpc-timeout=5s
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 25m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T07:11:49Z"
    generation: 2
    labels:
      app: cartservice
      pod-template-hash: 6884ccc4d
    name: cartservice-6884ccc4d
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cartservice
      uid: a29ac8cf-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "7570380"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/cartservice-6884ccc4d
    uid: a29bc869-3441-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: cartservice
        pod-template-hash: 6884ccc4d
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cartservice
          pod-template-hash: 6884ccc4d
      spec:
        containers:
        - env:
          - name: REDIS_ADDR
            value: redis-cart:6379
          - name: PORT
            value: "7070"
          - name: LISTEN_ADDR
            value: 0.0.0.0
          image: gcr.io/google-samples/microservices-demo/cartservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7070
              - -rpc-timeout=5s
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 7070
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7070
              - -rpc-timeout=5s
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 300m
              memory: 128Mi
            requests:
              cpu: 100m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-01-30T04:52:48Z"
    generation: 3
    labels:
      app: cartservice
      pod-template-hash: 7c59b85886
    name: cartservice-7c59b85886
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cartservice
      uid: a29ac8cf-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "9209516"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/cartservice-7c59b85886
    uid: 5cca1a66-431c-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: cartservice
        pod-template-hash: 7c59b85886
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cartservice
          pod-template-hash: 7c59b85886
      spec:
        containers:
        - env:
          - name: REDIS_ADDR
            value: redis-cart:6379
          - name: PORT
            value: "7070"
          - name: LISTEN_ADDR
            value: 0.0.0.0
          image: gcr.io/google-samples/microservices-demo/cartservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7070
              - -rpc-timeout=5s
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 7070
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7070
              - -rpc-timeout=5s
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    observedGeneration: 3
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-30T04:44:17Z"
    generation: 2
    labels:
      app: cartservice
      pod-template-hash: f7fb7c956
    name: cartservice-f7fb7c956
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cartservice
      uid: a29ac8cf-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "7570143"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/cartservice-f7fb7c956
    uid: 2c32ecf7-431b-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: cartservice
        pod-template-hash: f7fb7c956
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: cartservice
          pod-template-hash: f7fb7c956
      spec:
        containers:
        - env:
          - name: REDIS_ADDR
            value: redis-cart:6379
          - name: PORT
            value: "7070"
          - name: LISTEN_ADDR
            value: 0.0.0.0
          image: gcr.io/google-samples/microservices-demo/cartservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7070
              - -rpc-timeout=5s
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 7070
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7070
              - -rpc-timeout=5s
            failureThreshold: 3
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 30m
              memory: 128Mi
            requests:
              cpu: 10m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-30T04:44:10Z"
    generation: 1
    labels:
      app: checkoutservice
      pod-template-hash: 77d8f889b7
    name: checkoutservice-77d8f889b7
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: checkoutservice
      uid: a0b32397-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "33302783"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/checkoutservice-77d8f889b7
    uid: 286e8b6c-431b-11ea-96d3-42010a80017a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: checkoutservice
        pod-template-hash: 77d8f889b7
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: checkoutservice
          pod-template-hash: 77d8f889b7
      spec:
        containers:
        - env:
          - name: PORT
            value: "5050"
          - name: PRODUCT_CATALOG_SERVICE_ADDR
            value: productcatalogservice:3550
          - name: SHIPPING_SERVICE_ADDR
            value: shippingservice:50051
          - name: PAYMENT_SERVICE_ADDR
            value: paymentservice:50051
          - name: EMAIL_SERVICE_ADDR
            value: emailservice:5000
          - name: CURRENCY_SERVICE_ADDR
            value: currencyservice:7000
          - name: CART_SERVICE_ADDR
            value: cartservice:7070
          image: gcr.io/google-samples/microservices-demo/checkoutservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:5050
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 5050
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:5050
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 20m
              memory: 128Mi
            requests:
              cpu: 5m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T07:11:46Z"
    generation: 2
    labels:
      app: checkoutservice
      pod-template-hash: 7f8444cc85
    name: checkoutservice-7f8444cc85
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: checkoutservice
      uid: a0b32397-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "7567242"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/checkoutservice-7f8444cc85
    uid: a0b4f54d-3441-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: checkoutservice
        pod-template-hash: 7f8444cc85
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: checkoutservice
          pod-template-hash: 7f8444cc85
      spec:
        containers:
        - env:
          - name: PORT
            value: "5050"
          - name: PRODUCT_CATALOG_SERVICE_ADDR
            value: productcatalogservice:3550
          - name: SHIPPING_SERVICE_ADDR
            value: shippingservice:50051
          - name: PAYMENT_SERVICE_ADDR
            value: paymentservice:50051
          - name: EMAIL_SERVICE_ADDR
            value: emailservice:5000
          - name: CURRENCY_SERVICE_ADDR
            value: currencyservice:7000
          - name: CART_SERVICE_ADDR
            value: cartservice:7070
          image: gcr.io/google-samples/microservices-demo/checkoutservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:5050
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 5050
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:5050
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-30T04:44:19Z"
    generation: 2
    labels:
      app: currencyservice
      pod-template-hash: 5bfb6949c5
    name: currencyservice-5bfb6949c5
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: currencyservice
      uid: a31b37af-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "7570169"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/currencyservice-5bfb6949c5
    uid: 2dc50a79-431b-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: currencyservice
        pod-template-hash: 5bfb6949c5
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: currencyservice
          pod-template-hash: 5bfb6949c5
      spec:
        containers:
        - env:
          - name: PORT
            value: "7000"
          image: gcr.io/google-samples/microservices-demo/currencyservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7000
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 7000
            name: grpc
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7000
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 20m
              memory: 128Mi
            requests:
              cpu: 10m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-01-30T04:52:48Z"
    generation: 2
    labels:
      app: currencyservice
      pod-template-hash: 7775f7949
    name: currencyservice-7775f7949
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: currencyservice
      uid: a31b37af-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "36841072"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/currencyservice-7775f7949
    uid: 5d09f261-431c-11ea-96d3-42010a80017a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: currencyservice
        pod-template-hash: 7775f7949
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: currencyservice
          pod-template-hash: 7775f7949
      spec:
        containers:
        - env:
          - name: PORT
            value: "7000"
          image: gcr.io/google-samples/microservices-demo/currencyservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7000
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 7000
            name: grpc
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7000
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T07:11:50Z"
    generation: 2
    labels:
      app: currencyservice
      pod-template-hash: 7c75f54c7
    name: currencyservice-7c75f54c7
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: currencyservice
      uid: a31b37af-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "7570393"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/currencyservice-7c75f54c7
    uid: a31c4b07-3441-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: currencyservice
        pod-template-hash: 7c75f54c7
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: currencyservice
          pod-template-hash: 7c75f54c7
      spec:
        containers:
        - env:
          - name: PORT
            value: "7000"
          image: gcr.io/google-samples/microservices-demo/currencyservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7000
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 7000
            name: grpc
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:7000
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 100m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T07:11:45Z"
    generation: 2
    labels:
      app: emailservice
      pod-template-hash: 5df6786475
    name: emailservice-5df6786475
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: emailservice
      uid: a0685217-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "7570596"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/emailservice-5df6786475
    uid: a06a1cd9-3441-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: emailservice
        pod-template-hash: 5df6786475
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: emailservice
          pod-template-hash: 5df6786475
      spec:
        containers:
        - env:
          - name: PORT
            value: "8080"
          - name: ENABLE_PROFILER
            value: "0"
          image: gcr.io/google-samples/microservices-demo/emailservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:8080
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 8080
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:8080
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-01-30T04:52:46Z"
    generation: 2
    labels:
      app: emailservice
      pod-template-hash: 6545668f4f
    name: emailservice-6545668f4f
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: emailservice
      uid: a0685217-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "36840782"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/emailservice-6545668f4f
    uid: 5bd1e06f-431c-11ea-96d3-42010a80017a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: emailservice
        pod-template-hash: 6545668f4f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: emailservice
          pod-template-hash: 6545668f4f
      spec:
        containers:
        - env:
          - name: PORT
            value: "8080"
          - name: ENABLE_PROFILER
            value: "0"
          image: gcr.io/google-samples/microservices-demo/emailservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:8080
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 8080
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:8080
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-30T04:44:09Z"
    generation: 2
    labels:
      app: emailservice
      pod-template-hash: 84dbb97b49
    name: emailservice-84dbb97b49
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: emailservice
      uid: a0685217-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "7570085"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/emailservice-84dbb97b49
    uid: 27c8dcc1-431b-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: emailservice
        pod-template-hash: 84dbb97b49
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: emailservice
          pod-template-hash: 84dbb97b49
      spec:
        containers:
        - env:
          - name: PORT
            value: "8080"
          - name: ENABLE_PROFILER
            value: "0"
          image: gcr.io/google-samples/microservices-demo/emailservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:8080
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 8080
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:8080
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 20m
              memory: 128Mi
            requests:
              cpu: 5m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T07:11:47Z"
    generation: 2
    labels:
      app: frontend
      pod-template-hash: 7b4bf446ff
    name: frontend-7b4bf446ff
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: frontend
      uid: a15c8a4d-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "7567369"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/frontend-7b4bf446ff
    uid: a15d8551-3441-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: frontend
        pod-template-hash: 7b4bf446ff
    template:
      metadata:
        annotations:
          sidecar.istio.io/rewriteAppHTTPProbers: "true"
        creationTimestamp: null
        labels:
          app: frontend
          pod-template-hash: 7b4bf446ff
      spec:
        containers:
        - env:
          - name: PORT
            value: "8080"
          - name: PRODUCT_CATALOG_SERVICE_ADDR
            value: productcatalogservice:3550
          - name: CURRENCY_SERVICE_ADDR
            value: currencyservice:7000
          - name: CART_SERVICE_ADDR
            value: cartservice:7070
          - name: RECOMMENDATION_SERVICE_ADDR
            value: recommendationservice:8080
          - name: SHIPPING_SERVICE_ADDR
            value: shippingservice:50051
          - name: CHECKOUT_SERVICE_ADDR
            value: checkoutservice:5050
          - name: AD_SERVICE_ADDR
            value: adservice:9555
          image: gcr.io/google-samples/microservices-demo/frontend:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              - name: Cookie
                value: shop_session-id=x-liveness-probe
              path: /_healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 8080
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              - name: Cookie
                value: shop_session-id=x-readiness-probe
              path: /_healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-30T04:44:13Z"
    generation: 1
    labels:
      app: frontend
      pod-template-hash: 7dbdf6c769
    name: frontend-7dbdf6c769
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: frontend
      uid: a15c8a4d-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "36841022"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/frontend-7dbdf6c769
    uid: 29bbf1e3-431b-11ea-96d3-42010a80017a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: frontend
        pod-template-hash: 7dbdf6c769
    template:
      metadata:
        annotations:
          sidecar.istio.io/rewriteAppHTTPProbers: "true"
        creationTimestamp: null
        labels:
          app: frontend
          pod-template-hash: 7dbdf6c769
      spec:
        containers:
        - env:
          - name: PORT
            value: "8080"
          - name: PRODUCT_CATALOG_SERVICE_ADDR
            value: productcatalogservice:3550
          - name: CURRENCY_SERVICE_ADDR
            value: currencyservice:7000
          - name: CART_SERVICE_ADDR
            value: cartservice:7070
          - name: RECOMMENDATION_SERVICE_ADDR
            value: recommendationservice:8080
          - name: SHIPPING_SERVICE_ADDR
            value: shippingservice:50051
          - name: CHECKOUT_SERVICE_ADDR
            value: checkoutservice:5050
          - name: AD_SERVICE_ADDR
            value: adservice:9555
          image: gcr.io/google-samples/microservices-demo/frontend:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              - name: Cookie
                value: shop_session-id=x-liveness-probe
              path: /_healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 8080
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              - name: Cookie
                value: shop_session-id=x-readiness-probe
              path: /_healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 20m
              memory: 128Mi
            requests:
              cpu: 5m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-30T04:44:18Z"
    generation: 1
    labels:
      app: loadgenerator
      pod-template-hash: 547598db87
    name: loadgenerator-547598db87
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: loadgenerator
      uid: a2e2343e-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "33303376"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/loadgenerator-547598db87
    uid: 2d2e941d-431b-11ea-96d3-42010a80017a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: loadgenerator
        pod-template-hash: 547598db87
    template:
      metadata:
        annotations:
          sidecar.istio.io/rewriteAppHTTPProbers: "true"
        creationTimestamp: null
        labels:
          app: loadgenerator
          pod-template-hash: 547598db87
      spec:
        containers:
        - env:
          - name: FRONTEND_ADDR
            value: frontend:80
          - name: USERS
            value: "10"
          image: gcr.io/google-samples/microservices-demo/loadgenerator:v0.1.3
          imagePullPolicy: IfNotPresent
          name: main
          resources:
            limits:
              cpu: 50m
              memory: 512Mi
            requests:
              cpu: 10m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T07:11:49Z"
    generation: 2
    labels:
      app: loadgenerator
      pod-template-hash: 56d8c759d5
    name: loadgenerator-56d8c759d5
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: loadgenerator
      uid: a2e2343e-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "7567287"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/loadgenerator-56d8c759d5
    uid: a2e32aed-3441-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: loadgenerator
        pod-template-hash: 56d8c759d5
    template:
      metadata:
        annotations:
          sidecar.istio.io/rewriteAppHTTPProbers: "true"
        creationTimestamp: null
        labels:
          app: loadgenerator
          pod-template-hash: 56d8c759d5
      spec:
        containers:
        - env:
          - name: FRONTEND_ADDR
            value: frontend:80
          - name: USERS
            value: "10"
          image: gcr.io/google-samples/microservices-demo/loadgenerator:v0.1.3
          imagePullPolicy: IfNotPresent
          name: main
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 10m
              memory: 256Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-30T04:44:14Z"
    generation: 2
    labels:
      app: paymentservice
      pod-template-hash: 57445cf999
    name: paymentservice-57445cf999
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: paymentservice
      uid: a1d5e17e-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "7570114"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/paymentservice-57445cf999
    uid: 2ab03e17-431b-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: paymentservice
        pod-template-hash: 57445cf999
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: paymentservice
          pod-template-hash: 57445cf999
      spec:
        containers:
        - env:
          - name: PORT
            value: "50051"
          image: gcr.io/google-samples/microservices-demo/paymentservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:50051
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 50051
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:50051
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 20m
              memory: 128Mi
            requests:
              cpu: 5m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T07:11:48Z"
    generation: 2
    labels:
      app: paymentservice
      pod-template-hash: 58ffcf9f77
    name: paymentservice-58ffcf9f77
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: paymentservice
      uid: a1d5e17e-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "7570343"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/paymentservice-58ffcf9f77
    uid: a1d6c8e9-3441-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: paymentservice
        pod-template-hash: 58ffcf9f77
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: paymentservice
          pod-template-hash: 58ffcf9f77
      spec:
        containers:
        - env:
          - name: PORT
            value: "50051"
          image: gcr.io/google-samples/microservices-demo/paymentservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:50051
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 50051
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:50051
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-01-30T04:52:47Z"
    generation: 2
    labels:
      app: paymentservice
      pod-template-hash: 6b9d88465f
    name: paymentservice-6b9d88465f
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: paymentservice
      uid: a1d5e17e-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "36841078"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/paymentservice-6b9d88465f
    uid: 5c7abd32-431c-11ea-96d3-42010a80017a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: paymentservice
        pod-template-hash: 6b9d88465f
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: paymentservice
          pod-template-hash: 6b9d88465f
      spec:
        containers:
        - env:
          - name: PORT
            value: "50051"
          image: gcr.io/google-samples/microservices-demo/paymentservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:50051
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 50051
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:50051
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T07:11:48Z"
    generation: 2
    labels:
      app: productcatalogservice
      pod-template-hash: 557c755fb5
    name: productcatalogservice-557c755fb5
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: productcatalogservice
      uid: a2397ecc-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "7567531"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/productcatalogservice-557c755fb5
    uid: a23a78da-3441-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: productcatalogservice
        pod-template-hash: 557c755fb5
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: productcatalogservice
          pod-template-hash: 557c755fb5
      spec:
        containers:
        - env:
          - name: PORT
            value: "3550"
          image: gcr.io/google-samples/microservices-demo/productcatalogservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:3550
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 3550
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:3550
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-30T04:44:15Z"
    generation: 1
    labels:
      app: productcatalogservice
      pod-template-hash: 7f5dc87d7
    name: productcatalogservice-7f5dc87d7
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: productcatalogservice
      uid: a2397ecc-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "37185769"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/productcatalogservice-7f5dc87d7
    uid: 2b65422c-431b-11ea-96d3-42010a80017a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: productcatalogservice
        pod-template-hash: 7f5dc87d7
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: productcatalogservice
          pod-template-hash: 7f5dc87d7
      spec:
        containers:
        - env:
          - name: PORT
            value: "3550"
          image: gcr.io/google-samples/microservices-demo/productcatalogservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:3550
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 3550
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:3550
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 20m
              memory: 128Mi
            requests:
              cpu: 5m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-30T04:44:12Z"
    generation: 1
    labels:
      app: recommendationservice
      pod-template-hash: 744d5589c7
    name: recommendationservice-744d5589c7
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: recommendationservice
      uid: a1103ee1-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "36841107"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/recommendationservice-744d5589c7
    uid: 29151317-431b-11ea-96d3-42010a80017a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: recommendationservice
        pod-template-hash: 744d5589c7
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: recommendationservice
          pod-template-hash: 744d5589c7
      spec:
        containers:
        - env:
          - name: PORT
            value: "8080"
          - name: PRODUCT_CATALOG_SERVICE_ADDR
            value: productcatalogservice:3550
          - name: ENABLE_PROFILER
            value: "0"
          image: gcr.io/google-samples/microservices-demo/recommendationservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:8080
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 8080
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:8080
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 20m
              memory: 450Mi
            requests:
              cpu: 5m
              memory: 220Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    fullyLabeledReplicas: 1
    observedGeneration: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T07:11:46Z"
    generation: 2
    labels:
      app: recommendationservice
      pod-template-hash: "7795565967"
    name: recommendationservice-7795565967
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: recommendationservice
      uid: a1103ee1-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "9589418"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/recommendationservice-7795565967
    uid: a11458b4-3441-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: recommendationservice
        pod-template-hash: "7795565967"
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: recommendationservice
          pod-template-hash: "7795565967"
      spec:
        containers:
        - env:
          - name: PORT
            value: "8080"
          - name: PRODUCT_CATALOG_SERVICE_ADDR
            value: productcatalogservice:3550
          - name: ENABLE_PROFILER
            value: "0"
          image: gcr.io/google-samples/microservices-demo/recommendationservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:8080
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 8080
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:8080
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 450Mi
            requests:
              cpu: 50m
              memory: 220Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 5
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-30T04:44:22Z"
    generation: 1
    labels:
      app: redis-cart
      pod-template-hash: 58764b9d5d
    name: redis-cart-58764b9d5d
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: redis-cart
      uid: a3dfa36d-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "33302849"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/redis-cart-58764b9d5d
    uid: 2f3a7fa6-431b-11ea-96d3-42010a80017a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: redis-cart
        pod-template-hash: 58764b9d5d
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: redis-cart
          pod-template-hash: 58764b9d5d
      spec:
        containers:
        - image: redis:alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 6379
            timeoutSeconds: 1
          name: redis
          ports:
          - containerPort: 6379
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 6379
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 75m
              memory: 256Mi
            requests:
              cpu: 40m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: redis-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: redis-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T07:11:51Z"
    generation: 2
    labels:
      app: redis-cart
      pod-template-hash: 65bf66b8fd
    name: redis-cart-65bf66b8fd
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: redis-cart
      uid: a3dfa36d-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "7567297"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/redis-cart-65bf66b8fd
    uid: a3e0ad9f-3441-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: redis-cart
        pod-template-hash: 65bf66b8fd
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: redis-cart
          pod-template-hash: 65bf66b8fd
      spec:
        containers:
        - image: redis:alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 6379
            timeoutSeconds: 1
          name: redis
          ports:
          - containerPort: 6379
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            tcpSocket:
              port: 6379
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 125m
              memory: 256Mi
            requests:
              cpu: 70m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: redis-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: redis-data
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-30T04:44:21Z"
    generation: 1
    labels:
      app: shippingservice
      pod-template-hash: 5f96974545
    name: shippingservice-5f96974545
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: shippingservice
      uid: a38191a7-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "37192532"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/shippingservice-5f96974545
    uid: 2e796d8d-431b-11ea-96d3-42010a80017a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: shippingservice
        pod-template-hash: 5f96974545
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: shippingservice
          pod-template-hash: 5f96974545
      spec:
        containers:
        - env:
          - name: PORT
            value: "50051"
          image: gcr.io/google-samples/microservices-demo/shippingservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:50051
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 50051
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:50051
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 20m
              memory: 128Mi
            requests:
              cpu: 10m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T07:11:50Z"
    generation: 2
    labels:
      app: shippingservice
      pod-template-hash: 7b8dd7b7d7
    name: shippingservice-7b8dd7b7d7
    namespace: hipster
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: shippingservice
      uid: a38191a7-3441-11ea-9cdc-42010a8001cf
    resourceVersion: "7567344"
    selfLink: /apis/apps/v1/namespaces/hipster/replicasets/shippingservice-7b8dd7b7d7
    uid: a38288ba-3441-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: shippingservice
        pod-template-hash: 7b8dd7b7d7
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: shippingservice
          pod-template-hash: 7b8dd7b7d7
      spec:
        containers:
        - env:
          - name: PORT
            value: "50051"
          image: gcr.io/google-samples/microservices-demo/shippingservice:v0.1.3
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:50051
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: server
          ports:
          - containerPort: 50051
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/grpc_health_probe
              - -addr=:50051
            failureThreshold: 3
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 100m
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-22T17:44:02Z"
    generation: 2
    labels:
      k8s-app: calico-node-autoscaler
      pod-template-hash: 6d58db487
    name: calico-node-vertical-autoscaler-6d58db487
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-node-vertical-autoscaler
      uid: c737192c-3d3e-11ea-96d3-42010a80017a
    resourceVersion: "31420414"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/calico-node-vertical-autoscaler-6d58db487
    uid: c738a72a-3d3e-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: calico-node-autoscaler
        pod-template-hash: 6d58db487
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-node-autoscaler
          pod-template-hash: 6d58db487
      spec:
        containers:
        - command:
          - /cpvpa
          - --target=daemonset/calico-node
          - --namespace=kube-system
          - --logtostderr=true
          - --poll-period-seconds=30
          - --v=2
          - --config-file=/etc/config/node-autoscaler
          image: k8s.gcr.io/cpvpa-amd64:v0.7.1
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-cpva
        serviceAccountName: calico-cpva
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: calico-node-vertical-autoscaler
          name: config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-03-26T11:51:15Z"
    generation: 1
    labels:
      k8s-app: calico-node-autoscaler
      pod-template-hash: b889c775f
    name: calico-node-vertical-autoscaler-b889c775f
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-node-vertical-autoscaler
      uid: c737192c-3d3e-11ea-96d3-42010a80017a
    resourceVersion: "33571650"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/calico-node-vertical-autoscaler-b889c775f
    uid: 191987f3-6f58-11ea-8ebf-42010a800207
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: calico-node-autoscaler
        pod-template-hash: b889c775f
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-node-autoscaler
          pod-template-hash: b889c775f
      spec:
        containers:
        - command:
          - /cpvpa
          - --target=daemonset/calico-node
          - --namespace=kube-system
          - --logtostderr=true
          - --poll-period-seconds=30
          - --v=2
          - --config-file=/etc/config/node-autoscaler
          image: gke.gcr.io/cpvpa-amd64:v0.7.1-gke.0
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-cpva
        serviceAccountName: calico-cpva
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: calico-node-vertical-autoscaler
          name: config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-01-22T17:44:06Z"
    generation: 1
    labels:
      k8s-app: calico-typha
      pod-template-hash: 65bfd5544b
    name: calico-typha-65bfd5544b
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-typha
      uid: c7707f92-3d3e-11ea-96d3-42010a80017a
    resourceVersion: "33571165"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/calico-typha-65bfd5544b
    uid: c9954f80-3d3e-11ea-96d3-42010a80017a
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: calico-typha
        pod-template-hash: 65bfd5544b
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-typha
          pod-template-hash: 65bfd5544b
      spec:
        containers:
        - env:
          - name: TYPHA_LOGFILEPATH
            value: none
          - name: TYPHA_LOGSEVERITYSYS
            value: none
          - name: TYPHA_LOGSEVERITYSCREEN
            value: warning
          - name: TYPHA_PROMETHEUSMETRICSENABLED
            value: "true"
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: kubernetes
          - name: TYPHA_REPORTINGINTERVALSECS
            value: "0"
          - name: TYPHA_PROMETHEUSMETRICSPORT
            value: "9093"
          - name: TYPHA_DATASTORETYPE
            value: kubernetes
          - name: TYPHA_MAXCONNECTIONSLOWERLIMIT
            value: "1"
          - name: TYPHA_HEALTHENABLED
            value: "true"
          image: gcr.io/projectcalico-org/typha:v3.2.7
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              host: localhost
              path: /liveness
              port: 9098
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: calico-typha
          ports:
          - containerPort: 5473
            hostPort: 5473
            name: calico-typha
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: localhost
              path: /readiness
              port: 9098
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 200m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/calico
            name: etc-calico
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-sa
        serviceAccountName: calico-sa
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - hostPath:
            path: /etc/calico
            type: ""
          name: etc-calico
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-22T17:44:03Z"
    generation: 2
    labels:
      k8s-app: calico-typha
      pod-template-hash: 6fc97446fc
    name: calico-typha-6fc97446fc
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-typha
      uid: c7707f92-3d3e-11ea-96d3-42010a80017a
    resourceVersion: "4123109"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/calico-typha-6fc97446fc
    uid: c771cd54-3d3e-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: calico-typha
        pod-template-hash: 6fc97446fc
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-typha
          pod-template-hash: 6fc97446fc
      spec:
        containers:
        - env:
          - name: TYPHA_LOGFILEPATH
            value: none
          - name: TYPHA_LOGSEVERITYSYS
            value: none
          - name: TYPHA_LOGSEVERITYSCREEN
            value: warning
          - name: TYPHA_PROMETHEUSMETRICSENABLED
            value: "true"
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: kubernetes
          - name: TYPHA_REPORTINGINTERVALSECS
            value: "0"
          - name: TYPHA_PROMETHEUSMETRICSPORT
            value: "9093"
          - name: TYPHA_DATASTORETYPE
            value: kubernetes
          - name: TYPHA_MAXCONNECTIONSLOWERLIMIT
            value: "1"
          - name: TYPHA_HEALTHENABLED
            value: "true"
          image: gcr.io/projectcalico-org/typha:v3.2.7
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              host: localhost
              path: /liveness
              port: 9098
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 1
          name: calico-typha
          ports:
          - containerPort: 5473
            hostPort: 5473
            name: calico-typha
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: localhost
              path: /readiness
              port: 9098
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/calico
            name: etc-calico
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-sa
        serviceAccountName: calico-sa
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - hostPath:
            path: /etc/calico
            type: ""
          name: etc-calico
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-22T17:44:03Z"
    generation: 2
    labels:
      k8s-app: calico-typha-autoscaler
      pod-template-hash: 847fc7bc8d
    name: calico-typha-horizontal-autoscaler-847fc7bc8d
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-typha-horizontal-autoscaler
      uid: c7889f07-3d3e-11ea-96d3-42010a80017a
    resourceVersion: "31420450"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/calico-typha-horizontal-autoscaler-847fc7bc8d
    uid: c7898929-3d3e-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: calico-typha-autoscaler
        pod-template-hash: 847fc7bc8d
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-typha-autoscaler
          pod-template-hash: 847fc7bc8d
      spec:
        containers:
        - command:
          - /cluster-proportional-autoscaler
          - --namespace=kube-system
          - --configmap=calico-typha-horizontal-autoscaler
          - --target=deployment/calico-typha
          - --logtostderr=true
          - --v=2
          image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.1.2-r2
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources:
            limits:
              cpu: 10m
            requests:
              cpu: 10m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          supplementalGroups:
          - 65534
        serviceAccount: typha-cpha
        serviceAccountName: typha-cpha
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-03-26T11:51:16Z"
    generation: 1
    labels:
      k8s-app: calico-typha-autoscaler
      pod-template-hash: d777c75b4
    name: calico-typha-horizontal-autoscaler-d777c75b4
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-typha-horizontal-autoscaler
      uid: c7889f07-3d3e-11ea-96d3-42010a80017a
    resourceVersion: "33302482"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/calico-typha-horizontal-autoscaler-d777c75b4
    uid: 195b89f7-6f58-11ea-8ebf-42010a800207
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: calico-typha-autoscaler
        pod-template-hash: d777c75b4
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-typha-autoscaler
          pod-template-hash: d777c75b4
      spec:
        containers:
        - command:
          - /cluster-proportional-autoscaler
          - --namespace=kube-system
          - --configmap=calico-typha-horizontal-autoscaler
          - --target=deployment/calico-typha
          - --logtostderr=true
          - --v=2
          image: gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources:
            limits:
              cpu: 10m
            requests:
              cpu: 10m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          supplementalGroups:
          - 65534
        serviceAccount: typha-cpha
        serviceAccountName: typha-cpha
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-22T17:44:03Z"
    generation: 2
    labels:
      k8s-app: calico-typha-autoscaler
      pod-template-hash: 77b7f88f74
    name: calico-typha-vertical-autoscaler-77b7f88f74
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-typha-vertical-autoscaler
      uid: c7ac1f10-3d3e-11ea-96d3-42010a80017a
    resourceVersion: "31420463"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/calico-typha-vertical-autoscaler-77b7f88f74
    uid: c7ad7060-3d3e-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: calico-typha-autoscaler
        pod-template-hash: 77b7f88f74
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-typha-autoscaler
          pod-template-hash: 77b7f88f74
      spec:
        containers:
        - command:
          - /cpvpa
          - --target=deployment/calico-typha
          - --namespace=kube-system
          - --logtostderr=true
          - --poll-period-seconds=30
          - --v=2
          - --config-file=/etc/config/typha-autoscaler
          image: k8s.gcr.io/cpvpa-amd64:v0.7.1
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-cpva
        serviceAccountName: calico-cpva
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: calico-typha-vertical-autoscaler
          name: config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-03-26T11:51:16Z"
    generation: 1
    labels:
      k8s-app: calico-typha-autoscaler
      pod-template-hash: d9b7979f8
    name: calico-typha-vertical-autoscaler-d9b7979f8
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: calico-typha-vertical-autoscaler
      uid: c7ac1f10-3d3e-11ea-96d3-42010a80017a
    resourceVersion: "33573319"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/calico-typha-vertical-autoscaler-d9b7979f8
    uid: 19740c3a-6f58-11ea-8ebf-42010a800207
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: calico-typha-autoscaler
        pod-template-hash: d9b7979f8
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: calico-typha-autoscaler
          pod-template-hash: d9b7979f8
      spec:
        containers:
        - command:
          - /cpvpa
          - --target=deployment/calico-typha
          - --namespace=kube-system
          - --logtostderr=true
          - --poll-period-seconds=30
          - --v=2
          - --config-file=/etc/config/typha-autoscaler
          image: gke.gcr.io/cpvpa-amd64:v0.7.1-gke.0
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: config
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: calico-cpva
        serviceAccountName: calico-cpva
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: calico-typha-vertical-autoscaler
          name: config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T06:01:02Z"
    generation: 1
    labels:
      k8s-app: event-exporter
      pod-template-hash: 5f88c66fb7
      version: v0.2.4
    name: event-exporter-v0.2.4-5f88c66fb7
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: event-exporter-v0.2.4
      uid: beff6bf2-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "33571767"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/event-exporter-v0.2.4-5f88c66fb7
    uid: bf007eef-3437-11ea-9cdc-42010a8001cf
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: event-exporter
        pod-template-hash: 5f88c66fb7
        version: v0.2.4
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: event-exporter
          pod-template-hash: 5f88c66fb7
          version: v0.2.4
      spec:
        containers:
        - command:
          - /event-exporter
          - -sink-opts=-stackdriver-resource-model=old
          image: k8s.gcr.io/event-exporter:v0.2.4
          imagePullPolicy: IfNotPresent
          name: event-exporter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --source=event_exporter:http://localhost:80?whitelisted=stackdriver_sink_received_entry_count,stackdriver_sink_request_count,stackdriver_sink_successfully_sent_entry_count
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd-exporter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: event-exporter-sa
        serviceAccountName: event-exporter-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /etc/ssl/certs
            type: ""
          name: ssl-certs
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T06:01:13Z"
    generation: 2
    labels:
      k8s-app: fluentd-gcp-scaler
      pod-template-hash: 59b7b75cd7
    name: fluentd-gcp-scaler-59b7b75cd7
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: fluentd-gcp-scaler
      uid: c610e38a-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "31420718"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/fluentd-gcp-scaler-59b7b75cd7
    uid: c6126318-3437-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: fluentd-gcp-scaler
        pod-template-hash: 59b7b75cd7
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: fluentd-gcp-scaler
          pod-template-hash: 59b7b75cd7
      spec:
        containers:
        - command:
          - /scaler.sh
          - --ds-name=fluentd-gcp-v3.2.0
          - --scaling-policy=fluentd-gcp-scaling-policy
          env:
          - name: CPU_REQUEST
            value: 100m
          - name: MEMORY_REQUEST
            value: 200Mi
          - name: CPU_LIMIT
            value: 1000m
          - name: MEMORY_LIMIT
            value: 500Mi
          image: k8s.gcr.io/fluentd-gcp-scaler:0.5.2
          imagePullPolicy: IfNotPresent
          name: fluentd-gcp-scaler
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: fluentd-gcp-scaler
        serviceAccountName: fluentd-gcp-scaler
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-03-26T11:51:19Z"
    generation: 1
    labels:
      k8s-app: fluentd-gcp-scaler
      pod-template-hash: 6965bb45c9
    name: fluentd-gcp-scaler-6965bb45c9
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: fluentd-gcp-scaler
      uid: c610e38a-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "33571782"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/fluentd-gcp-scaler-6965bb45c9
    uid: 1b58e436-6f58-11ea-8ebf-42010a800207
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: fluentd-gcp-scaler
        pod-template-hash: 6965bb45c9
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: fluentd-gcp-scaler
          pod-template-hash: 6965bb45c9
      spec:
        containers:
        - command:
          - /scaler.sh
          - --ds-name=fluentd-gcp-v3.2.0
          - --scaling-policy=fluentd-gcp-scaling-policy
          env:
          - name: CPU_REQUEST
            value: 100m
          - name: MEMORY_REQUEST
            value: 200Mi
          - name: CPU_LIMIT
            value: "1"
          - name: MEMORY_LIMIT
            value: 500Mi
          image: k8s.gcr.io/fluentd-gcp-scaler:0.5.2
          imagePullPolicy: IfNotPresent
          name: fluentd-gcp-scaler
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: fluentd-gcp-scaler
        serviceAccountName: fluentd-gcp-scaler
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-03-26T11:53:06Z"
    generation: 1
    labels:
      k8s-app: heapster
      pod-template-hash: 566bdc98db
      version: v1.7.2
    name: heapster-gke-566bdc98db
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: heapster-gke
      uid: 19bb5a70-6f58-11ea-8ebf-42010a800207
    resourceVersion: "33571898"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/heapster-gke-566bdc98db
    uid: 5af6681d-6f58-11ea-8ebf-42010a800207
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: heapster
        pod-template-hash: 566bdc98db
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: heapster
          pod-template-hash: 566bdc98db
          version: v1.7.2
      spec:
        containers:
        - command:
          - /heapster
          - --source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id
          - --sink=stackdriver:?cluster_name=kubevious-samples&use_old_resources=true&use_new_resources=false&min_interval_sec=100&batch_export_timeout_sec=110&cluster_location=us-central1-a
          image: gke.gcr.io/heapster:v1.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8082
              scheme: HTTP
            initialDelaySeconds: 180
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: heapster
          resources:
            limits:
              cpu: 13m
              memory: 120Mi
            requests:
              cpu: 13m
              memory: 120Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prom-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=10m
          - --extra-cpu=0.5m
          - --memory=100Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=heapster-gke
          - --container=heapster
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.3
          imagePullPolicy: IfNotPresent
          name: heapster-nanny
          resources:
            limits:
              cpu: 50m
              memory: 92760Ki
            requests:
              cpu: 50m
              memory: 92760Ki
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: heapster-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          supplementalGroups:
          - 65534
        serviceAccount: heapster
        serviceAccountName: heapster
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: heapster-config
          name: heapster-config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-03-26T11:51:16Z"
    generation: 2
    labels:
      k8s-app: heapster
      pod-template-hash: fcdb559f8
      version: v1.7.2
    name: heapster-gke-fcdb559f8
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: heapster-gke
      uid: 19bb5a70-6f58-11ea-8ebf-42010a800207
    resourceVersion: "31421278"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/heapster-gke-fcdb559f8
    uid: 19bc67e7-6f58-11ea-8ebf-42010a800207
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: heapster
        pod-template-hash: fcdb559f8
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: heapster
          pod-template-hash: fcdb559f8
          version: v1.7.2
      spec:
        containers:
        - command:
          - /heapster
          - --source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id
          - --sink=stackdriver:?cluster_name=kubevious-samples&use_old_resources=true&use_new_resources=false&min_interval_sec=100&batch_export_timeout_sec=110&cluster_location=us-central1-a
          image: gke.gcr.io/heapster:v1.7.2
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8082
              scheme: HTTP
            initialDelaySeconds: 180
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: heapster
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prom-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=10m
          - --extra-cpu=0.5m
          - --memory=100Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=heapster-gke
          - --container=heapster
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.3
          imagePullPolicy: IfNotPresent
          name: heapster-nanny
          resources:
            limits:
              cpu: 50m
              memory: 92760Ki
            requests:
              cpu: 50m
              memory: 92760Ki
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: heapster-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          supplementalGroups:
          - 65534
        serviceAccount: heapster
        serviceAccountName: heapster
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: heapster-config
          name: heapster-config-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T06:01:01Z"
    generation: 8
    labels:
      k8s-app: kube-dns
      pod-template-hash: 79868f54c5
    name: kube-dns-79868f54c5
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-dns
      uid: bec14d00-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "33571711"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/kube-dns-79868f54c5
    uid: bec3208d-3437-11ea-9cdc-42010a8001cf
  spec:
    replicas: 2
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 79868f54c5
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: 79868f54c5
      spec:
        containers:
        - args:
          - --domain=cluster.local.
          - --dns-port=10053
          - --config-dir=/kube-dns-config
          - --v=2
          env:
          - name: PROMETHEUS_PORT
            value: "10055"
          image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/kubedns
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kubedns
          ports:
          - containerPort: 10053
            name: dns-local
            protocol: UDP
          - containerPort: 10053
            name: dns-tcp-local
            protocol: TCP
          - containerPort: 10055
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /kube-dns-config
            name: kube-dns-config
        - args:
          - -v=2
          - -logtostderr
          - -configDir=/etc/k8s/dns/dnsmasq-nanny
          - -restartDnsmasq=true
          - --
          - -k
          - --cache-size=1000
          - --no-negcache
          - --log-facility=-
          - --server=/cluster.local/127.0.0.1#10053
          - --server=/in-addr.arpa/127.0.0.1#10053
          - --server=/ip6.arpa/127.0.0.1#10053
          image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/dnsmasq
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: dnsmasq
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          resources:
            requests:
              cpu: 150m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/k8s/dns/dnsmasq-nanny
            name: kube-dns-config
        - args:
          - --v=2
          - --logtostderr
          - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV
          - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV
          image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /metrics
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: sidecar
          ports:
          - containerPort: 10054
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=kubedns:http://localhost:10054?whitelisted=probe_kubedns_latency_ms,probe_kubedns_errors,dnsmasq_misses,dnsmasq_hits
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          - --v=2
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.4.2
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: Default
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-dns
        serviceAccountName: kube-dns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-dns
            optional: true
          name: kube-dns-config
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 8
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-03-26T11:51:16Z"
    generation: 1
    labels:
      k8s-app: kube-dns-autoscaler
      pod-template-hash: 8687c64fc
    name: kube-dns-autoscaler-8687c64fc
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-dns-autoscaler
      uid: bee75c2d-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "33302451"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/kube-dns-autoscaler-8687c64fc
    uid: 198d8621-6f58-11ea-8ebf-42010a800207
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns-autoscaler
        pod-template-hash: 8687c64fc
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: kube-dns-autoscaler
          pod-template-hash: 8687c64fc
      spec:
        containers:
        - command:
          - /cluster-proportional-autoscaler
          - --namespace=kube-system
          - --configmap=kube-dns-autoscaler
          - --target=Deployment/kube-dns
          - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"preventSinglePointFailure":true}}
          - --logtostderr=true
          - --v=2
          image: gke.gcr.io/cluster-proportional-autoscaler-amd64:1.7.1-gke.0
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources:
            requests:
              cpu: 20m
              memory: 10Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          supplementalGroups:
          - 65534
        serviceAccount: kube-dns-autoscaler
        serviceAccountName: kube-dns-autoscaler
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T06:01:01Z"
    generation: 2
    labels:
      k8s-app: kube-dns-autoscaler
      pod-template-hash: bb58c6784
    name: kube-dns-autoscaler-bb58c6784
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-dns-autoscaler
      uid: bee75c2d-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "31420782"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/kube-dns-autoscaler-bb58c6784
    uid: bee846de-3437-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kube-dns-autoscaler
        pod-template-hash: bb58c6784
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: kube-dns-autoscaler
          pod-template-hash: bb58c6784
      spec:
        containers:
        - command:
          - /cluster-proportional-autoscaler
          - --namespace=kube-system
          - --configmap=kube-dns-autoscaler
          - --target=Deployment/kube-dns
          - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"preventSinglePointFailure":true}}
          - --logtostderr=true
          - --v=2
          image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.3.0
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources:
            requests:
              cpu: 20m
              memory: 10Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          supplementalGroups:
          - 65534
        serviceAccount: kube-dns-autoscaler
        serviceAccountName: kube-dns-autoscaler
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T06:01:01Z"
    generation: 1
    labels:
      k8s-app: glbc
      name: glbc
      pod-template-hash: fd59995cd
    name: l7-default-backend-fd59995cd
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: l7-default-backend
      uid: be955236-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "33571682"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/l7-default-backend-fd59995cd
    uid: be96f3e7-3437-11ea-9cdc-42010a8001cf
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: glbc
        pod-template-hash: fd59995cd
    template:
      metadata:
        annotations:
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: glbc
          name: glbc
          pod-template-hash: fd59995cd
      spec:
        containers:
        - image: k8s.gcr.io/defaultbackend-amd64:1.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: default-http-backend
          ports:
          - containerPort: 8080
            protocol: TCP
          resources:
            limits:
              cpu: 10m
              memory: 20Mi
            requests:
              cpu: 10m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "12"
      deployment.kubernetes.io/revision-history: 2,6,8,10
    creationTimestamp: "2020-01-11T06:01:27Z"
    generation: 10
    labels:
      k8s-app: metrics-server
      pod-template-hash: 57c75779f
      version: v0.3.1
    name: metrics-server-v0.3.1-57c75779f
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server-v0.3.1
      uid: c0161ad4-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "31420523"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/metrics-server-v0.3.1-57c75779f
    uid: ce0f3aaf-3437-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 57c75779f
        version: v0.3.1
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 57c75779f
          version: v0.3.1
        name: metrics-server
      spec:
        containers:
        - command:
          - /metrics-server
          - --metric-resolution=30s
          - --kubelet-port=10255
          - --deprecated-kubelet-completely-insecure=true
          - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
          image: k8s.gcr.io/metrics-server-amd64:v0.3.1
          imagePullPolicy: IfNotPresent
          name: metrics-server
          ports:
          - containerPort: 443
            name: https
            protocol: TCP
          resources:
            limits:
              cpu: 43m
              memory: 55Mi
            requests:
              cpu: 43m
              memory: 55Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=40m
          - --extra-cpu=0.5m
          - --memory=35Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=metrics-server-v0.3.1
          - --container=metrics-server
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.4
          imagePullPolicy: IfNotPresent
          name: metrics-server-nanny
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 5m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: metrics-server-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: metrics-server-config
          name: metrics-server-config-volume
  status:
    observedGeneration: 10
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "13"
    creationTimestamp: "2020-03-26T11:51:17Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      pod-template-hash: 5c6fbf777
      version: v0.3.1
    name: metrics-server-v0.3.1-5c6fbf777
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server-v0.3.1
      uid: c0161ad4-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "33571971"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/metrics-server-v0.3.1-5c6fbf777
    uid: 19e46983-6f58-11ea-8ebf-42010a800207
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 5c6fbf777
        version: v0.3.1
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 5c6fbf777
          version: v0.3.1
        name: metrics-server
      spec:
        containers:
        - command:
          - /metrics-server
          - --metric-resolution=30s
          - --kubelet-port=10255
          - --deprecated-kubelet-completely-insecure=true
          - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
          image: k8s.gcr.io/metrics-server-amd64:v0.3.1
          imagePullPolicy: IfNotPresent
          name: metrics-server
          ports:
          - containerPort: 443
            name: https
            protocol: TCP
          resources:
            limits:
              cpu: 43m
              memory: 55Mi
            requests:
              cpu: 43m
              memory: 55Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=40m
          - --extra-cpu=0.5m
          - --memory=35Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=metrics-server-v0.3.1
          - --container=metrics-server
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: gke.gcr.io/addon-resizer:1.8.4-gke.0
          imagePullPolicy: IfNotPresent
          name: metrics-server-nanny
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 5m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: metrics-server-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: metrics-server-config
          name: metrics-server-config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2020-01-23T00:17:25Z"
    generation: 2
    labels:
      k8s-app: metrics-server
      pod-template-hash: 7475db9b79
      version: v0.3.1
    name: metrics-server-v0.3.1-7475db9b79
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server-v0.3.1
      uid: c0161ad4-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "7590975"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/metrics-server-v0.3.1-7475db9b79
    uid: bb447aab-3d75-11ea-96d3-42010a80017a
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 7475db9b79
        version: v0.3.1
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 7475db9b79
          version: v0.3.1
        name: metrics-server
      spec:
        containers:
        - command:
          - /metrics-server
          - --metric-resolution=30s
          - --kubelet-port=10255
          - --deprecated-kubelet-completely-insecure=true
          - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
          image: k8s.gcr.io/metrics-server-amd64:v0.3.1
          imagePullPolicy: IfNotPresent
          name: metrics-server
          ports:
          - containerPort: 443
            name: https
            protocol: TCP
          resources:
            limits:
              cpu: 45m
              memory: 75Mi
            requests:
              cpu: 45m
              memory: 75Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=40m
          - --extra-cpu=0.5m
          - --memory=35Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=metrics-server-v0.3.1
          - --container=metrics-server
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.4
          imagePullPolicy: IfNotPresent
          name: metrics-server-nanny
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 5m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: metrics-server-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: metrics-server-config
          name: metrics-server-config-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "11"
      deployment.kubernetes.io/revision-history: 3,5,7,9
    creationTimestamp: "2020-01-18T00:54:37Z"
    generation: 10
    labels:
      k8s-app: metrics-server
      pod-template-hash: 7b4d7f457
      version: v0.3.1
    name: metrics-server-v0.3.1-7b4d7f457
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server-v0.3.1
      uid: c0161ad4-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "29689967"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/metrics-server-v0.3.1-7b4d7f457
    uid: 1995d3c7-398d-11ea-b115-42010a8001d6
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 7b4d7f457
        version: v0.3.1
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 7b4d7f457
          version: v0.3.1
        name: metrics-server
      spec:
        containers:
        - command:
          - /metrics-server
          - --metric-resolution=30s
          - --kubelet-port=10255
          - --deprecated-kubelet-completely-insecure=true
          - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
          image: k8s.gcr.io/metrics-server-amd64:v0.3.1
          imagePullPolicy: IfNotPresent
          name: metrics-server
          ports:
          - containerPort: 443
            name: https
            protocol: TCP
          resources:
            limits:
              cpu: 44m
              memory: 63Mi
            requests:
              cpu: 44m
              memory: 63Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=40m
          - --extra-cpu=0.5m
          - --memory=35Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=metrics-server-v0.3.1
          - --container=metrics-server
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.4
          imagePullPolicy: IfNotPresent
          name: metrics-server-nanny
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 5m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: metrics-server-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: metrics-server-config
          name: metrics-server-config-volume
  status:
    observedGeneration: 10
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-11T06:01:03Z"
    generation: 2
    labels:
      k8s-app: metrics-server
      pod-template-hash: c4cddd5f5
      version: v0.3.1
    name: metrics-server-v0.3.1-c4cddd5f5
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server-v0.3.1
      uid: c0161ad4-3437-11ea-9cdc-42010a8001cf
    resourceVersion: "675"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/metrics-server-v0.3.1-c4cddd5f5
    uid: c01fd545-3437-11ea-9cdc-42010a8001cf
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: c4cddd5f5
        version: v0.3.1
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: c4cddd5f5
          version: v0.3.1
        name: metrics-server
      spec:
        containers:
        - command:
          - /metrics-server
          - --metric-resolution=30s
          - --kubelet-port=10255
          - --deprecated-kubelet-completely-insecure=true
          - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
          image: k8s.gcr.io/metrics-server-amd64:v0.3.1
          imagePullPolicy: IfNotPresent
          name: metrics-server
          ports:
          - containerPort: 443
            name: https
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=40m
          - --extra-cpu=0.5m
          - --memory=35Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=metrics-server-v0.3.1
          - --container=metrics-server
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.4
          imagePullPolicy: IfNotPresent
          name: metrics-server-nanny
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 5m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: metrics-server-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: metrics-server-config
          name: metrics-server-config-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-04-02T02:02:58Z"
    generation: 2
    labels:
      k8s-app: kubevious
      pod-template-hash: 55c9b88bfc
    name: kubevious-55c9b88bfc
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious
      uid: 8f93d9b0-707f-11ea-8ebf-42010a800207
    resourceVersion: "34959986"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-55c9b88bfc
    uid: 136c4f6e-7486-11ea-8ebf-42010a800207
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious
        pod-template-hash: 55c9b88bfc
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious
          pod-template-hash: 55c9b88bfc
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/kubevious:0.4.9
          imagePullPolicy: IfNotPresent
          name: kubevious
          ports:
          - containerPort: 4000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious
        serviceAccountName: kubevious
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-03-28T00:00:50Z"
    generation: 2
    labels:
      k8s-app: kubevious
      pod-template-hash: 6b7f7f78d6
    name: kubevious-6b7f7f78d6
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious
      uid: 8f93d9b0-707f-11ea-8ebf-42010a800207
    resourceVersion: "33763824"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-6b7f7f78d6
    uid: 2f91b458-7087-11ea-8ebf-42010a800207
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious
        pod-template-hash: 6b7f7f78d6
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious
          pod-template-hash: 6b7f7f78d6
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/kubevious:0.4.8
          imagePullPolicy: IfNotPresent
          name: kubevious
          ports:
          - containerPort: 4000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious
        serviceAccountName: kubevious
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2020-04-06T02:09:45Z"
    generation: 2
    labels:
      k8s-app: kubevious
      pod-template-hash: 794d54fdb6
    name: kubevious-794d54fdb6
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious
      uid: 8f93d9b0-707f-11ea-8ebf-42010a800207
    resourceVersion: "36202612"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-794d54fdb6
    uid: af7e969c-77ab-11ea-bb1c-42010a800057
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious
        pod-template-hash: 794d54fdb6
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious
          pod-template-hash: 794d54fdb6
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/kubevious:0.5.1
          imagePullPolicy: IfNotPresent
          name: kubevious
          ports:
          - containerPort: 4000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious
        serviceAccountName: kubevious
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-03-27T23:06:16Z"
    generation: 2
    labels:
      k8s-app: kubevious
      pod-template-hash: 7f89f68d6c
    name: kubevious-7f89f68d6c
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious
      uid: 8f93d9b0-707f-11ea-8ebf-42010a800207
    resourceVersion: "32002157"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-7f89f68d6c
    uid: 8f94bdd7-707f-11ea-8ebf-42010a800207
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious
        pod-template-hash: 7f89f68d6c
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious
          pod-template-hash: 7f89f68d6c
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/kubevious:0.4.7
          imagePullPolicy: IfNotPresent
          name: kubevious
          ports:
          - containerPort: 4000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious
        serviceAccountName: kubevious
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2020-04-08T07:32:59Z"
    generation: 1
    labels:
      k8s-app: kubevious
      pod-template-hash: 865cf6dfcf
    name: kubevious-865cf6dfcf
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious
      uid: 8f93d9b0-707f-11ea-8ebf-42010a800207
    resourceVersion: "36841062"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-865cf6dfcf
    uid: 2c352d7e-796b-11ea-bb1c-42010a800057
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kubevious
        pod-template-hash: 865cf6dfcf
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious
          pod-template-hash: 865cf6dfcf
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/kubevious:0.5.2
          imagePullPolicy: IfNotPresent
          name: kubevious
          ports:
          - containerPort: 4000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious
        serviceAccountName: kubevious
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2020-04-05T02:53:19Z"
    generation: 2
    labels:
      k8s-app: kubevious
      pod-template-hash: d6b55cfc9
    name: kubevious-d6b55cfc9
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious
      uid: 8f93d9b0-707f-11ea-8ebf-42010a800207
    resourceVersion: "35336260"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-d6b55cfc9
    uid: 9af57068-76e8-11ea-bb1c-42010a800057
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious
        pod-template-hash: d6b55cfc9
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious
          pod-template-hash: d6b55cfc9
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/kubevious:0.4.10
          imagePullPolicy: IfNotPresent
          name: kubevious
          ports:
          - containerPort: 4000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious
        serviceAccountName: kubevious
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-04-02T02:02:58Z"
    generation: 2
    labels:
      k8s-app: kubevious-parser
      pod-template-hash: 6594d58d68
    name: kubevious-parser-6594d58d68
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious-parser
      uid: 13795f4a-7486-11ea-8ebf-42010a800207
    resourceVersion: "34959935"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-parser-6594d58d68
    uid: 137af4b2-7486-11ea-8ebf-42010a800207
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious-parser
        pod-template-hash: 6594d58d68
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-parser
          pod-template-hash: 6594d58d68
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: KUBEVIOUS_COLLECTOR
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000/api/v1/collect
          image: kubevious/parser:0.4.11
          imagePullPolicy: IfNotPresent
          name: kubevious-parser
          ports:
          - containerPort: 4500
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-parser
        serviceAccountName: kubevious-parser
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-04-05T02:53:19Z"
    generation: 2
    labels:
      k8s-app: kubevious-parser
      pod-template-hash: 679556b9f6
    name: kubevious-parser-679556b9f6
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious-parser
      uid: 13795f4a-7486-11ea-8ebf-42010a800207
    resourceVersion: "34968410"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-parser-679556b9f6
    uid: 9b0cd6ad-76e8-11ea-bb1c-42010a800057
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious-parser
        pod-template-hash: 679556b9f6
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-parser
          pod-template-hash: 679556b9f6
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: KUBEVIOUS_COLLECTOR
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000/api/v1/collect
          image: kubevious/parser:0.4.20
          imagePullPolicy: IfNotPresent
          name: kubevious-parser
          ports:
          - containerPort: 4500
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-parser
        serviceAccountName: kubevious-parser
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2020-04-06T02:09:45Z"
    generation: 2
    labels:
      k8s-app: kubevious-parser
      pod-template-hash: 6c6749879
    name: kubevious-parser-6c6749879
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious-parser
      uid: 13795f4a-7486-11ea-8ebf-42010a800207
    resourceVersion: "36202565"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-parser-6c6749879
    uid: afa4fa35-77ab-11ea-bb1c-42010a800057
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious-parser
        pod-template-hash: 6c6749879
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-parser
          pod-template-hash: 6c6749879
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: KUBEVIOUS_COLLECTOR
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000/api/v1/collect
          image: kubevious/parser:0.5.2
          imagePullPolicy: IfNotPresent
          name: kubevious-parser
          ports:
          - containerPort: 4500
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-parser
        serviceAccountName: kubevious-parser
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-04-05T03:28:33Z"
    generation: 2
    labels:
      k8s-app: kubevious-parser
      pod-template-hash: 6f8c445555
    name: kubevious-parser-6f8c445555
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious-parser
      uid: 13795f4a-7486-11ea-8ebf-42010a800207
    resourceVersion: "34982080"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-parser-6f8c445555
    uid: 86dccd94-76ed-11ea-bb1c-42010a800057
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious-parser
        pod-template-hash: 6f8c445555
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-parser
          pod-template-hash: 6f8c445555
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: KUBEVIOUS_COLLECTOR
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000/api/v1/collect
          image: kubevious/parser:0.4.21
          imagePullPolicy: IfNotPresent
          name: kubevious-parser
          ports:
          - containerPort: 4500
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-parser
        serviceAccountName: kubevious-parser
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2020-04-05T04:18:29Z"
    generation: 2
    labels:
      k8s-app: kubevious-parser
      pod-template-hash: 76bc6566b
    name: kubevious-parser-76bc6566b
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious-parser
      uid: 13795f4a-7486-11ea-8ebf-42010a800207
    resourceVersion: "35336289"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-parser-76bc6566b
    uid: 810fbb39-76f4-11ea-bb1c-42010a800057
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious-parser
        pod-template-hash: 76bc6566b
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-parser
          pod-template-hash: 76bc6566b
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: KUBEVIOUS_COLLECTOR
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000/api/v1/collect
          image: kubevious/parser:0.4.22
          imagePullPolicy: IfNotPresent
          name: kubevious-parser
          ports:
          - containerPort: 4500
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-parser
        serviceAccountName: kubevious-parser
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2020-04-08T07:33:00Z"
    generation: 1
    labels:
      k8s-app: kubevious-parser
      pod-template-hash: 85d9bf6d8c
    name: kubevious-parser-85d9bf6d8c
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious-parser
      uid: 13795f4a-7486-11ea-8ebf-42010a800207
    resourceVersion: "36202560"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-parser-85d9bf6d8c
    uid: 2c4fdf29-796b-11ea-bb1c-42010a800057
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kubevious-parser
        pod-template-hash: 85d9bf6d8c
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-parser
          pod-template-hash: 85d9bf6d8c
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: KUBEVIOUS_COLLECTOR
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000/api/v1/collect
          image: kubevious/parser:0.5.6
          imagePullPolicy: IfNotPresent
          name: kubevious-parser
          ports:
          - containerPort: 4500
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-parser
        serviceAccountName: kubevious-parser
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2020-04-05T02:53:19Z"
    generation: 2
    labels:
      k8s-app: kubevious-ui
      pod-template-hash: 54dc98746b
    name: kubevious-ui-54dc98746b
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious-ui
      uid: 8f945e6c-707f-11ea-8ebf-42010a800207
    resourceVersion: "35336306"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-ui-54dc98746b
    uid: 9b298252-76e8-11ea-bb1c-42010a800057
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious-ui
        pod-template-hash: 54dc98746b
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-ui
          pod-template-hash: 54dc98746b
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: BACKEND_URL
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000
          - name: FORCE_HTTPS
            value: "true"
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/ui:0.4.7
          imagePullPolicy: IfNotPresent
          name: kubevious-ui
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 25m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/views/partials
            name: header-config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-ui
        serviceAccountName: kubevious-ui
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kubevious-ui-header
          name: header-config-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "4"
    creationTimestamp: "2020-04-02T02:02:59Z"
    generation: 2
    labels:
      k8s-app: kubevious-ui
      pod-template-hash: 66697c58fb
    name: kubevious-ui-66697c58fb
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious-ui
      uid: 8f945e6c-707f-11ea-8ebf-42010a800207
    resourceVersion: "33777818"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-ui-66697c58fb
    uid: 1397ec6e-7486-11ea-8ebf-42010a800207
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious-ui
        pod-template-hash: 66697c58fb
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-ui
          pod-template-hash: 66697c58fb
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: BACKEND_URL
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000
          - name: FORCE_HTTPS
            value: "true"
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/ui:0.4.5
          imagePullPolicy: IfNotPresent
          name: kubevious-ui
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 25m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/views/partials
            name: header-config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-ui
        serviceAccountName: kubevious-ui
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kubevious-ui-header
          name: header-config-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-03-27T23:06:16Z"
    generation: 2
    labels:
      k8s-app: kubevious-ui
      pod-template-hash: 679dc5b85b
    name: kubevious-ui-679dc5b85b
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious-ui
      uid: 8f945e6c-707f-11ea-8ebf-42010a800207
    resourceVersion: "32002175"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-ui-679dc5b85b
    uid: 8f954243-707f-11ea-8ebf-42010a800207
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious-ui
        pod-template-hash: 679dc5b85b
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-ui
          pod-template-hash: 679dc5b85b
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: BACKEND_URL
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000
          - name: FORCE_HTTPS
            value: "true"
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/ui:0.4.3
          imagePullPolicy: IfNotPresent
          name: kubevious-ui
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 25m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-ui
        serviceAccountName: kubevious-ui
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "8"
    creationTimestamp: "2020-04-08T07:33:00Z"
    generation: 1
    labels:
      k8s-app: kubevious-ui
      pod-template-hash: 686cfc86f
    name: kubevious-ui-686cfc86f
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious-ui
      uid: 8f945e6c-707f-11ea-8ebf-42010a800207
    resourceVersion: "36202504"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-ui-686cfc86f
    uid: 2c68b921-796b-11ea-bb1c-42010a800057
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kubevious-ui
        pod-template-hash: 686cfc86f
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-ui
          pod-template-hash: 686cfc86f
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: BACKEND_URL
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000
          - name: FORCE_HTTPS
            value: "true"
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/ui:0.5.3
          imagePullPolicy: IfNotPresent
          name: kubevious-ui
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 25m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/views/partials
            name: header-config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-ui
        serviceAccountName: kubevious-ui
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kubevious-ui-header
          name: header-config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-03-28T00:00:51Z"
    generation: 2
    labels:
      k8s-app: kubevious-ui
      pod-template-hash: 757d9cc4cc
    name: kubevious-ui-757d9cc4cc
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious-ui
      uid: 8f945e6c-707f-11ea-8ebf-42010a800207
    resourceVersion: "32003636"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-ui-757d9cc4cc
    uid: 2fac46a0-7087-11ea-8ebf-42010a800207
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious-ui
        pod-template-hash: 757d9cc4cc
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-ui
          pod-template-hash: 757d9cc4cc
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: BACKEND_URL
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000
          - name: FORCE_HTTPS
            value: "true"
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/ui:0.4.4
          imagePullPolicy: IfNotPresent
          name: kubevious-ui
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 25m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-ui
        serviceAccountName: kubevious-ui
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "5"
    creationTimestamp: "2020-04-02T02:53:34Z"
    generation: 2
    labels:
      k8s-app: kubevious-ui
      pod-template-hash: 79cccf9fb9
    name: kubevious-ui-79cccf9fb9
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious-ui
      uid: 8f945e6c-707f-11ea-8ebf-42010a800207
    resourceVersion: "34960017"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-ui-79cccf9fb9
    uid: 24840308-748d-11ea-8ebf-42010a800207
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious-ui
        pod-template-hash: 79cccf9fb9
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-ui
          pod-template-hash: 79cccf9fb9
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: BACKEND_URL
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000
          - name: FORCE_HTTPS
            value: "true"
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/ui:0.4.6
          imagePullPolicy: IfNotPresent
          name: kubevious-ui
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 25m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/views/partials
            name: header-config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-ui
        serviceAccountName: kubevious-ui
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kubevious-ui-header
          name: header-config-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "7"
    creationTimestamp: "2020-04-06T02:09:46Z"
    generation: 2
    labels:
      k8s-app: kubevious-ui
      pod-template-hash: 85f4cb5959
    name: kubevious-ui-85f4cb5959
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious-ui
      uid: 8f945e6c-707f-11ea-8ebf-42010a800207
    resourceVersion: "36202511"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-ui-85f4cb5959
    uid: afd0b29a-77ab-11ea-bb1c-42010a800057
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious-ui
        pod-template-hash: 85f4cb5959
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-ui
          pod-template-hash: 85f4cb5959
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: BACKEND_URL
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000
          - name: FORCE_HTTPS
            value: "true"
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/ui:0.5.1
          imagePullPolicy: IfNotPresent
          name: kubevious-ui
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 25m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/views/partials
            name: header-config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-ui
        serviceAccountName: kubevious-ui
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kubevious-ui-header
          name: header-config-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2020-03-28T00:06:07Z"
    generation: 2
    labels:
      k8s-app: kubevious-ui
      pod-template-hash: f6c46df8c
    name: kubevious-ui-f6c46df8c
    namespace: kubevious
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubevious-ui
      uid: 8f945e6c-707f-11ea-8ebf-42010a800207
    resourceVersion: "33763864"
    selfLink: /apis/apps/v1/namespaces/kubevious/replicasets/kubevious-ui-f6c46df8c
    uid: ec1eaf58-7087-11ea-8ebf-42010a800207
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kubevious-ui
        pod-template-hash: f6c46df8c
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-ui
          pod-template-hash: f6c46df8c
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: BACKEND_URL
            value: http://kubevious-svc.kubevious.svc.cluster.local:4000
          - name: FORCE_HTTPS
            value: "true"
          envFrom:
          - configMapRef:
              name: kubevious-mysql-client
          image: kubevious/ui:0.4.4
          imagePullPolicy: IfNotPresent
          name: kubevious-ui
          ports:
          - containerPort: 3000
            name: http
            protocol: TCP
          resources:
            requests:
              cpu: 25m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /app/views/partials
            name: header-config-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kubevious-ui
        serviceAccountName: kubevious-ui
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: kubevious-ui-header
          name: header-config-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:05:49Z"
    generation: 1
    labels:
      app: alertmanager
      pod-template-hash: 8487d7f7bb
    name: alertmanager-8487d7f7bb
    namespace: openfaas
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: alertmanager
      uid: 486a20ed-3986-11ea-b115-42010a8001d6
    resourceVersion: "33302682"
    selfLink: /apis/apps/v1/namespaces/openfaas/replicasets/alertmanager-8487d7f7bb
    uid: 4870a1f1-3986-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: alertmanager
        pod-template-hash: 8487d7f7bb
    template:
      metadata:
        annotations:
          checksum/alertmanager-config: fc705a1674460ecc032254f930b2d2877349afe3e22607ff974a68360b57ab3e
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app: alertmanager
          pod-template-hash: 8487d7f7bb
      spec:
        containers:
        - command:
          - alertmanager
          - --config.file=/alertmanager.yml
          - --storage.path=/alertmanager
          - --cluster.listen-address=
          image: prom/alertmanager:v0.18.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9093
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: alertmanager
          ports:
          - containerPort: 9093
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/ready
              port: 9093
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          resources:
            limits:
              memory: 50Mi
            requests:
              memory: 25Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /alertmanager.yml
            name: alertmanager-config
            subPath: alertmanager.yml
          - mountPath: /var/secrets
            name: auth
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/arch: amd64
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: alertmanager.yml
              mode: 420
              path: alertmanager.yml
            name: alertmanager-config
          name: alertmanager-config
        - name: auth
          secret:
            defaultMode: 420
            secretName: basic-auth
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:05:49Z"
    generation: 1
    labels:
      app: basic-auth-plugin
      pod-template-hash: 6b4c47c965
    name: basic-auth-plugin-6b4c47c965
    namespace: openfaas
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: basic-auth-plugin
      uid: 48697343-3986-11ea-b115-42010a8001d6
    resourceVersion: "33571821"
    selfLink: /apis/apps/v1/namespaces/openfaas/replicasets/basic-auth-plugin-6b4c47c965
    uid: 48705297-3986-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: basic-auth-plugin
        pod-template-hash: 6b4c47c965
    template:
      metadata:
        annotations:
          prometheus.io.scrape: "false"
        creationTimestamp: null
        labels:
          app: basic-auth-plugin
          pod-template-hash: 6b4c47c965
      spec:
        containers:
        - env:
          - name: secret_mount_path
            value: /var/secrets
          - name: basic_auth
            value: "true"
          image: openfaas/basic-auth-plugin:0.17.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: basic-auth-plugin
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 20m
              memory: 50Mi
          securityContext:
            readOnlyRootFilesystem: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/secrets
            name: auth
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/arch: amd64
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: auth
          secret:
            defaultMode: 420
            secretName: basic-auth
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:05:49Z"
    generation: 1
    labels:
      app: faas-idler
      pod-template-hash: 66ff47fdf5
    name: faas-idler-66ff47fdf5
    namespace: openfaas
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: faas-idler
      uid: 4869389d-3986-11ea-b115-42010a8001d6
    resourceVersion: "33302395"
    selfLink: /apis/apps/v1/namespaces/openfaas/replicasets/faas-idler-66ff47fdf5
    uid: 48700c42-3986-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: faas-idler
        pod-template-hash: 66ff47fdf5
    template:
      metadata:
        annotations:
          prometheus.io.scrape: "false"
        creationTimestamp: null
        labels:
          app: faas-idler
          pod-template-hash: 66ff47fdf5
      spec:
        containers:
        - command:
          - /home/app/faas-idler
          - -dry-run=true
          env:
          - name: gateway_url
            value: http://gateway.openfaas:8080/
          - name: prometheus_host
            value: prometheus.openfaas
          - name: prometheus_port
            value: "9090"
          - name: inactivity_duration
            value: 30m
          - name: reconcile_interval
            value: 2m
          image: openfaas/faas-idler:0.2.1
          imagePullPolicy: Always
          name: faas-idler
          resources:
            requests:
              memory: 64Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/secrets/
            name: auth
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/arch: amd64
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: auth
          secret:
            defaultMode: 420
            secretName: basic-auth
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-03-21T23:56:36Z"
    generation: 1
    labels:
      app: gateway
      pod-template-hash: d56c44b6d
    name: gateway-d56c44b6d
    namespace: openfaas
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gateway
      uid: 4868b189-3986-11ea-b115-42010a8001d6
    resourceVersion: "33572234"
    selfLink: /apis/apps/v1/namespaces/openfaas/replicasets/gateway-d56c44b6d
    uid: 9954d670-6bcf-11ea-923a-42010a800150
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: gateway
        pod-template-hash: d56c44b6d
    template:
      metadata:
        annotations:
          prometheus.io.port: "8082"
          prometheus.io.scrape: "true"
        creationTimestamp: null
        labels:
          app: gateway
          pod-template-hash: d56c44b6d
      spec:
        containers:
        - env:
          - name: read_timeout
            value: 65s
          - name: write_timeout
            value: 65s
          - name: upstream_timeout
            value: 60s
          - name: functions_provider_url
            value: http://127.0.0.1:8081/
          - name: direct_functions
            value: "true"
          - name: direct_functions_suffix
            value: openfaas-fn.svc.cluster.local
          - name: function_namespace
            value: openfaas-fn
          - name: faas_nats_address
            value: nats.openfaas.svc.cluster.local
          - name: faas_nats_port
            value: "4222"
          - name: faas_nats_channel
            value: faas-request
          - name: basic_auth
            value: "true"
          - name: secret_mount_path
            value: /var/secrets
          - name: auth_proxy_url
            value: http://basic-auth-plugin.openfaas:8080/validate
          - name: auth_pass_body
            value: "false"
          - name: scale_from_zero
            value: "true"
          - name: max_idle_conns
            value: "1024"
          - name: max_idle_conns_per_host
            value: "1024"
          image: openfaas/gateway:0.18.7
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: gateway
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 25m
              memory: 120Mi
          securityContext:
            readOnlyRootFilesystem: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/secrets
            name: auth
            readOnly: true
        - env:
          - name: port
            value: "8081"
          - name: function_namespace
            value: openfaas-fn
          - name: read_timeout
            value: 60s
          - name: write_timeout
            value: 60s
          - name: image_pull_policy
            value: Always
          - name: http_probe
            value: "true"
          - name: set_nonroot_user
            value: "false"
          - name: readiness_probe_initial_delay_seconds
            value: "2"
          - name: readiness_probe_timeout_seconds
            value: "1"
          - name: readiness_probe_period_seconds
            value: "2"
          - name: liveness_probe_initial_delay_seconds
            value: "2"
          - name: liveness_probe_timeout_seconds
            value: "1"
          - name: liveness_probe_period_seconds
            value: "2"
          image: openfaas/faas-netes:0.9.15
          imagePullPolicy: IfNotPresent
          name: faas-netes
          ports:
          - containerPort: 8081
            protocol: TCP
          resources:
            requests:
              cpu: 25m
              memory: 120Mi
          securityContext:
            readOnlyRootFilesystem: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: faas-netes-temp-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/arch: amd64
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: openfaas-controller
        serviceAccountName: openfaas-controller
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: faas-netes-temp-volume
        - name: auth
          secret:
            defaultMode: 420
            secretName: basic-auth
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:05:49Z"
    generation: 2
    labels:
      app: gateway
      pod-template-hash: dcc7d75b
    name: gateway-dcc7d75b
    namespace: openfaas
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: gateway
      uid: 4868b189-3986-11ea-b115-42010a8001d6
    resourceVersion: "29701830"
    selfLink: /apis/apps/v1/namespaces/openfaas/replicasets/gateway-dcc7d75b
    uid: 486fc0f6-3986-11ea-b115-42010a8001d6
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: gateway
        pod-template-hash: dcc7d75b
    template:
      metadata:
        annotations:
          prometheus.io.port: "8082"
          prometheus.io.scrape: "true"
        creationTimestamp: null
        labels:
          app: gateway
          pod-template-hash: dcc7d75b
      spec:
        containers:
        - env:
          - name: read_timeout
            value: 65s
          - name: write_timeout
            value: 65s
          - name: upstream_timeout
            value: 60s
          - name: functions_provider_url
            value: http://127.0.0.1:8081/
          - name: direct_functions
            value: "true"
          - name: direct_functions_suffix
            value: openfaas-fn.svc.cluster.local
          - name: function_namespace
            value: openfaas-fn
          - name: faas_nats_address
            value: nats.openfaas.svc.cluster.local
          - name: faas_nats_port
            value: "4222"
          - name: faas_nats_channel
            value: faas-request
          - name: basic_auth
            value: "true"
          - name: secret_mount_path
            value: /var/secrets
          - name: auth_proxy_url
            value: http://basic-auth-plugin.openfaas:8080/validate
          - name: auth_pass_body
            value: "false"
          - name: scale_from_zero
            value: "true"
          - name: max_idle_conns
            value: "1024"
          - name: max_idle_conns_per_host
            value: "1024"
          image: openfaas/gateway:0.18.7
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: gateway
          ports:
          - containerPort: 8080
            name: http
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 50m
              memory: 120Mi
          securityContext:
            readOnlyRootFilesystem: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/secrets
            name: auth
            readOnly: true
        - env:
          - name: port
            value: "8081"
          - name: function_namespace
            value: openfaas-fn
          - name: read_timeout
            value: 60s
          - name: write_timeout
            value: 60s
          - name: image_pull_policy
            value: Always
          - name: http_probe
            value: "true"
          - name: set_nonroot_user
            value: "false"
          - name: readiness_probe_initial_delay_seconds
            value: "2"
          - name: readiness_probe_timeout_seconds
            value: "1"
          - name: readiness_probe_period_seconds
            value: "2"
          - name: liveness_probe_initial_delay_seconds
            value: "2"
          - name: liveness_probe_timeout_seconds
            value: "1"
          - name: liveness_probe_period_seconds
            value: "2"
          image: openfaas/faas-netes:0.9.15
          imagePullPolicy: IfNotPresent
          name: faas-netes
          ports:
          - containerPort: 8081
            protocol: TCP
          resources:
            requests:
              cpu: 50m
              memory: 120Mi
          securityContext:
            readOnlyRootFilesystem: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: faas-netes-temp-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/arch: amd64
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: openfaas-controller
        serviceAccountName: openfaas-controller
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: faas-netes-temp-volume
        - name: auth
          secret:
            defaultMode: 420
            secretName: basic-auth
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:05:49Z"
    generation: 1
    labels:
      app: nats
      pod-template-hash: 7666fb76bd
    name: nats-7666fb76bd
    namespace: openfaas
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: nats
      uid: 48686ac4-3986-11ea-b115-42010a8001d6
    resourceVersion: "33302509"
    selfLink: /apis/apps/v1/namespaces/openfaas/replicasets/nats-7666fb76bd
    uid: 486f7690-3986-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: nats
        pod-template-hash: 7666fb76bd
    template:
      metadata:
        annotations:
          prometheus.io.scrape: "false"
          sidecar.istio.io/inject: "false"
        creationTimestamp: null
        labels:
          app: nats
          pod-template-hash: 7666fb76bd
      spec:
        containers:
        - args:
          - --store
          - memory
          - --cluster_id
          - faas-cluster
          command:
          - /nats-streaming-server
          image: nats-streaming:0.11.2
          imagePullPolicy: Always
          name: nats
          ports:
          - containerPort: 4222
            protocol: TCP
          resources:
            requests:
              memory: 120Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/arch: amd64
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:05:49Z"
    generation: 1
    labels:
      app: prometheus
      pod-template-hash: 6d4c6646b9
    name: prometheus-6d4c6646b9
    namespace: openfaas
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: prometheus
      uid: 4869f7a7-3986-11ea-b115-42010a8001d6
    resourceVersion: "33571930"
    selfLink: /apis/apps/v1/namespaces/openfaas/replicasets/prometheus-6d4c6646b9
    uid: 487de90f-3986-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: prometheus
        pod-template-hash: 6d4c6646b9
    template:
      metadata:
        annotations:
          checksum/prometheus-config: c3bbdb127a0bfc5afa2014b462e13989f96cb8931d7e3650f50503b9586ed0e2
          sidecar.istio.io/inject: "true"
        creationTimestamp: null
        labels:
          app: prometheus
          pod-template-hash: 6d4c6646b9
      spec:
        containers:
        - command:
          - prometheus
          - --config.file=/etc/prometheus/prometheus.yml
          image: prom/prometheus:v2.11.0
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          name: prometheus
          ports:
          - containerPort: 9090
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          resources:
            requests:
              memory: 512Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/prometheus/prometheus.yml
            name: prometheus-config
            subPath: prometheus.yml
          - mountPath: /etc/prometheus/alert.rules.yml
            name: prometheus-config
            subPath: alert.rules.yml
          - mountPath: /prometheus/data
            name: prom-data
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/arch: amd64
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: openfaas-prometheus
        serviceAccountName: openfaas-prometheus
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: prometheus.yml
              mode: 420
              path: prometheus.yml
            - key: alert.rules.yml
              mode: 420
              path: alert.rules.yml
            name: prometheus-config
          name: prometheus-config
        - emptyDir: {}
          name: prom-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-18T00:05:49Z"
    generation: 1
    labels:
      app: queue-worker
      pod-template-hash: 75658c877d
    name: queue-worker-75658c877d
    namespace: openfaas
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: queue-worker
      uid: 486a0cb9-3986-11ea-b115-42010a8001d6
    resourceVersion: "33572226"
    selfLink: /apis/apps/v1/namespaces/openfaas/replicasets/queue-worker-75658c877d
    uid: 487e5b5d-3986-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: queue-worker
        pod-template-hash: 75658c877d
    template:
      metadata:
        annotations:
          prometheus.io.scrape: "false"
        creationTimestamp: null
        labels:
          app: queue-worker
          pod-template-hash: 75658c877d
      spec:
        containers:
        - env:
          - name: faas_nats_address
            value: nats.openfaas.svc.cluster.local
          - name: faas_nats_durable_queue_subscription
            value: "false"
          - name: faas_nats_channel
            value: faas-request
          - name: faas_nats_queue_group
            value: faas
          - name: faas_gateway_address
            value: gateway.openfaas.svc.cluster.local
          - name: gateway_invoke
            value: "true"
          - name: faas_function_suffix
            value: .openfaas-fn.svc.cluster.local
          - name: ack_wait
            value: 60s
          - name: secret_mount_path
            value: /var/secrets
          - name: basic_auth
            value: "true"
          image: openfaas/queue-worker:0.9.0
          imagePullPolicy: Always
          name: queue-worker
          resources:
            requests:
              cpu: 50m
              memory: 120Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/secrets
            name: auth
            readOnly: true
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/arch: amd64
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: auth
          secret:
            defaultMode: 420
            secretName: basic-auth
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-02-27T04:07:09Z"
    generation: 2
    labels:
      app: polaris
      component: dashboard
      pod-template-hash: 7795686ff7
    name: polaris-dashboard-7795686ff7
    namespace: polaris
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: polaris-dashboard
      uid: 9ffb2e13-5916-11ea-8ab4-42010a8000a7
    resourceVersion: "29701352"
    selfLink: /apis/apps/v1/namespaces/polaris/replicasets/polaris-dashboard-7795686ff7
    uid: 9ffcdde9-5916-11ea-8ab4-42010a8000a7
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: polaris
        component: dashboard
        pod-template-hash: 7795686ff7
    template:
      metadata:
        annotations:
          checksum/config: 8aa5a565fba7a2db98d46752087de8c1dcc83b70cd762c5829d5ba01270d54a2
        creationTimestamp: null
        labels:
          app: polaris
          component: dashboard
          pod-template-hash: 7795686ff7
      spec:
        containers:
        - command:
          - polaris
          - --dashboard
          - --config
          - /opt/app/config.yaml
          image: quay.io/fairwinds/polaris:0.6
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          name: dashboard
          ports:
          - containerPort: 8080
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/app/config.yaml
            name: config
            readOnly: true
            subPath: config.yaml
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: polaris-dashboard
        serviceAccountName: polaris-dashboard
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: polaris
          name: config
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2020-03-21T23:57:14Z"
    generation: 1
    labels:
      app: polaris
      component: dashboard
      pod-template-hash: 8554786c49
    name: polaris-dashboard-8554786c49
    namespace: polaris
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: polaris-dashboard
      uid: 9ffb2e13-5916-11ea-8ab4-42010a8000a7
    resourceVersion: "33302979"
    selfLink: /apis/apps/v1/namespaces/polaris/replicasets/polaris-dashboard-8554786c49
    uid: b00f0760-6bcf-11ea-923a-42010a800150
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: polaris
        component: dashboard
        pod-template-hash: 8554786c49
    template:
      metadata:
        annotations:
          checksum/config: 8aa5a565fba7a2db98d46752087de8c1dcc83b70cd762c5829d5ba01270d54a2
        creationTimestamp: null
        labels:
          app: polaris
          component: dashboard
          pod-template-hash: 8554786c49
      spec:
        containers:
        - command:
          - polaris
          - --dashboard
          - --config
          - /opt/app/config.yaml
          image: quay.io/fairwinds/polaris:0.6
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          name: dashboard
          ports:
          - containerPort: 8080
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 20
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 50m
              memory: 128Mi
            requests:
              cpu: 10m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            privileged: false
            readOnlyRootFilesystem: true
            runAsNonRoot: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /opt/app/config.yaml
            name: config
            readOnly: true
            subPath: config.yaml
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: polaris-dashboard
        serviceAccountName: polaris-dashboard
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: polaris
          name: config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-17T21:24:51Z"
    generation: 1
    labels:
      name: carts
      pod-template-hash: 6bfcf84f4
    name: carts-6bfcf84f4
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: carts
      uid: cc4244d8-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571838"
    selfLink: /apis/apps/v1/namespaces/sock-shop/replicasets/carts-6bfcf84f4
    uid: cc4356c4-396f-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: carts
        pod-template-hash: 6bfcf84f4
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: carts
          pod-template-hash: 6bfcf84f4
      spec:
        containers:
        - env:
          - name: ZIPKIN
            value: zipkin.jaeger.svc.cluster.local
          - name: JAVA_OPTS
            value: -Xms64m -Xmx128m -XX:PermSize=32m -XX:MaxPermSize=64m -XX:+UseG1GC
              -Djava.security.egd=file:/dev/urandom
          image: weaveworksdemos/carts:0.4.8
          imagePullPolicy: IfNotPresent
          name: carts
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: tmp-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-17T21:24:51Z"
    generation: 1
    labels:
      name: carts-db
      pod-template-hash: 6bfc588c5f
    name: carts-db-6bfc588c5f
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: carts-db
      uid: cbf7bfd6-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571746"
    selfLink: /apis/apps/v1/namespaces/sock-shop/replicasets/carts-db-6bfc588c5f
    uid: cbf93c93-396f-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: carts-db
        pod-template-hash: 6bfc588c5f
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: carts-db
          pod-template-hash: 6bfc588c5f
      spec:
        containers:
        - image: mongo
          imagePullPolicy: Always
          name: carts-db
          ports:
          - containerPort: 27017
            name: mongo
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              - SETGID
              - SETUID
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: tmp-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-17T21:24:52Z"
    generation: 1
    labels:
      name: catalogue
      pod-template-hash: 77d4f66dbf
    name: catalogue-77d4f66dbf
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: catalogue
      uid: ccd0c82a-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571676"
    selfLink: /apis/apps/v1/namespaces/sock-shop/replicasets/catalogue-77d4f66dbf
    uid: ccd19f94-396f-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: catalogue
        pod-template-hash: 77d4f66dbf
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: catalogue
          pod-template-hash: 77d4f66dbf
      spec:
        containers:
        - image: weaveworksdemos/catalogue:0.3.5
          imagePullPolicy: IfNotPresent
          name: catalogue
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-17T21:24:52Z"
    generation: 1
    labels:
      name: catalogue-db
      pod-template-hash: 99cbcbb88
    name: catalogue-db-99cbcbb88
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: catalogue-db
      uid: cc8f5088-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571909"
    selfLink: /apis/apps/v1/namespaces/sock-shop/replicasets/catalogue-db-99cbcbb88
    uid: cc905ce0-396f-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: catalogue-db
        pod-template-hash: 99cbcbb88
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: catalogue-db
          pod-template-hash: 99cbcbb88
      spec:
        containers:
        - env:
          - name: MYSQL_ROOT_PASSWORD
            value: fake_password
          - name: MYSQL_DATABASE
            value: socksdb
          image: weaveworksdemos/catalogue-db:0.3.0
          imagePullPolicy: IfNotPresent
          name: catalogue-db
          ports:
          - containerPort: 3306
            name: mysql
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-17T21:24:53Z"
    generation: 1
    labels:
      name: front-end
      pod-template-hash: b5f568888
    name: front-end-b5f568888
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: front-end
      uid: cd15ff05-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571872"
    selfLink: /apis/apps/v1/namespaces/sock-shop/replicasets/front-end-b5f568888
    uid: cd16e418-396f-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: front-end
        pod-template-hash: b5f568888
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: front-end
          pod-template-hash: b5f568888
      spec:
        containers:
        - image: weaveworksdemos/front-end:0.3.12
          imagePullPolicy: IfNotPresent
          name: front-end
          ports:
          - containerPort: 8079
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            capabilities:
              drop:
              - all
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-17T21:24:54Z"
    generation: 1
    labels:
      name: orders
      pod-template-hash: 54d7666f75
    name: orders-54d7666f75
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: orders
      uid: cd9a8d49-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571762"
    selfLink: /apis/apps/v1/namespaces/sock-shop/replicasets/orders-54d7666f75
    uid: cd9b96c2-396f-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: orders
        pod-template-hash: 54d7666f75
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: orders
          pod-template-hash: 54d7666f75
      spec:
        containers:
        - env:
          - name: ZIPKIN
            value: zipkin.jaeger.svc.cluster.local
          - name: JAVA_OPTS
            value: -Xms64m -Xmx128m -XX:PermSize=32m -XX:MaxPermSize=64m -XX:+UseG1GC
              -Djava.security.egd=file:/dev/urandom
          image: weaveworksdemos/orders:0.4.7
          imagePullPolicy: IfNotPresent
          name: orders
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: tmp-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-17T21:24:53Z"
    generation: 1
    labels:
      name: orders-db
      pod-template-hash: 7888765df9
    name: orders-db-7888765df9
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: orders-db
      uid: cd582567-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571773"
    selfLink: /apis/apps/v1/namespaces/sock-shop/replicasets/orders-db-7888765df9
    uid: cd591f13-396f-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: orders-db
        pod-template-hash: 7888765df9
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: orders-db
          pod-template-hash: 7888765df9
      spec:
        containers:
        - image: mongo
          imagePullPolicy: Always
          name: orders-db
          ports:
          - containerPort: 27017
            name: mongo
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              - SETGID
              - SETUID
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: tmp-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-17T21:24:54Z"
    generation: 1
    labels:
      name: payment
      pod-template-hash: 7d8497bcd7
    name: payment-7d8497bcd7
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: payment
      uid: cddd5e77-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571917"
    selfLink: /apis/apps/v1/namespaces/sock-shop/replicasets/payment-7d8497bcd7
    uid: cdde9d8b-396f-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: payment
        pod-template-hash: 7d8497bcd7
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: payment
          pod-template-hash: 7d8497bcd7
      spec:
        containers:
        - image: weaveworksdemos/payment:0.4.3
          imagePullPolicy: IfNotPresent
          name: payment
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-17T21:24:55Z"
    generation: 1
    labels:
      name: queue-master
      pod-template-hash: 6bb75d8867
    name: queue-master-6bb75d8867
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: queue-master
      uid: ce209a3d-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571891"
    selfLink: /apis/apps/v1/namespaces/sock-shop/replicasets/queue-master-6bb75d8867
    uid: ce2359f4-396f-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: queue-master
        pod-template-hash: 6bb75d8867
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: queue-master
          pod-template-hash: 6bb75d8867
      spec:
        containers:
        - image: weaveworksdemos/queue-master:0.3.1
          imagePullPolicy: IfNotPresent
          name: queue-master
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-17T21:24:55Z"
    generation: 1
    labels:
      name: rabbitmq
      pod-template-hash: 5786759fc9
    name: rabbitmq-5786759fc9
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rabbitmq
      uid: ce751d94-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571645"
    selfLink: /apis/apps/v1/namespaces/sock-shop/replicasets/rabbitmq-5786759fc9
    uid: ce77cf2d-396f-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: rabbitmq
        pod-template-hash: 5786759fc9
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: rabbitmq
          pod-template-hash: 5786759fc9
      spec:
        containers:
        - image: rabbitmq:3.6.8
          imagePullPolicy: IfNotPresent
          name: rabbitmq
          ports:
          - containerPort: 5672
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              - SETGID
              - SETUID
              - DAC_OVERRIDE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-17T21:24:56Z"
    generation: 1
    labels:
      name: shipping
      pod-template-hash: 58bc954d85
    name: shipping-58bc954d85
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: shipping
      uid: cebd0063-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571885"
    selfLink: /apis/apps/v1/namespaces/sock-shop/replicasets/shipping-58bc954d85
    uid: cebe06a1-396f-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: shipping
        pod-template-hash: 58bc954d85
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: shipping
          pod-template-hash: 58bc954d85
      spec:
        containers:
        - env:
          - name: ZIPKIN
            value: zipkin.jaeger.svc.cluster.local
          - name: JAVA_OPTS
            value: -Xms64m -Xmx128m -XX:PermSize=32m -XX:MaxPermSize=64m -XX:+UseG1GC
              -Djava.security.egd=file:/dev/urandom
          image: weaveworksdemos/shipping:0.4.8
          imagePullPolicy: IfNotPresent
          name: shipping
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: tmp-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-17T21:24:56Z"
    generation: 1
    labels:
      name: user
      pod-template-hash: 6b5f6896c4
    name: user-6b5f6896c4
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: user
      uid: cf4394d3-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571849"
    selfLink: /apis/apps/v1/namespaces/sock-shop/replicasets/user-6b5f6896c4
    uid: cf44f035-396f-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: user
        pod-template-hash: 6b5f6896c4
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: user
          pod-template-hash: 6b5f6896c4
      spec:
        containers:
        - env:
          - name: MONGO_HOST
            value: user-db:27017
          image: weaveworksdemos/user:0.4.7
          imagePullPolicy: IfNotPresent
          name: user
          ports:
          - containerPort: 80
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 10001
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2020-01-17T21:24:56Z"
    generation: 1
    labels:
      name: user-db
      pod-template-hash: 6d5c9f6d84
    name: user-db-6d5c9f6d84
    namespace: sock-shop
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: user-db
      uid: cefe8b27-396f-11ea-b115-42010a8001d6
    resourceVersion: "33571787"
    selfLink: /apis/apps/v1/namespaces/sock-shop/replicasets/user-db-6d5c9f6d84
    uid: ceff8b17-396f-11ea-b115-42010a8001d6
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: user-db
        pod-template-hash: 6d5c9f6d84
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: user-db
          pod-template-hash: 6d5c9f6d84
      spec:
        containers:
        - image: weaveworksdemos/user-db:0.4.0
          imagePullPolicy: IfNotPresent
          name: user-db
          ports:
          - containerPort: 27017
            name: mongo
            protocol: TCP
          resources: {}
          securityContext:
            capabilities:
              add:
              - CHOWN
              - SETGID
              - SETUID
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: tmp-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 2
    labels:
      app: gitaly
      chart: gitaly
      heritage: Helm
      release: gitlab
    name: gitlab-gitaly
    namespace: gitlab
    resourceVersion: "33572292"
    selfLink: /apis/apps/v1/namespaces/gitlab/statefulsets/gitlab-gitaly
    uid: 999dceb7-398b-11ea-b115-42010a8001d6
  spec:
    podManagementPolicy: Parallel
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: gitaly
        release: gitlab
    serviceName: gitlab-gitaly
    template:
      metadata:
        annotations:
          checksum/config: 164952b2fb54733dd4a2c06a3f1429e87c5410ecfcd72236bb546ddea4bf277a
        creationTimestamp: null
        labels:
          app: gitaly
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: gitaly
                    release: gitlab
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /etc/gitaly/templates
          - name: CONFIG_DIRECTORY
            value: /etc/gitaly
          - name: GITALY_CONFIG_FILE
            value: /etc/gitaly/config.toml
          - name: SSL_CERT_DIR
            value: /etc/ssl/certs
          - name: GITALY_PROMETHEUS_LISTEN_ADDR
            value: :9236
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          image: registry.gitlab.com/gitlab-org/build/cng/gitaly:v1.77.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /scripts/healthcheck
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: gitaly
          ports:
          - containerPort: 8075
            protocol: TCP
          - containerPort: 9236
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /scripts/healthcheck
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          resources:
            requests:
              cpu: 50m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
          - mountPath: /etc/gitaly/templates
            name: gitaly-config
          - mountPath: /etc/gitlab-secrets
            name: gitaly-secrets
            readOnly: true
          - mountPath: /home/git/repositories
            name: repo-data
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: gitaly-config
            readOnly: true
          - mountPath: /init-config
            name: init-gitaly-secrets
            readOnly: true
          - mountPath: /init-secrets
            name: gitaly-secrets
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: gitlab-gitaly
          name: gitaly-config
        - emptyDir:
            medium: Memory
          name: gitaly-secrets
        - name: init-gitaly-secrets
          projected:
            defaultMode: 288
            sources:
            - secret:
                items:
                - key: token
                  path: gitaly_token
                name: gitlab-gitaly-secret
            - secret:
                items:
                - key: secret
                  path: .gitlab_shell_secret
                name: gitlab-gitlab-shell-secret
            - secret:
                items:
                - key: secret
                  path: redis_password
                name: gitlab-redis-secret
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - metadata:
        creationTimestamp: null
        labels:
          app: gitaly
          release: gitlab
        name: repo-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 50Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    collisionCount: 0
    currentReplicas: 1
    currentRevision: gitlab-gitaly-7cf784db64
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updateRevision: gitlab-gitaly-7cf784db64
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    generation: 4
    labels:
      app: redis
      chart: redis-ha
      heritage: Helm
      name: redis-server
      redis-node: "true"
      release: gitlab
    name: gitlab-redis-server
    namespace: gitlab
    resourceVersion: "33572222"
    selfLink: /apis/apps/v1/namespaces/gitlab/statefulsets/gitlab-redis-server
    uid: 999be56c-398b-11ea-b115-42010a8001d6
  spec:
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: redis
        name: redis-server
        redis-node: "true"
        release: gitlab
    serviceName: gitlab-redis
    template:
      metadata:
        annotations:
          checksum/config: a17ca01b2bd51c3d701b455723a4d2324df05a7ec7436349a928b62f74fbee96
        creationTimestamp: null
        labels:
          app: redis
          chart: redis-ha-0.1.0
          heritage: Helm
          name: redis-server
          redis-node: "true"
          release: gitlab
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - redis
                  - key: release
                    operator: In
                    values:
                    - gitlab
                  - key: redis-role
                    operator: In
                    values:
                    - master
                    - slave
                topologyKey: kubernetes.io/hostname
              weight: 1
        containers:
        - env:
          - name: REDIS_SENTINEL_SERVICE_HOST
            value: redis-sentinel
          - name: REDIS_CHART_PREFIX
            value: gitlab-redis
          - name: REDIS_PASSWORD_FILE
            value: /etc/redis/pass
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-redis-ha:732704f18e34ba469df34b10c3b2465e0469d484
          imagePullPolicy: Always
          name: redis
          ports:
          - containerPort: 6379
            protocol: TCP
          resources:
            requests:
              cpu: 25m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /redis-master-data
            name: data
          - mountPath: /etc/redis/
            name: gitlab-config
        - env:
          - name: REDIS_FILE
            value: /metrics/redis
          image: oliver006/redis_exporter:v0.34.1-alpine
          imagePullPolicy: IfNotPresent
          name: metrics
          ports:
          - containerPort: 9121
            name: metrics
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /metrics
            name: gitlab-metrics
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: gitlab
            readOnly: true
          - mountPath: /redis
            name: gitlab-config
          - mountPath: /metrics
            name: gitlab-metrics
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 2000
          runAsUser: 2000
        serviceAccount: gitlab-redis
        serviceAccountName: gitlab-redis
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: gitlab-config
        - emptyDir:
            medium: Memory
          name: gitlab-metrics
        - name: gitlab
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: redis-master.conf
                  path: redis-master.conf
                - key: redis-slave.conf
                  path: redis-slave.conf
                - key: configure
                  path: configure
                name: gitlab-redis
            - secret:
                items:
                - key: secret
                  path: password
                name: gitlab-redis-secret
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 8Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    collisionCount: 0
    currentReplicas: 1
    currentRevision: gitlab-redis-server-6df9884d78
    observedGeneration: 4
    readyReplicas: 1
    replicas: 1
    updateRevision: gitlab-redis-server-6df9884d78
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    creationTimestamp: "2020-03-27T23:06:16Z"
    generation: 4
    name: kubevious-mysql
    namespace: kubevious
    resourceVersion: "33572069"
    selfLink: /apis/apps/v1/namespaces/kubevious/statefulsets/kubevious-mysql
    uid: 8f9f0338-707f-11ea-8ebf-42010a800207
  spec:
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kubevious-mysql
    serviceName: kubevious-mysql-svc
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kubevious-mysql
      spec:
        containers:
        - env:
          - name: MYSQL_DATABASE
            value: kubevious
          - name: MYSQL_ALLOW_EMPTY_PASSWORD
            value: "1"
          image: mysql:8.0.19
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - mysqladmin
              - ping
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: mysql
          ports:
          - containerPort: 3306
            name: mysql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - mysql
              - -h
              - 127.0.0.1
              - -e
              - SELECT 1
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 2
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 50m
              memory: 500Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/mysql
            name: data
            subPath: mysql
          - mountPath: /etc/mysql/conf.d
            name: conf
          - mountPath: /docker-entrypoint-initdb.d
            name: init-script
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - bash
          - -c
          - |
            set -ex
            echo "[mysqld]" > /mnt/conf.d/server-id.cnf
            echo "server-id=1" >> /mnt/conf.d/server-id.cnf
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          image: mysql:8.0.19
          imagePullPolicy: IfNotPresent
          name: init-mysql
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /mnt/conf.d
            name: conf
          - mountPath: /mnt/config-map
            name: config-map
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: conf
        - configMap:
            defaultMode: 420
            name: kubevious-mysql-conf
          name: config-map
        - configMap:
            defaultMode: 420
            name: kubevious-mysql-init-script
          name: init-script
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
    volumeClaimTemplates:
    - metadata:
        creationTimestamp: null
        name: data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 30Gi
        volumeMode: Filesystem
      status:
        phase: Pending
  status:
    collisionCount: 0
    currentReplicas: 1
    currentRevision: kubevious-mysql-597f894869
    observedGeneration: 4
    readyReplicas: 1
    replicas: 1
    updateRevision: kubevious-mysql-597f894869
    updatedReplicas: 1
- apiVersion: autoscaling/v1
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      autoscaling.alpha.kubernetes.io/conditions: '[{"type":"AbleToScale","status":"True","lastTransitionTime":"2020-01-18T00:44:09Z","reason":"ReadyForNewScale","message":"recommended
        size matches current size"},{"type":"ScalingActive","status":"True","lastTransitionTime":"2020-04-10T00:10:05Z","reason":"ValidMetricFound","message":"the
        HPA was able to successfully calculate a replica count from cpu resource"},{"type":"ScalingLimited","status":"False","lastTransitionTime":"2020-03-31T02:19:23Z","reason":"DesiredWithinRange","message":"the
        desired count is within the acceptable range"}]'
      autoscaling.alpha.kubernetes.io/current-metrics: '[{"type":"Resource","resource":{"name":"cpu","currentAverageValue":"1m"}}]'
      autoscaling.alpha.kubernetes.io/metrics: '[{"type":"Resource","resource":{"name":"cpu","targetAverageValue":"100m"}}]'
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"autoscaling/v1","kind":"HorizontalPodAutoscaler","metadata":{"annotations":{"autoscaling.alpha.kubernetes.io/conditions":"[{\"type\":\"AbleToScale\",\"status\":\"True\",\"lastTransitionTime\":\"2020-01-18T00:44:09Z\",\"reason\":\"ReadyForNewScale\",\"message\":\"recommended size matches current size\"},{\"type\":\"ScalingActive\",\"status\":\"True\",\"lastTransitionTime\":\"2020-01-30T07:05:49Z\",\"reason\":\"ValidMetricFound\",\"message\":\"the HPA was able to successfully calculate a replica count from cpu resource\"},{\"type\":\"ScalingLimited\",\"status\":\"True\",\"lastTransitionTime\":\"2020-01-30T07:19:56Z\",\"reason\":\"TooFewReplicas\",\"message\":\"the desired replica count is increasing faster than the maximum scale rate\"}]","autoscaling.alpha.kubernetes.io/current-metrics":"[{\"type\":\"Resource\",\"resource\":{\"name\":\"cpu\",\"currentAverageValue\":\"2m\"}}]","autoscaling.alpha.kubernetes.io/metrics":"[{\"type\":\"Resource\",\"resource\":{\"name\":\"cpu\",\"targetAverageValue\":\"100m\"}}]"},"creationTimestamp":"2020-01-18T00:43:52Z","labels":{"app":"gitlab-shell","chart":"gitlab-shell-2.6.5","heritage":"Helm","release":"gitlab"},"name":"gitlab-gitlab-shell","namespace":"gitlab","selfLink":"/apis/autoscaling/v1/namespaces/gitlab/horizontalpodautoscalers/gitlab-gitlab-shell","uid":"998ad60c-398b-11ea-b115-42010a8001d6"},"spec":{"maxReplicas":3,"minReplicas":1,"scaleTargetRef":{"apiVersion":"apps/v1","kind":"Deployment","name":"gitlab-gitlab-shell"}},"status":{"currentReplicas":2,"desiredReplicas":2,"lastScaleTime":"2020-01-18T00:44:11Z"}}
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: gitlab-shell
      chart: gitlab-shell-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-gitlab-shell
    namespace: gitlab
    resourceVersion: "37202428"
    selfLink: /apis/autoscaling/v1/namespaces/gitlab/horizontalpodautoscalers/gitlab-gitlab-shell
    uid: 998ad60c-398b-11ea-b115-42010a8001d6
  spec:
    maxReplicas: 3
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: gitlab-gitlab-shell
  status:
    currentReplicas: 1
    desiredReplicas: 1
    lastScaleTime: "2020-04-01T14:01:15Z"
- apiVersion: autoscaling/v1
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      autoscaling.alpha.kubernetes.io/conditions: '[{"type":"AbleToScale","status":"True","lastTransitionTime":"2020-01-18T00:44:08Z","reason":"ReadyForNewScale","message":"recommended
        size matches current size"},{"type":"ScalingActive","status":"True","lastTransitionTime":"2020-04-10T22:56:37Z","reason":"ValidMetricFound","message":"the
        HPA was able to successfully calculate a replica count from cpu resource utilization
        (percentage of request)"},{"type":"ScalingLimited","status":"False","lastTransitionTime":"2020-02-04T00:55:09Z","reason":"DesiredWithinRange","message":"the
        desired count is within the acceptable range"}]'
      autoscaling.alpha.kubernetes.io/current-metrics: '[{"type":"Resource","resource":{"name":"cpu","currentAverageUtilization":2,"currentAverageValue":"1m"}}]'
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"autoscaling/v1","kind":"HorizontalPodAutoscaler","metadata":{"annotations":{"autoscaling.alpha.kubernetes.io/conditions":"[{\"type\":\"AbleToScale\",\"status\":\"True\",\"lastTransitionTime\":\"2020-01-18T00:44:08Z\",\"reason\":\"ReadyForNewScale\",\"message\":\"recommended size matches current size\"},{\"type\":\"ScalingActive\",\"status\":\"True\",\"lastTransitionTime\":\"2020-01-30T07:05:49Z\",\"reason\":\"ValidMetricFound\",\"message\":\"the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)\"},{\"type\":\"ScalingLimited\",\"status\":\"True\",\"lastTransitionTime\":\"2020-01-30T07:20:11Z\",\"reason\":\"TooFewReplicas\",\"message\":\"the desired replica count is increasing faster than the maximum scale rate\"}]","autoscaling.alpha.kubernetes.io/current-metrics":"[{\"type\":\"Resource\",\"resource\":{\"name\":\"cpu\",\"currentAverageUtilization\":2,\"currentAverageValue\":\"1m\"}}]"},"creationTimestamp":"2020-01-18T00:43:52Z","labels":{"app":"registry","chart":"registry-0.3.0","heritage":"Helm","release":"gitlab"},"name":"gitlab-registry","namespace":"gitlab","selfLink":"/apis/autoscaling/v1/namespaces/gitlab/horizontalpodautoscalers/gitlab-registry","uid":"998a3b00-398b-11ea-b115-42010a8001d6"},"spec":{"maxReplicas":4,"minReplicas":1,"scaleTargetRef":{"apiVersion":"apps/v1","kind":"Deployment","name":"gitlab-registry"},"targetCPUUtilizationPercentage":75},"status":{"currentCPUUtilizationPercentage":2,"currentReplicas":2,"desiredReplicas":2}}
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: registry
      chart: registry-0.3.0
      heritage: Helm
      release: gitlab
    name: gitlab-registry
    namespace: gitlab
    resourceVersion: "37202430"
    selfLink: /apis/autoscaling/v1/namespaces/gitlab/horizontalpodautoscalers/gitlab-registry
    uid: 998a3b00-398b-11ea-b115-42010a8001d6
  spec:
    maxReplicas: 4
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: gitlab-registry
    targetCPUUtilizationPercentage: 75
  status:
    currentCPUUtilizationPercentage: 2
    currentReplicas: 1
    desiredReplicas: 1
    lastScaleTime: "2020-04-10T23:27:36Z"
- apiVersion: autoscaling/v1
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      autoscaling.alpha.kubernetes.io/conditions: '[{"type":"AbleToScale","status":"True","lastTransitionTime":"2020-01-18T00:44:11Z","reason":"ReadyForNewScale","message":"recommended
        size matches current size"},{"type":"ScalingActive","status":"True","lastTransitionTime":"2020-04-10T00:10:05Z","reason":"ValidMetricFound","message":"the
        HPA was able to successfully calculate a replica count from cpu resource"},{"type":"ScalingLimited","status":"False","lastTransitionTime":"2020-03-29T09:44:39Z","reason":"DesiredWithinRange","message":"the
        desired count is within the acceptable range"}]'
      autoscaling.alpha.kubernetes.io/current-metrics: '[{"type":"Resource","resource":{"name":"cpu","currentAverageValue":"48m"}}]'
      autoscaling.alpha.kubernetes.io/metrics: '[{"type":"Resource","resource":{"name":"cpu","targetAverageValue":"350m"}}]'
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"autoscaling/v1","kind":"HorizontalPodAutoscaler","metadata":{"annotations":{"autoscaling.alpha.kubernetes.io/conditions":"[{\"type\":\"AbleToScale\",\"status\":\"True\",\"lastTransitionTime\":\"2020-01-18T00:44:11Z\",\"reason\":\"ReadyForNewScale\",\"message\":\"recommended size matches current size\"},{\"type\":\"ScalingActive\",\"status\":\"True\",\"lastTransitionTime\":\"2020-02-03T19:10:28Z\",\"reason\":\"ValidMetricFound\",\"message\":\"the HPA was able to successfully calculate a replica count from cpu resource\"},{\"type\":\"ScalingLimited\",\"status\":\"False\",\"lastTransitionTime\":\"2020-01-18T01:01:34Z\",\"reason\":\"DesiredWithinRange\",\"message\":\"the desired count is within the acceptable range\"}]","autoscaling.alpha.kubernetes.io/current-metrics":"[{\"type\":\"Resource\",\"resource\":{\"name\":\"cpu\",\"currentAverageValue\":\"15m\"}}]","autoscaling.alpha.kubernetes.io/metrics":"[{\"type\":\"Resource\",\"resource\":{\"name\":\"cpu\",\"targetAverageValue\":\"350m\"}}]"},"creationTimestamp":"2020-01-18T00:43:52Z","labels":{"app":"sidekiq","chart":"sidekiq-2.6.5","heritage":"Helm","release":"gitlab"},"name":"gitlab-sidekiq-all-in-1","namespace":"gitlab","selfLink":"/apis/autoscaling/v1/namespaces/gitlab/horizontalpodautoscalers/gitlab-sidekiq-all-in-1","uid":"998ae407-398b-11ea-b115-42010a8001d6"},"spec":{"maxReplicas":2,"minReplicas":1,"scaleTargetRef":{"apiVersion":"apps/v1","kind":"Deployment","name":"gitlab-sidekiq-all-in-1"}},"status":{"currentReplicas":9,"desiredReplicas":9,"lastScaleTime":"2020-02-02T23:45:10Z"}}
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: sidekiq
      chart: sidekiq-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-sidekiq-all-in-1
    namespace: gitlab
    resourceVersion: "37202431"
    selfLink: /apis/autoscaling/v1/namespaces/gitlab/horizontalpodautoscalers/gitlab-sidekiq-all-in-1
    uid: 998ae407-398b-11ea-b115-42010a8001d6
  spec:
    maxReplicas: 2
    minReplicas: 1
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: gitlab-sidekiq-all-in-1
  status:
    currentReplicas: 1
    desiredReplicas: 1
    lastScaleTime: "2020-04-01T14:01:09Z"
- apiVersion: autoscaling/v1
  kind: HorizontalPodAutoscaler
  metadata:
    annotations:
      autoscaling.alpha.kubernetes.io/conditions: '[{"type":"AbleToScale","status":"True","lastTransitionTime":"2020-01-18T00:44:08Z","reason":"ReadyForNewScale","message":"recommended
        size matches current size"},{"type":"ScalingActive","status":"True","lastTransitionTime":"2020-04-08T02:24:00Z","reason":"ValidMetricFound","message":"the
        HPA was able to successfully calculate a replica count from cpu resource"},{"type":"ScalingLimited","status":"True","lastTransitionTime":"2020-04-10T23:01:16Z","reason":"TooFewReplicas","message":"the
        desired replica count is more than the maximum replica count"}]'
      autoscaling.alpha.kubernetes.io/current-metrics: '[{"type":"Resource","resource":{"name":"cpu","currentAverageValue":"8m"}}]'
      autoscaling.alpha.kubernetes.io/metrics: '[{"type":"Resource","resource":{"name":"cpu","targetAverageValue":"1"}}]'
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"autoscaling/v1","kind":"HorizontalPodAutoscaler","metadata":{"annotations":{"autoscaling.alpha.kubernetes.io/conditions":"[{\"type\":\"AbleToScale\",\"status\":\"True\",\"lastTransitionTime\":\"2020-01-18T00:44:08Z\",\"reason\":\"ScaleDownStabilized\",\"message\":\"recent recommendations were higher than current one, applying the highest recent recommendation\"},{\"type\":\"ScalingActive\",\"status\":\"True\",\"lastTransitionTime\":\"2020-02-02T23:56:47Z\",\"reason\":\"ValidMetricFound\",\"message\":\"the HPA was able to successfully calculate a replica count from cpu resource\"},{\"type\":\"ScalingLimited\",\"status\":\"False\",\"lastTransitionTime\":\"2020-02-04T00:51:27Z\",\"reason\":\"DesiredWithinRange\",\"message\":\"the desired count is within the acceptable range\"}]","autoscaling.alpha.kubernetes.io/current-metrics":"[{\"type\":\"Resource\",\"resource\":{\"name\":\"cpu\",\"currentAverageValue\":\"4m\"}}]","autoscaling.alpha.kubernetes.io/metrics":"[{\"type\":\"Resource\",\"resource\":{\"name\":\"cpu\",\"targetAverageValue\":\"1\"}}]"},"creationTimestamp":"2020-01-18T00:43:52Z","labels":{"app":"unicorn","chart":"unicorn-2.6.5","heritage":"Helm","release":"gitlab"},"name":"gitlab-unicorn","namespace":"gitlab","selfLink":"/apis/autoscaling/v1/namespaces/gitlab/horizontalpodautoscalers/gitlab-unicorn","uid":"998b0a32-398b-11ea-b115-42010a8001d6"},"spec":{"maxReplicas":3,"minReplicas":2,"scaleTargetRef":{"apiVersion":"apps/v1","kind":"Deployment","name":"gitlab-unicorn"}},"status":{"currentReplicas":2,"desiredReplicas":2,"lastScaleTime":"2020-02-03T00:26:40Z"}}
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: unicorn
      chart: unicorn-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-unicorn
    namespace: gitlab
    resourceVersion: "37202427"
    selfLink: /apis/autoscaling/v1/namespaces/gitlab/horizontalpodautoscalers/gitlab-unicorn
    uid: 998b0a32-398b-11ea-b115-42010a8001d6
  spec:
    maxReplicas: 3
    minReplicas: 2
    scaleTargetRef:
      apiVersion: apps/v1
      kind: Deployment
      name: gitlab-unicorn
  status:
    currentReplicas: 2
    desiredReplicas: 2
    lastScaleTime: "2020-03-28T22:40:23Z"
- apiVersion: batch/v1
  kind: Job
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: certmanager-issuer
      chart: certmanager-issuer-0.1.0
      heritage: Helm
      release: gitlab
    name: gitlab-issuer.1
    namespace: gitlab
    resourceVersion: "3690645"
    selfLink: /apis/batch/v1/namespaces/gitlab/jobs/gitlab-issuer.1
    uid: 99aa2209-398b-11ea-b115-42010a8001d6
  spec:
    activeDeadlineSeconds: 300
    backoffLimit: 6
    completions: 1
    parallelism: 1
    selector:
      matchLabels:
        controller-uid: 99aa2209-398b-11ea-b115-42010a8001d6
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: certmanager-issuer
          controller-uid: 99aa2209-398b-11ea-b115-42010a8001d6
          job-name: gitlab-issuer.1
          release: gitlab
      spec:
        containers:
        - command:
          - /bin/bash
          - /scripts/create-issuer
          - /scripts/issuer.yml
          image: registry.gitlab.com/gitlab-org/build/cng/kubectl:1.13.12
          imagePullPolicy: IfNotPresent
          name: create-issuer
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /scripts
            name: scripts
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: gitlab-certmanager-issuer
        serviceAccountName: gitlab-certmanager-issuer
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: gitlab-certmanager-issuer-certmanager
          name: scripts
  status:
    conditions:
    - lastProbeTime: "2020-01-18T00:49:04Z"
      lastTransitionTime: "2020-01-18T00:49:04Z"
      message: Job was active longer than specified deadline
      reason: DeadlineExceeded
      status: "True"
      type: Failed
    failed: 1
    startTime: "2020-01-18T00:43:53Z"
- apiVersion: batch/v1
  kind: Job
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: migrations
      chart: migrations-2.6.5
      heritage: Helm
      release: gitlab
    name: gitlab-migrations.1
    namespace: gitlab
    resourceVersion: "2020541"
    selfLink: /apis/batch/v1/namespaces/gitlab/jobs/gitlab-migrations.1
    uid: 99aab3c6-398b-11ea-b115-42010a8001d6
  spec:
    activeDeadlineSeconds: 3600
    backoffLimit: 6
    completions: 1
    parallelism: 1
    selector:
      matchLabels:
        controller-uid: 99aab3c6-398b-11ea-b115-42010a8001d6
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: migrations
          controller-uid: 99aab3c6-398b-11ea-b115-42010a8001d6
          job-name: gitlab-migrations.1
          release: gitlab
      spec:
        containers:
        - args:
          - /scripts/wait-for-deps
          - /scripts/db-migrate
          env:
          - name: GITLAB_SHARED_RUNNERS_REGISTRATION_TOKEN
            valueFrom:
              secretKeyRef:
                key: runner-registration-token
                name: gitlab-gitlab-runner-secret
          - name: CONFIG_TEMPLATE_DIRECTORY
            value: /var/opt/gitlab/templates
          - name: CONFIG_DIRECTORY
            value: /srv/gitlab/config
          - name: BYPASS_SCHEMA_VERSION
            value: "true"
          - name: ENABLE_BOOTSNAP
            value: "1"
          image: registry.gitlab.com/gitlab-org/build/cng/gitlab-rails-ee:v12.6.4
          imagePullPolicy: IfNotPresent
          name: migrations
          resources:
            requests:
              cpu: 50m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/opt/gitlab/templates
            name: migrations-config
          - mountPath: /etc/gitlab
            name: migrations-secrets
            readOnly: true
          - mountPath: /srv/gitlab/config/secrets.yml
            name: migrations-secrets
            subPath: rails-secrets/secrets.yml
          - mountPath: /srv/gitlab/config/initial_root_password
            name: migrations-secrets
            subPath: migrations/initial_root_password
          - mountPath: /etc/ssl/certs/
            name: etc-ssl-certs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - image: registry.gitlab.com/gitlab-org/build/cng/alpine-certificates:20171114-r3
          imagePullPolicy: IfNotPresent
          name: certificates
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ssl/certs
            name: etc-ssl-certs
        - command:
          - sh
          - /config/configure
          image: busybox:latest
          imagePullPolicy: Always
          name: configure
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: migrations-config
            readOnly: true
          - mountPath: /init-config
            name: init-migrations-secrets
            readOnly: true
          - mountPath: /init-secrets
            name: migrations-secrets
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 1000
          runAsUser: 1000
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: gitlab-migrations
          name: migrations-config
        - name: init-migrations-secrets
          projected:
            defaultMode: 256
            sources:
            - secret:
                items:
                - key: secrets.yml
                  path: rails-secrets/secrets.yml
                name: gitlab-rails-secret
            - secret:
                items:
                - key: token
                  path: gitaly/gitaly_token
                name: gitlab-gitaly-secret
            - secret:
                items:
                - key: secret
                  path: redis/password
                name: gitlab-redis-secret
            - secret:
                items:
                - key: postgres-password
                  path: postgres/psql-password
                name: gitlab-postgresql-password
            - secret:
                items:
                - key: password
                  path: migrations/initial_root_password
                name: gitlab-gitlab-initial-root-password
        - emptyDir:
            medium: Memory
          name: migrations-secrets
        - emptyDir:
            medium: Memory
          name: etc-ssl-certs
  status:
    completionTime: "2020-01-18T00:55:12Z"
    conditions:
    - lastProbeTime: "2020-01-18T00:55:12Z"
      lastTransitionTime: "2020-01-18T00:55:12Z"
      status: "True"
      type: Complete
    startTime: "2020-01-18T00:43:53Z"
    succeeded: 1
- apiVersion: batch/v1
  kind: Job
  metadata:
    creationTimestamp: "2020-01-18T00:43:52Z"
    labels:
      app: minio
      chart: minio-0.4.3
      heritage: Helm
      release: gitlab
    name: gitlab-minio-create-buckets.1
    namespace: gitlab
    resourceVersion: "2020277"
    selfLink: /apis/batch/v1/namespaces/gitlab/jobs/gitlab-minio-create-buckets.1
    uid: 99aa421b-398b-11ea-b115-42010a8001d6
  spec:
    activeDeadlineSeconds: 600
    backoffLimit: 6
    completions: 1
    parallelism: 1
    selector:
      matchLabels:
        controller-uid: 99aa421b-398b-11ea-b115-42010a8001d6
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: minio
          component: create-buckets
          controller-uid: 99aa421b-398b-11ea-b115-42010a8001d6
          job-name: gitlab-minio-create-buckets.1
          release: gitlab
      spec:
        containers:
        - command:
          - /bin/sh
          - /config/initialize
          env:
          - name: MINIO_ENDPOINT
            value: gitlab-minio-svc
          - name: MINIO_PORT
            value: "9000"
          image: minio/mc:RELEASE.2018-07-13T00-53-22Z
          imagePullPolicy: IfNotPresent
          name: minio-mc
          resources:
            requests:
              cpu: 50m
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /config
            name: minio-configuration
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: minio-configuration
          projected:
            defaultMode: 420
            sources:
            - configMap:
                name: gitlab-minio-config-cm
            - secret:
                name: gitlab-minio-secret
  status:
    conditions:
    - lastProbeTime: "2020-01-18T00:54:35Z"
      lastTransitionTime: "2020-01-18T00:54:35Z"
      message: Job was active longer than specified deadline
      reason: DeadlineExceeded
      status: "True"
      type: Failed
    failed: 2
    startTime: "2020-01-18T00:43:53Z"
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
